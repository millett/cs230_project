{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Adapted from https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # Rather than Torch, for example\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Add, Multiply, BatchNormalization\n",
    "from keras import initializers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.objectives import binary_crossentropy\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "import random\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data, make sure it's numeric\n",
    "X = pd.read_csv(\"../aline_imputed.csv\")\n",
    "#X = X.iloc[:,4:]  # We don't include the patient, hospital, or ICU stay ID's because they're meaningless in this context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['icu_los_day', 'hospital_los_day', 'hosp_exp_flag', 'icu_exp_flag',\n",
       "       'day_28_flag', 'chf_flag', 'afib_flag', 'renal_flag', 'liver_flag',\n",
       "       'copd_flag', 'cad_flag', 'stroke_flag', 'malignancy_flag',\n",
       "       'respfail_flag', 'endocarditis_flag', 'ards_flag', 'pneumonia_flag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_vars = X.columns[65:]\n",
    "outcome_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into a train/test fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Set the train/test size\n",
    "#X_model_input = X.drop(outcome_vars, axis=1)\n",
    "X_train, X_test = sklearn.model_selection.train_test_split(X, train_size=0.8)\n",
    "Y_train, Y_test = X_train[outcome_vars], X_test[outcome_vars]\n",
    "X_train, X_test = X_train.drop(list(outcome_vars), axis=1), X_test.drop(list(outcome_vars), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert your inputs to numpy arrays (Keras doesn't play nice with pandas dataframes)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41899, 65)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a decoder, generator and decoder+generator VAE on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vae_model(X_train, X_test, latent_dimensions, intermediate_dimensions, n_epochs = 20):\n",
    "    \n",
    "    #Hyperparameters of interest\n",
    "    intermediate_dim = intermediate_dimensions\n",
    "    latent_dim = latent_dimensions\n",
    "    \n",
    "    # Other hyperparameters\n",
    "    original_dim = X_test.shape[1]  # Calculated automatically\n",
    "    batch_size = 1000  # This might affect the learning rate, but shouldn't affect the overall model outcome\n",
    "    #n_epochs Not as important when we use the early stopping method (as we do in this method)\n",
    "    epsilon_std = 1.0  # This shouldn't significantly change the model performance, as the z_log_sigma will adjust to accommodate\n",
    "    \n",
    "    # The Encoder, from the original data to the latent dimensions\n",
    "    inputs = Input(shape=(original_dim,))\n",
    "    hidden = Dense(intermediate_dim, activation='relu', kernel_initializer=initializers.he_normal(seed=None))(inputs)\n",
    "    bn = BatchNormalization()(hidden)\n",
    "    z_mean = Dense(latent_dim, activation='linear', kernel_initializer=initializers.he_normal(seed=None))(bn)\n",
    "    z_log_sigma = Dense(latent_dim, activation='linear', kernel_initializer=initializers.Zeros())(bn)\n",
    "    \n",
    "    # The sampler\n",
    "    def sample_z(args):\n",
    "        z_mean, z_log_sigma = args\n",
    "        eps = K.random_normal(shape=(K.shape(inputs)[0], latent_dim),\n",
    "                             mean=0., stddev=1.)\n",
    "        return z_mean + K.exp(z_log_sigma / 2) * eps  # Element-wise product of SD with gaussian noise, + mean vector\n",
    "\n",
    "    z = Lambda(sample_z, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "    \n",
    "    # The Decoder\n",
    "    decoder_h = Dense(intermediate_dim, activation='relu', kernel_initializer=initializers.he_normal(seed=None))  # We don't specify the inputs because we'll use...\n",
    "    decoder_bn = BatchNormalization()\n",
    "    decoder_mean = Dense(original_dim, activation='linear', kernel_initializer=initializers.he_normal(seed=None))  # ...the same layers again for the generator model below\n",
    "    decoder_log_sigma = Dense(original_dim, activation='linear', kernel_initializer=initializers.Zeros())\n",
    "    h_decoded = decoder_h(z)\n",
    "    bn_decoded = decoder_bn(h_decoded)\n",
    "    output_decoded_mean = decoder_mean(bn_decoded)  # Our output is a mean vector (point estimate) and...\n",
    "    output_decoded_log_sigma = decoder_log_sigma(bn_decoded)  # ...a log_sigma, or log_variance vector...\n",
    "                                                            # which quantifies our certainty about the point estimate\n",
    "        \n",
    "    # The end-to-end autoencoder\n",
    "    vae = Model(inputs, output_decoded_mean)\n",
    "\n",
    "    # An encoder, from inputs to the latent space\n",
    "    encoder = Model(inputs, z_mean)\n",
    "\n",
    "    # A generator, from the latent space to the reconstructed inputs\n",
    "    generator_input = Input(shape=(latent_dim,))\n",
    "    generator_h_decoded = decoder_h(generator_input)\n",
    "    generator_bn_decoded = decoder_bn(generator_h_decoded)\n",
    "    generator_output_decoded_mean = decoder_mean(generator_bn_decoded)\n",
    "    generator_output_decoded_log_sigma = decoder_log_sigma(generator_bn_decoded)\n",
    "    generator = Model(generator_input, [generator_output_decoded_mean, generator_output_decoded_log_sigma])\n",
    "    \n",
    "    def vae_loss(y_true, y_pred):\n",
    "        \"\"\" Calculate loss = reconstruction loss + KL loss for each data in minibatch \"\"\"\n",
    "        # -E[log P(X|z)]    \n",
    "        reconstruction_loss = neg_log_likelihood(y_true, y_pred)\n",
    "\n",
    "        # D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n",
    "        kl_loss = KL_divergence_loss(y_true, y_pred)\n",
    "\n",
    "        return reconstruction_loss + kl_loss\n",
    "\n",
    "    def neg_log_likelihood(y_true, y_pred):\n",
    "        \"\"\" Calculate a proper negative log-likelihood where NLL = -log p(y_pred | mu=y_true, sigma=output_decoded_log_sigma)\"\"\"\n",
    "        negative_log_likelihood = (1./2.) * \\\n",
    "                        (\n",
    "                            K.int_shape(inputs)[1] * K.log(2. * np.pi) + \\\n",
    "                            K.sum(output_decoded_log_sigma, axis=-1, keepdims=True) + \\\n",
    "                            K.sum(\n",
    "                                K.square(y_true - y_pred) * (1. / K.exp(output_decoded_log_sigma)), \n",
    "                                axis=-1, keepdims=True\n",
    "                            )\n",
    "                        )\n",
    "        return negative_log_likelihood\n",
    "    \n",
    "    # An alternative loss function... should give similar results as the above, but isn't a proper NLL per-se\n",
    "    def squared_difference_loss(y_true, y_pred):\n",
    "        \"\"\" Calculate a naive reconstruction loss, i.e. ||y_true - y_pred||^2 \"\"\"\n",
    "        return K.sum(K.square(y_true - y_pred), axis=-1)  # TODO! This isn't strictly speaking the NLL, but it's a half decent approximation for now\n",
    "\n",
    "    def KL_divergence_loss(y_true, y_pred):\n",
    "        \"\"\" Calculate the KL Divergence portion of the loss, i.e. D_KL(Q(z|X) || P(z|X)) \"\"\"\n",
    "        return 0.5 * K.sum(K.exp(z_log_sigma) + K.square(z_mean) - 1. - z_log_sigma, axis=-1)\n",
    "    \n",
    "    adam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\n",
    "    \n",
    "    vae.compile(optimizer=adam, \n",
    "                loss=vae_loss, \n",
    "                metrics=[squared_difference_loss, \n",
    "                         KL_divergence_loss, \n",
    "                         neg_log_likelihood\n",
    "                        ]\n",
    "               )\n",
    "\n",
    "    #define an early stopping callback criterion so that we don't overfit\n",
    "    earlystop = keras.callbacks.EarlyStopping(monitor='val_loss',  # The quantity to be monitored\n",
    "                                             min_delta=0.0001,  # Minimum change in the monitored quantity in order to qualify\n",
    "                                                                # as an improvement (if an absolute change of less than min_delta occurs,\n",
    "                                                                # will not count as no improvement)\n",
    "                                             patience=10,  # The number of epochs with no improvement after which training will be stopped\n",
    "                                             verbose=0,  # Verbosity mode\n",
    "                                             mode='min')  # Training will stop when the quantity monitored has stopped decreasing\n",
    "    \n",
    "    \n",
    "    callbacks_list = [earlystop, keras.callbacks.TerminateOnNaN()]\n",
    "    \n",
    "    history = vae.fit(X_train,\n",
    "                      X_train,\n",
    "                      shuffle=True,\n",
    "                      epochs=n_epochs,\n",
    "                      verbose=2,\n",
    "                      batch_size=batch_size,\n",
    "                      callbacks=callbacks_list,\n",
    "                      validation_split=0.3)\n",
    "    \n",
    "    X_test_encoded = encoder.predict(X_test)  # We take our test data directly from data-space to latent variable\n",
    "    X_test_decoded_mean, X_test_decoded_log_sigma = generator.predict(X_test_encoded)  # space to calculate the\n",
    "    \n",
    "    # marginal log likelihood of the data under our fully-trained model (i.e. we don't add noise)\n",
    "    test_nll = (1./2.) * \\\n",
    "               (\n",
    "                   X_test.shape[1] * np.log(2. * np.pi) + \\\n",
    "                   np.sum(X_test_decoded_log_sigma, axis=-1, keepdims=True) + \\\n",
    "                   np.sum(\n",
    "                       np.square(X_test - X_test_decoded_mean) * (1. / np.exp(X_test_decoded_log_sigma)), \n",
    "                       axis=-1, keepdims=True\n",
    "                   )\n",
    "               )\n",
    "    \n",
    "    return np.mean(test_nll), vae, encoder, generator, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/20\n",
      " - 12s - loss: 95.5860 - squared_difference_loss: 65.4979 - KL_divergence_loss: 3.1060 - neg_log_likelihood: 92.4799 - val_loss: 86.8187 - val_squared_difference_loss: 46.2313 - val_KL_divergence_loss: 3.9721 - val_neg_log_likelihood: 82.8466\n",
      "Epoch 2/20\n",
      " - 1s - loss: 84.6858 - squared_difference_loss: 42.7577 - KL_divergence_loss: 3.5760 - neg_log_likelihood: 81.1099 - val_loss: 84.4731 - val_squared_difference_loss: 40.5100 - val_KL_divergence_loss: 4.4871 - val_neg_log_likelihood: 79.9860\n",
      "Epoch 3/20\n",
      " - 1s - loss: 83.5767 - squared_difference_loss: 39.9477 - KL_divergence_loss: 3.8719 - neg_log_likelihood: 79.7048 - val_loss: 83.9630 - val_squared_difference_loss: 39.5222 - val_KL_divergence_loss: 4.4709 - val_neg_log_likelihood: 79.4921\n",
      "Epoch 4/20\n",
      " - 1s - loss: 83.2160 - squared_difference_loss: 38.9873 - KL_divergence_loss: 3.9913 - neg_log_likelihood: 79.2246 - val_loss: 83.6281 - val_squared_difference_loss: 38.9997 - val_KL_divergence_loss: 4.3973 - val_neg_log_likelihood: 79.2309\n",
      "Epoch 5/20\n",
      " - 1s - loss: 82.9951 - squared_difference_loss: 38.4245 - KL_divergence_loss: 4.0519 - neg_log_likelihood: 78.9432 - val_loss: 83.4890 - val_squared_difference_loss: 38.6211 - val_KL_divergence_loss: 4.4475 - val_neg_log_likelihood: 79.0415\n",
      "Epoch 6/20\n",
      " - 1s - loss: 82.8201 - squared_difference_loss: 37.9460 - KL_divergence_loss: 4.1161 - neg_log_likelihood: 78.7040 - val_loss: 83.2670 - val_squared_difference_loss: 38.1838 - val_KL_divergence_loss: 4.4441 - val_neg_log_likelihood: 78.8229\n",
      "Epoch 7/20\n",
      " - 1s - loss: 82.6951 - squared_difference_loss: 37.6774 - KL_divergence_loss: 4.1254 - neg_log_likelihood: 78.5697 - val_loss: 83.1515 - val_squared_difference_loss: 37.9207 - val_KL_divergence_loss: 4.4602 - val_neg_log_likelihood: 78.6914\n",
      "Epoch 8/20\n",
      " - 1s - loss: 82.6129 - squared_difference_loss: 37.4272 - KL_divergence_loss: 4.1683 - neg_log_likelihood: 78.4446 - val_loss: 83.1383 - val_squared_difference_loss: 38.1973 - val_KL_divergence_loss: 4.3086 - val_neg_log_likelihood: 78.8297\n",
      "Epoch 9/20\n",
      " - 1s - loss: 82.5395 - squared_difference_loss: 37.3039 - KL_divergence_loss: 4.1565 - neg_log_likelihood: 78.3830 - val_loss: 83.1592 - val_squared_difference_loss: 37.7680 - val_KL_divergence_loss: 4.5442 - val_neg_log_likelihood: 78.6150\n",
      "Epoch 10/20\n",
      " - 1s - loss: 82.4442 - squared_difference_loss: 37.0583 - KL_divergence_loss: 4.1840 - neg_log_likelihood: 78.2602 - val_loss: 83.0130 - val_squared_difference_loss: 37.8789 - val_KL_divergence_loss: 4.3425 - val_neg_log_likelihood: 78.6704\n",
      "Epoch 11/20\n",
      " - 1s - loss: 82.3495 - squared_difference_loss: 36.8549 - KL_divergence_loss: 4.1910 - neg_log_likelihood: 78.1585 - val_loss: 83.0321 - val_squared_difference_loss: 37.7766 - val_KL_divergence_loss: 4.4128 - val_neg_log_likelihood: 78.6193\n",
      "Epoch 12/20\n",
      " - 1s - loss: 82.3516 - squared_difference_loss: 36.8108 - KL_divergence_loss: 4.2152 - neg_log_likelihood: 78.1364 - val_loss: 82.8631 - val_squared_difference_loss: 37.3249 - val_KL_divergence_loss: 4.4696 - val_neg_log_likelihood: 78.3934\n",
      "Epoch 13/20\n",
      " - 1s - loss: 82.2618 - squared_difference_loss: 36.5745 - KL_divergence_loss: 4.2435 - neg_log_likelihood: 78.0182 - val_loss: 82.8311 - val_squared_difference_loss: 37.2305 - val_KL_divergence_loss: 4.4849 - val_neg_log_likelihood: 78.3462\n",
      "Epoch 14/20\n",
      " - 1s - loss: 82.1884 - squared_difference_loss: 36.4939 - KL_divergence_loss: 4.2105 - neg_log_likelihood: 77.9780 - val_loss: 82.7305 - val_squared_difference_loss: 37.2191 - val_KL_divergence_loss: 4.3899 - val_neg_log_likelihood: 78.3406\n",
      "Epoch 15/20\n",
      " - 1s - loss: 82.1564 - squared_difference_loss: 36.3997 - KL_divergence_loss: 4.2255 - neg_log_likelihood: 77.9308 - val_loss: 82.5376 - val_squared_difference_loss: 37.0087 - val_KL_divergence_loss: 4.3022 - val_neg_log_likelihood: 78.2353\n",
      "Epoch 16/20\n",
      " - 1s - loss: 82.1215 - squared_difference_loss: 36.2755 - KL_divergence_loss: 4.2527 - neg_log_likelihood: 77.8688 - val_loss: 82.5532 - val_squared_difference_loss: 36.8677 - val_KL_divergence_loss: 4.3883 - val_neg_log_likelihood: 78.1649\n",
      "Epoch 17/20\n",
      " - 1s - loss: 82.1142 - squared_difference_loss: 36.2645 - KL_divergence_loss: 4.2510 - neg_log_likelihood: 77.8633 - val_loss: 82.4781 - val_squared_difference_loss: 36.7571 - val_KL_divergence_loss: 4.3685 - val_neg_log_likelihood: 78.1095\n",
      "Epoch 18/20\n",
      " - 1s - loss: 82.0269 - squared_difference_loss: 36.0778 - KL_divergence_loss: 4.2570 - neg_log_likelihood: 77.7699 - val_loss: 82.4046 - val_squared_difference_loss: 36.6193 - val_KL_divergence_loss: 4.3640 - val_neg_log_likelihood: 78.0406\n",
      "Epoch 19/20\n",
      " - 1s - loss: 81.9808 - squared_difference_loss: 35.9456 - KL_divergence_loss: 4.2770 - neg_log_likelihood: 77.7038 - val_loss: 82.3849 - val_squared_difference_loss: 36.3678 - val_KL_divergence_loss: 4.4700 - val_neg_log_likelihood: 77.9149\n",
      "Epoch 20/20\n",
      " - 1s - loss: 81.9236 - squared_difference_loss: 35.9449 - KL_divergence_loss: 4.2202 - neg_log_likelihood: 77.7035 - val_loss: 82.3226 - val_squared_difference_loss: 36.0691 - val_KL_divergence_loss: 4.5570 - val_neg_log_likelihood: 77.7656\n"
     ]
    }
   ],
   "source": [
    "n_z = 5 # Number of units in the latent variable representation\n",
    "n_h = 65  # Number of hidden units in encoder/decoder\n",
    "test_err, vae, encoder, generator, history = evaluate_vae_model(X_train, X_test, n_z, n_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph VAE latent outputs for a subset of patients X_test, add colors for output variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['icu_los_day', 'hospital_los_day', 'hosp_exp_flag', 'icu_exp_flag',\n",
       "       'day_28_flag', 'chf_flag', 'afib_flag', 'renal_flag', 'liver_flag',\n",
       "       'copd_flag', 'cad_flag', 'stroke_flag', 'malignancy_flag',\n",
       "       'respfail_flag', 'endocarditis_flag', 'ards_flag',\n",
       "       'pneumonia_flag'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f35e6954cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i is 999\n",
      "scatter plot drawn\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f35b2284978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i is 999\n",
      "scatter plot drawn\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f35b2b81f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i is 999\n",
      "scatter plot drawn\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f35adf34080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i is 999\n",
      "scatter plot drawn\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6701c5540ce0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mplot_labeled_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_points\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'figures/tsne_vanillaVAE_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/cs230_project/GMVAE_Experiments/utils.py\u001b[0m in \u001b[0;36mplot_labeled_data\u001b[0;34m(X, Y, file_name, tsne, perplexity)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mtrue_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanifold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mtrue_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \"\"\"\n\u001b[0;32m--> 858\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    768\u001b[0m                           \u001b[0mX_embedded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m                           \u001b[0mneighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneighbors_nn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m                           skip_num_points=skip_num_points)\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mP\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_exaggeration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         params, kl_divergence, it = _gradient_descent(obj_func, params,\n\u001b[0;32m--> 812\u001b[0;31m                                                       **opt_args)\n\u001b[0m\u001b[1;32m    813\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m             print(\"[t-SNE] KL divergence after %d iterations with early \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_kl_divergence_bh\u001b[0;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose)\u001b[0m\n\u001b[1;32m    245\u001b[0m     error = _barnes_hut_tsne.gradient(val_P, X_embedded, neighbors, indptr,\n\u001b[1;32m    246\u001b[0m                                       \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                                       dof=degrees_of_freedom)\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdegrees_of_freedom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f35b27f7b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "flag = 'hosp_exp_flag'\n",
    "n_points = 1000\n",
    "sys.path.append(\"/home/ubuntu/cs230_project/GMVAE_Experiments\")\n",
    "from utils import plot_labeled_data\n",
    "import sklearn.manifold\n",
    "latent_test = encoder.predict(X_train[:n_points])\n",
    "\n",
    "for flag in Y_test.columns.values:\n",
    "    plot_labeled_data(latent_test, np.array(Y_test[flag][:n_points]), 'figures/tsne_vanillaVAE_'+flag+'.png', True, perplexity=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Train Loss & Validation Loss vs. Number of Epochs to see our model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "ax.plot(history.history['squared_difference_loss'], label='Train Loss')\n",
    "ax.plot(history.history['val_squared_difference_loss'], label='Validation Loss')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "ax.set_ylabel('NELBO')\n",
    "ax.set_xlabel('# epochs')\n",
    "\n",
    "plt.title('VAE Loss vs. # of Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the relationship between the latent variables and the original variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vars = [c for c in X.columns if c not in outcome_vars]\n",
    "n_examples = 20000\n",
    "old_pats = X.iloc[:n_examples, :]\n",
    "zvals = encoder.predict(np.array(old_pats.drop(outcome_vars, axis=1)))\n",
    "# get some known patients.\n",
    "min_z = -3\n",
    "max_z = 3\n",
    "skip = 1\n",
    "\n",
    "latent_vars = {}\n",
    "latent_input_var_spectra = {}\n",
    "latent_outcome_var_spectra = {}\n",
    "print(\"n_z = \",n_z)\n",
    "for i in range(n_z):  # For each latent variable...\n",
    "    latent_name = \"z\" + str(i)\n",
    "    latent_vars[latent_name] = {}\n",
    "    latent_input_var_spectra[latent_name] = np.zeros(len(input_vars))\n",
    "    latent_outcome_var_spectra[latent_name] = np.zeros(len(outcome_vars))\n",
    "    layer = latent_vars[latent_name]\n",
    "    print(\"latent name\",latent_name)\n",
    "    for z in range(min_z, max_z, skip):  # ...fix the value of the latent variable (for a suite of values)\n",
    "        zvals[:, i] = z\n",
    "        new_pats = generator.predict(zvals)  # new_pats[0] is mu_x, new_pats[1] is sigma^2_x\n",
    "        new_z = np.ones(n_examples) * z\n",
    "        if \"x\" in layer:\n",
    "            layer[\"x\"] = np.concatenate((layer[\"x\"], new_pats[0]))\n",
    "        else:\n",
    "            layer[\"x\"] = new_pats[0]\n",
    "        if \"z\" in layer:\n",
    "            layer[\"z\"] = np.concatenate((layer[\"z\"], new_z), axis=0)\n",
    "        else:\n",
    "            layer[\"z\"] = new_z\n",
    "    # Take the correlation between the latent variable and all other original data variables,\n",
    "    # we'll call this the latent variable's \"fingerprint\" or \"spectrum\" and we want to compare the different latent vars\n",
    "    for j, real_varname in enumerate(input_vars):\n",
    "        latent_input_var_spectra[latent_name][j] = np.corrcoef(latent_vars[latent_name]['z'], latent_vars[latent_name]['x'][:, j])[0, 1]\n",
    "    for j, real_outcome_varname in enumerate(outcome_vars):\n",
    "        correlations = np.corrcoef(latent_vars[latent_name]['z'], latent_vars[latent_name]['x'][:, j])[0, 1]\n",
    "        latent_outcome_var_spectra[latent_name][j] = correlations\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the top n absolute value correlations for each latent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "for i in range(n_z):\n",
    "    latent_name = \"z\" + str(i)\n",
    "    print('\\nTop {} associated variables for Latent Variable {}:'.format(n, latent_name))\n",
    "    top_n_associated_indxs = np.argsort(latent_input_var_spectra[latent_name])[::-1][:n]\n",
    "    for association in zip(np.array(input_vars)[top_n_associated_indxs], \n",
    "                           latent_input_var_spectra[latent_name][top_n_associated_indxs]):\n",
    "        print(association)\n",
    "print('-----')   \n",
    "for i in range(n_z):\n",
    "    latent_name = \"z\" + str(i)\n",
    "    print('\\nTop {} associated variables for Latent Variable {}:'.format(n, latent_name))\n",
    "    top_n_associated_indxs = np.argsort(latent_input_var_spectra[latent_name])[::-1][:n]\n",
    "    for association in zip(np.array(outcome_vars)[top_n_associated_indxs], \n",
    "                           latent_input_var_spectra[latent_name][top_n_associated_indxs]):\n",
    "        print(association)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show correlations for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_z):  # For each latent variable...\n",
    "    plt.plot(latent_input_var_spectra[\"z\" + str(i)], label=\"z\" + str(i))\n",
    "plt.xlabel('Original Variable Index')\n",
    "plt.ylabel('Correlation with Latent Variable')\n",
    "plt.title('Latent Variable Correlations for Vanilla VAE')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(latent_input_var_spectra, index=X.columns).sort_values(by=['z0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each latent variable against each real value\n",
    "x_z_data = []  # {x: xvals, z: zvals, varname}\n",
    "fignum = 0\n",
    "for z_i in range(n_z):\n",
    "    latent_varname = \"z\" + str(z_i)\n",
    "    print(latent_varname)\n",
    "    for j, real_varname in enumerate(input_vars):\n",
    "        fignum += 1\n",
    "        title = latent_varname + \"_vs_\" + real_varname\n",
    "        x_data = latent_vars[latent_varname][\"x\"][:,j]\n",
    "        z_data = latent_vars[latent_varname][\"z\"]\n",
    "\n",
    "        plt.scatter(z_data, x_data)  # we control the hidden var, z.\n",
    "        plt.title(title)\n",
    "        plt.xlabel(latent_varname)\n",
    "        plt.ylabel(real_varname)\n",
    "        plt.savefig(\"figures/title\"+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>weight_first</th>\n",
       "      <th>height_first</th>\n",
       "      <th>bmi</th>\n",
       "      <th>sofa_first</th>\n",
       "      <th>heartrate_min</th>\n",
       "      <th>heartrate_max</th>\n",
       "      <th>heartrate_mean</th>\n",
       "      <th>sysbp_min</th>\n",
       "      <th>sysbp_max</th>\n",
       "      <th>...</th>\n",
       "      <th>inr_min</th>\n",
       "      <th>inr_max</th>\n",
       "      <th>pt_min</th>\n",
       "      <th>pt_max</th>\n",
       "      <th>sodium_min</th>\n",
       "      <th>sodium_max</th>\n",
       "      <th>bun_min</th>\n",
       "      <th>bun_max</th>\n",
       "      <th>wbc_min</th>\n",
       "      <th>wbc_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.174367</td>\n",
       "      <td>-0.873550</td>\n",
       "      <td>0.083629</td>\n",
       "      <td>-0.986812</td>\n",
       "      <td>0.593058</td>\n",
       "      <td>1.114613</td>\n",
       "      <td>1.453867</td>\n",
       "      <td>0.973430</td>\n",
       "      <td>-0.168020</td>\n",
       "      <td>-1.294207</td>\n",
       "      <td>...</td>\n",
       "      <td>1.720931</td>\n",
       "      <td>0.600923</td>\n",
       "      <td>2.215430</td>\n",
       "      <td>0.980729</td>\n",
       "      <td>0.485343</td>\n",
       "      <td>-0.238332</td>\n",
       "      <td>2.951934</td>\n",
       "      <td>2.392836</td>\n",
       "      <td>-0.979649</td>\n",
       "      <td>-0.976192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.912443</td>\n",
       "      <td>-0.127044</td>\n",
       "      <td>0.159537</td>\n",
       "      <td>-0.237994</td>\n",
       "      <td>0.593058</td>\n",
       "      <td>0.057324</td>\n",
       "      <td>0.879022</td>\n",
       "      <td>0.885848</td>\n",
       "      <td>-0.928996</td>\n",
       "      <td>-0.343922</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100969</td>\n",
       "      <td>0.458634</td>\n",
       "      <td>-0.134198</td>\n",
       "      <td>0.639881</td>\n",
       "      <td>0.686075</td>\n",
       "      <td>0.844105</td>\n",
       "      <td>-0.252647</td>\n",
       "      <td>-0.324815</td>\n",
       "      <td>0.549150</td>\n",
       "      <td>2.729527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.578724</td>\n",
       "      <td>0.094646</td>\n",
       "      <td>-0.371818</td>\n",
       "      <td>-0.225810</td>\n",
       "      <td>-1.029837</td>\n",
       "      <td>-0.603482</td>\n",
       "      <td>-0.941321</td>\n",
       "      <td>-0.724069</td>\n",
       "      <td>-0.168020</td>\n",
       "      <td>-1.294207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.252794</td>\n",
       "      <td>-0.323959</td>\n",
       "      <td>-0.292156</td>\n",
       "      <td>-0.372008</td>\n",
       "      <td>-0.317582</td>\n",
       "      <td>-0.671307</td>\n",
       "      <td>-0.553076</td>\n",
       "      <td>-0.631647</td>\n",
       "      <td>-0.814029</td>\n",
       "      <td>0.090332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.198488</td>\n",
       "      <td>2.067231</td>\n",
       "      <td>0.652937</td>\n",
       "      <td>1.511980</td>\n",
       "      <td>-1.029837</td>\n",
       "      <td>0.585969</td>\n",
       "      <td>0.016754</td>\n",
       "      <td>0.546532</td>\n",
       "      <td>-0.222375</td>\n",
       "      <td>-0.219972</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404619</td>\n",
       "      <td>-0.323959</td>\n",
       "      <td>-0.331646</td>\n",
       "      <td>-0.340054</td>\n",
       "      <td>-0.317582</td>\n",
       "      <td>-0.021845</td>\n",
       "      <td>-0.803434</td>\n",
       "      <td>-0.675480</td>\n",
       "      <td>-0.329910</td>\n",
       "      <td>-0.226010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.955160</td>\n",
       "      <td>0.311811</td>\n",
       "      <td>-0.675449</td>\n",
       "      <td>0.740424</td>\n",
       "      <td>-0.380679</td>\n",
       "      <td>1.114613</td>\n",
       "      <td>0.112562</td>\n",
       "      <td>0.655540</td>\n",
       "      <td>-0.331086</td>\n",
       "      <td>-0.798406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404619</td>\n",
       "      <td>-0.110524</td>\n",
       "      <td>-0.390880</td>\n",
       "      <td>-0.073767</td>\n",
       "      <td>-0.317582</td>\n",
       "      <td>0.627617</td>\n",
       "      <td>-0.452933</td>\n",
       "      <td>-0.543981</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.054179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  weight_first  height_first       bmi  sofa_first  heartrate_min  \\\n",
       "0 -0.174367     -0.873550      0.083629 -0.986812    0.593058       1.114613   \n",
       "1 -0.912443     -0.127044      0.159537 -0.237994    0.593058       0.057324   \n",
       "2 -0.578724      0.094646     -0.371818 -0.225810   -1.029837      -0.603482   \n",
       "3 -1.198488      2.067231      0.652937  1.511980   -1.029837       0.585969   \n",
       "4 -0.955160      0.311811     -0.675449  0.740424   -0.380679       1.114613   \n",
       "\n",
       "   heartrate_max  heartrate_mean  sysbp_min  sysbp_max    ...      inr_min  \\\n",
       "0       1.453867        0.973430  -0.168020  -1.294207    ...     1.720931   \n",
       "1       0.879022        0.885848  -0.928996  -0.343922    ...    -0.100969   \n",
       "2      -0.941321       -0.724069  -0.168020  -1.294207    ...    -0.252794   \n",
       "3       0.016754        0.546532  -0.222375  -0.219972    ...    -0.404619   \n",
       "4       0.112562        0.655540  -0.331086  -0.798406    ...    -0.404619   \n",
       "\n",
       "    inr_max    pt_min    pt_max  sodium_min  sodium_max   bun_min   bun_max  \\\n",
       "0  0.600923  2.215430  0.980729    0.485343   -0.238332  2.951934  2.392836   \n",
       "1  0.458634 -0.134198  0.639881    0.686075    0.844105 -0.252647 -0.324815   \n",
       "2 -0.323959 -0.292156 -0.372008   -0.317582   -0.671307 -0.553076 -0.631647   \n",
       "3 -0.323959 -0.331646 -0.340054   -0.317582   -0.021845 -0.803434 -0.675480   \n",
       "4 -0.110524 -0.390880 -0.073767   -0.317582    0.627617 -0.452933 -0.543981   \n",
       "\n",
       "    wbc_min   wbc_max  \n",
       "0 -0.979649 -0.976192  \n",
       "1  0.549150  2.729527  \n",
       "2 -0.814029  0.090332  \n",
       "3 -0.329910 -0.226010  \n",
       "4  0.001330  0.054179  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[X.columns.values[:65]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>weight_first</th>\n",
       "      <th>height_first</th>\n",
       "      <th>bmi</th>\n",
       "      <th>sofa_first</th>\n",
       "      <th>heartrate_min</th>\n",
       "      <th>heartrate_max</th>\n",
       "      <th>heartrate_mean</th>\n",
       "      <th>sysbp_min</th>\n",
       "      <th>sysbp_max</th>\n",
       "      <th>...</th>\n",
       "      <th>inr_min</th>\n",
       "      <th>inr_max</th>\n",
       "      <th>pt_min</th>\n",
       "      <th>pt_max</th>\n",
       "      <th>sodium_min</th>\n",
       "      <th>sodium_max</th>\n",
       "      <th>bun_min</th>\n",
       "      <th>bun_max</th>\n",
       "      <th>wbc_min</th>\n",
       "      <th>wbc_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.174367</td>\n",
       "      <td>-0.873550</td>\n",
       "      <td>0.083629</td>\n",
       "      <td>-0.986812</td>\n",
       "      <td>0.593058</td>\n",
       "      <td>1.114613</td>\n",
       "      <td>1.453867</td>\n",
       "      <td>0.973430</td>\n",
       "      <td>-0.168020</td>\n",
       "      <td>-1.294207</td>\n",
       "      <td>...</td>\n",
       "      <td>1.720931</td>\n",
       "      <td>0.600923</td>\n",
       "      <td>2.215430</td>\n",
       "      <td>0.980729</td>\n",
       "      <td>0.485343</td>\n",
       "      <td>-0.238332</td>\n",
       "      <td>2.951934</td>\n",
       "      <td>2.392836</td>\n",
       "      <td>-0.979649</td>\n",
       "      <td>-0.976192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.912443</td>\n",
       "      <td>-0.127044</td>\n",
       "      <td>0.159537</td>\n",
       "      <td>-0.237994</td>\n",
       "      <td>0.593058</td>\n",
       "      <td>0.057324</td>\n",
       "      <td>0.879022</td>\n",
       "      <td>0.885848</td>\n",
       "      <td>-0.928996</td>\n",
       "      <td>-0.343922</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100969</td>\n",
       "      <td>0.458634</td>\n",
       "      <td>-0.134198</td>\n",
       "      <td>0.639881</td>\n",
       "      <td>0.686075</td>\n",
       "      <td>0.844105</td>\n",
       "      <td>-0.252647</td>\n",
       "      <td>-0.324815</td>\n",
       "      <td>0.549150</td>\n",
       "      <td>2.729527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.578724</td>\n",
       "      <td>0.094646</td>\n",
       "      <td>-0.371818</td>\n",
       "      <td>-0.225810</td>\n",
       "      <td>-1.029837</td>\n",
       "      <td>-0.603482</td>\n",
       "      <td>-0.941321</td>\n",
       "      <td>-0.724069</td>\n",
       "      <td>-0.168020</td>\n",
       "      <td>-1.294207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.252794</td>\n",
       "      <td>-0.323959</td>\n",
       "      <td>-0.292156</td>\n",
       "      <td>-0.372008</td>\n",
       "      <td>-0.317582</td>\n",
       "      <td>-0.671307</td>\n",
       "      <td>-0.553076</td>\n",
       "      <td>-0.631647</td>\n",
       "      <td>-0.814029</td>\n",
       "      <td>0.090332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.198488</td>\n",
       "      <td>2.067231</td>\n",
       "      <td>0.652937</td>\n",
       "      <td>1.511980</td>\n",
       "      <td>-1.029837</td>\n",
       "      <td>0.585969</td>\n",
       "      <td>0.016754</td>\n",
       "      <td>0.546532</td>\n",
       "      <td>-0.222375</td>\n",
       "      <td>-0.219972</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404619</td>\n",
       "      <td>-0.323959</td>\n",
       "      <td>-0.331646</td>\n",
       "      <td>-0.340054</td>\n",
       "      <td>-0.317582</td>\n",
       "      <td>-0.021845</td>\n",
       "      <td>-0.803434</td>\n",
       "      <td>-0.675480</td>\n",
       "      <td>-0.329910</td>\n",
       "      <td>-0.226010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.955160</td>\n",
       "      <td>0.311811</td>\n",
       "      <td>-0.675449</td>\n",
       "      <td>0.740424</td>\n",
       "      <td>-0.380679</td>\n",
       "      <td>1.114613</td>\n",
       "      <td>0.112562</td>\n",
       "      <td>0.655540</td>\n",
       "      <td>-0.331086</td>\n",
       "      <td>-0.798406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404619</td>\n",
       "      <td>-0.110524</td>\n",
       "      <td>-0.390880</td>\n",
       "      <td>-0.073767</td>\n",
       "      <td>-0.317582</td>\n",
       "      <td>0.627617</td>\n",
       "      <td>-0.452933</td>\n",
       "      <td>-0.543981</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.054179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  weight_first  height_first       bmi  sofa_first  heartrate_min  \\\n",
       "0 -0.174367     -0.873550      0.083629 -0.986812    0.593058       1.114613   \n",
       "1 -0.912443     -0.127044      0.159537 -0.237994    0.593058       0.057324   \n",
       "2 -0.578724      0.094646     -0.371818 -0.225810   -1.029837      -0.603482   \n",
       "3 -1.198488      2.067231      0.652937  1.511980   -1.029837       0.585969   \n",
       "4 -0.955160      0.311811     -0.675449  0.740424   -0.380679       1.114613   \n",
       "\n",
       "   heartrate_max  heartrate_mean  sysbp_min  sysbp_max    ...      inr_min  \\\n",
       "0       1.453867        0.973430  -0.168020  -1.294207    ...     1.720931   \n",
       "1       0.879022        0.885848  -0.928996  -0.343922    ...    -0.100969   \n",
       "2      -0.941321       -0.724069  -0.168020  -1.294207    ...    -0.252794   \n",
       "3       0.016754        0.546532  -0.222375  -0.219972    ...    -0.404619   \n",
       "4       0.112562        0.655540  -0.331086  -0.798406    ...    -0.404619   \n",
       "\n",
       "    inr_max    pt_min    pt_max  sodium_min  sodium_max   bun_min   bun_max  \\\n",
       "0  0.600923  2.215430  0.980729    0.485343   -0.238332  2.951934  2.392836   \n",
       "1  0.458634 -0.134198  0.639881    0.686075    0.844105 -0.252647 -0.324815   \n",
       "2 -0.323959 -0.292156 -0.372008   -0.317582   -0.671307 -0.553076 -0.631647   \n",
       "3 -0.323959 -0.331646 -0.340054   -0.317582   -0.021845 -0.803434 -0.675480   \n",
       "4 -0.110524 -0.390880 -0.073767   -0.317582    0.627617 -0.452933 -0.543981   \n",
       "\n",
       "    wbc_min   wbc_max  \n",
       "0 -0.979649 -0.976192  \n",
       "1  0.549150  2.729527  \n",
       "2 -0.814029  0.090332  \n",
       "3 -0.329910 -0.226010  \n",
       "4  0.001330  0.054179  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[X.columns.values[:65]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent Var Num = 3\n",
      "Hidden Var Num = 10\n",
      "Simulation #1: VAE with 3 Latent Variable(s) and 10 Hidden Variable(s)...\n",
      "\tEvaluating model on Fold 1\n",
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/50\n",
      " - 12s - loss: 104.7182 - squared_difference_loss: 84.8179 - KL_divergence_loss: 2.5782 - neg_log_likelihood: 102.1399 - val_loss: 91.5247 - val_squared_difference_loss: 58.8726 - val_KL_divergence_loss: 2.3574 - val_neg_log_likelihood: 89.1673\n",
      "Epoch 2/50\n",
      " - 1s - loss: 89.1371 - squared_difference_loss: 56.2116 - KL_divergence_loss: 1.3003 - neg_log_likelihood: 87.8368 - val_loss: 87.9612 - val_squared_difference_loss: 52.9963 - val_KL_divergence_loss: 1.7321 - val_neg_log_likelihood: 86.2292\n",
      "Epoch 3/50\n",
      " - 1s - loss: 87.3067 - squared_difference_loss: 52.3294 - KL_divergence_loss: 1.4111 - neg_log_likelihood: 85.8957 - val_loss: 86.8326 - val_squared_difference_loss: 49.4326 - val_KL_divergence_loss: 2.3853 - val_neg_log_likelihood: 84.4473\n",
      "Epoch 4/50\n",
      " - 1s - loss: 86.3690 - squared_difference_loss: 49.7295 - KL_divergence_loss: 1.7733 - neg_log_likelihood: 84.5958 - val_loss: 86.0449 - val_squared_difference_loss: 47.4686 - val_KL_divergence_loss: 2.5796 - val_neg_log_likelihood: 83.4653\n",
      "Epoch 5/50\n",
      " - 1s - loss: 85.7976 - squared_difference_loss: 47.9884 - KL_divergence_loss: 2.0724 - neg_log_likelihood: 83.7252 - val_loss: 85.4760 - val_squared_difference_loss: 46.2753 - val_KL_divergence_loss: 2.6074 - val_neg_log_likelihood: 82.8687\n",
      "Epoch 6/50\n",
      " - 1s - loss: 85.5104 - squared_difference_loss: 47.1474 - KL_divergence_loss: 2.2057 - neg_log_likelihood: 83.3047 - val_loss: 85.1933 - val_squared_difference_loss: 45.7872 - val_KL_divergence_loss: 2.5687 - val_neg_log_likelihood: 82.6246\n",
      "Epoch 7/50\n",
      " - 1s - loss: 85.3281 - squared_difference_loss: 46.6360 - KL_divergence_loss: 2.2791 - neg_log_likelihood: 83.0490 - val_loss: 85.0350 - val_squared_difference_loss: 45.5862 - val_KL_divergence_loss: 2.5109 - val_neg_log_likelihood: 82.5241\n",
      "Epoch 8/50\n",
      " - 1s - loss: 85.2239 - squared_difference_loss: 46.3197 - KL_divergence_loss: 2.3331 - neg_log_likelihood: 82.8908 - val_loss: 84.9301 - val_squared_difference_loss: 45.4507 - val_KL_divergence_loss: 2.4738 - val_neg_log_likelihood: 82.4564\n",
      "Epoch 9/50\n",
      " - 1s - loss: 85.1217 - squared_difference_loss: 46.0599 - KL_divergence_loss: 2.3608 - neg_log_likelihood: 82.7609 - val_loss: 84.8705 - val_squared_difference_loss: 45.2807 - val_KL_divergence_loss: 2.4991 - val_neg_log_likelihood: 82.3714\n",
      "Epoch 10/50\n",
      " - 1s - loss: 85.0704 - squared_difference_loss: 45.8693 - KL_divergence_loss: 2.4047 - neg_log_likelihood: 82.6656 - val_loss: 84.7864 - val_squared_difference_loss: 45.2295 - val_KL_divergence_loss: 2.4406 - val_neg_log_likelihood: 82.3457\n",
      "Epoch 11/50\n",
      " - 1s - loss: 85.0127 - squared_difference_loss: 45.6944 - KL_divergence_loss: 2.4344 - neg_log_likelihood: 82.5782 - val_loss: 84.7778 - val_squared_difference_loss: 45.0163 - val_KL_divergence_loss: 2.5387 - val_neg_log_likelihood: 82.2392\n",
      "Epoch 12/50\n",
      " - 1s - loss: 84.9890 - squared_difference_loss: 45.5851 - KL_divergence_loss: 2.4655 - neg_log_likelihood: 82.5235 - val_loss: 84.7844 - val_squared_difference_loss: 45.1261 - val_KL_divergence_loss: 2.4903 - val_neg_log_likelihood: 82.2941\n",
      "Epoch 13/50\n",
      " - 1s - loss: 84.9450 - squared_difference_loss: 45.4406 - KL_divergence_loss: 2.4937 - neg_log_likelihood: 82.4513 - val_loss: 84.8024 - val_squared_difference_loss: 44.9587 - val_KL_divergence_loss: 2.5920 - val_neg_log_likelihood: 82.2104\n",
      "Epoch 14/50\n",
      " - 1s - loss: 84.9267 - squared_difference_loss: 45.3949 - KL_divergence_loss: 2.4983 - neg_log_likelihood: 82.4284 - val_loss: 84.7536 - val_squared_difference_loss: 44.9647 - val_KL_divergence_loss: 2.5403 - val_neg_log_likelihood: 82.2133\n",
      "Epoch 15/50\n",
      " - 1s - loss: 84.8923 - squared_difference_loss: 45.3104 - KL_divergence_loss: 2.5061 - neg_log_likelihood: 82.3862 - val_loss: 84.7275 - val_squared_difference_loss: 44.8335 - val_KL_divergence_loss: 2.5798 - val_neg_log_likelihood: 82.1477\n",
      "Epoch 16/50\n",
      " - 1s - loss: 84.8491 - squared_difference_loss: 45.1956 - KL_divergence_loss: 2.5203 - neg_log_likelihood: 82.3288 - val_loss: 84.7999 - val_squared_difference_loss: 45.0265 - val_KL_divergence_loss: 2.5557 - val_neg_log_likelihood: 82.2442\n",
      "Epoch 17/50\n",
      " - 1s - loss: 84.8088 - squared_difference_loss: 45.1294 - KL_divergence_loss: 2.5131 - neg_log_likelihood: 82.2957 - val_loss: 84.8393 - val_squared_difference_loss: 45.1052 - val_KL_divergence_loss: 2.5557 - val_neg_log_likelihood: 82.2836\n",
      "Epoch 18/50\n",
      " - 1s - loss: 84.7851 - squared_difference_loss: 45.0207 - KL_divergence_loss: 2.5438 - neg_log_likelihood: 82.2414 - val_loss: 84.7576 - val_squared_difference_loss: 44.9130 - val_KL_divergence_loss: 2.5701 - val_neg_log_likelihood: 82.1875\n",
      "Epoch 19/50\n",
      " - 1s - loss: 84.7356 - squared_difference_loss: 44.9169 - KL_divergence_loss: 2.5461 - neg_log_likelihood: 82.1895 - val_loss: 84.7445 - val_squared_difference_loss: 44.7467 - val_KL_divergence_loss: 2.6402 - val_neg_log_likelihood: 82.1044\n",
      "Epoch 20/50\n",
      " - 1s - loss: 84.7127 - squared_difference_loss: 44.8242 - KL_divergence_loss: 2.5696 - neg_log_likelihood: 82.1431 - val_loss: 84.6700 - val_squared_difference_loss: 44.6681 - val_KL_divergence_loss: 2.6049 - val_neg_log_likelihood: 82.0651\n",
      "Epoch 21/50\n",
      " - 1s - loss: 84.6693 - squared_difference_loss: 44.7122 - KL_divergence_loss: 2.5822 - neg_log_likelihood: 82.0871 - val_loss: 84.5617 - val_squared_difference_loss: 44.3701 - val_KL_divergence_loss: 2.6456 - val_neg_log_likelihood: 81.9160\n",
      "Epoch 22/50\n",
      " - 1s - loss: 84.6428 - squared_difference_loss: 44.6431 - KL_divergence_loss: 2.5903 - neg_log_likelihood: 82.0525 - val_loss: 84.6537 - val_squared_difference_loss: 44.4695 - val_KL_divergence_loss: 2.6880 - val_neg_log_likelihood: 81.9658\n",
      "Epoch 23/50\n",
      " - 1s - loss: 84.5978 - squared_difference_loss: 44.5283 - KL_divergence_loss: 2.6026 - neg_log_likelihood: 81.9951 - val_loss: 84.5838 - val_squared_difference_loss: 44.2178 - val_KL_divergence_loss: 2.7439 - val_neg_log_likelihood: 81.8399\n",
      "Epoch 24/50\n",
      " - 1s - loss: 84.5524 - squared_difference_loss: 44.3927 - KL_divergence_loss: 2.6251 - neg_log_likelihood: 81.9274 - val_loss: 84.4798 - val_squared_difference_loss: 44.2487 - val_KL_divergence_loss: 2.6244 - val_neg_log_likelihood: 81.8554\n",
      "Epoch 25/50\n",
      " - 1s - loss: 84.5176 - squared_difference_loss: 44.3734 - KL_divergence_loss: 2.5999 - neg_log_likelihood: 81.9177 - val_loss: 84.4920 - val_squared_difference_loss: 44.0179 - val_KL_divergence_loss: 2.7520 - val_neg_log_likelihood: 81.7400\n",
      "Epoch 26/50\n",
      " - 1s - loss: 84.5046 - squared_difference_loss: 44.3139 - KL_divergence_loss: 2.6166 - neg_log_likelihood: 81.8880 - val_loss: 84.5950 - val_squared_difference_loss: 44.3140 - val_KL_divergence_loss: 2.7069 - val_neg_log_likelihood: 81.8880\n",
      "Epoch 27/50\n",
      " - 1s - loss: 84.4844 - squared_difference_loss: 44.2569 - KL_divergence_loss: 2.6250 - neg_log_likelihood: 81.8594 - val_loss: 84.6496 - val_squared_difference_loss: 44.4216 - val_KL_divergence_loss: 2.7078 - val_neg_log_likelihood: 81.9418\n",
      "Epoch 28/50\n",
      " - 1s - loss: 84.4475 - squared_difference_loss: 44.2041 - KL_divergence_loss: 2.6144 - neg_log_likelihood: 81.8331 - val_loss: 84.6450 - val_squared_difference_loss: 44.2629 - val_KL_divergence_loss: 2.7826 - val_neg_log_likelihood: 81.8624\n",
      "Epoch 29/50\n",
      " - 1s - loss: 84.4330 - squared_difference_loss: 44.1094 - KL_divergence_loss: 2.6473 - neg_log_likelihood: 81.7857 - val_loss: 84.7833 - val_squared_difference_loss: 44.6946 - val_KL_divergence_loss: 2.7050 - val_neg_log_likelihood: 82.0783\n",
      "Epoch 30/50\n",
      " - 1s - loss: 84.3733 - squared_difference_loss: 43.9971 - KL_divergence_loss: 2.6437 - neg_log_likelihood: 81.7295 - val_loss: 84.8623 - val_squared_difference_loss: 44.8986 - val_KL_divergence_loss: 2.6820 - val_neg_log_likelihood: 82.1803\n",
      "Epoch 31/50\n",
      " - 1s - loss: 84.3726 - squared_difference_loss: 43.9923 - KL_divergence_loss: 2.6454 - neg_log_likelihood: 81.7272 - val_loss: 84.5322 - val_squared_difference_loss: 44.0824 - val_KL_divergence_loss: 2.7600 - val_neg_log_likelihood: 81.7722\n",
      "Epoch 32/50\n",
      " - 1s - loss: 84.3343 - squared_difference_loss: 43.9192 - KL_divergence_loss: 2.6437 - neg_log_likelihood: 81.6906 - val_loss: 84.9202 - val_squared_difference_loss: 44.8617 - val_KL_divergence_loss: 2.7584 - val_neg_log_likelihood: 82.1619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n",
      " - 1s - loss: 84.3043 - squared_difference_loss: 43.8837 - KL_divergence_loss: 2.6314 - neg_log_likelihood: 81.6729 - val_loss: 84.7724 - val_squared_difference_loss: 44.5041 - val_KL_divergence_loss: 2.7894 - val_neg_log_likelihood: 81.9831\n",
      "Epoch 34/50\n",
      " - 1s - loss: 84.2601 - squared_difference_loss: 43.6978 - KL_divergence_loss: 2.6802 - neg_log_likelihood: 81.5799 - val_loss: 84.8037 - val_squared_difference_loss: 44.8554 - val_KL_divergence_loss: 2.6450 - val_neg_log_likelihood: 82.1587\n",
      "\tAverage Validation Squared Error = 82.13622222597867\n",
      "Latent Var Num = 3\n",
      "Hidden Var Num = 20\n",
      "Simulation #2: VAE with 3 Latent Variable(s) and 20 Hidden Variable(s)...\n",
      "\tEvaluating model on Fold 1\n",
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/50\n",
      " - 12s - loss: 100.0771 - squared_difference_loss: 76.2332 - KL_divergence_loss: 2.2295 - neg_log_likelihood: 97.8476 - val_loss: 88.6045 - val_squared_difference_loss: 53.3908 - val_KL_divergence_loss: 2.1781 - val_neg_log_likelihood: 86.4264\n",
      "Epoch 2/50\n",
      " - 1s - loss: 87.3475 - squared_difference_loss: 51.5714 - KL_divergence_loss: 1.8308 - neg_log_likelihood: 85.5167 - val_loss: 86.1711 - val_squared_difference_loss: 47.4990 - val_KL_divergence_loss: 2.6906 - val_neg_log_likelihood: 83.4805\n",
      "Epoch 3/50\n",
      " - 1s - loss: 85.8826 - squared_difference_loss: 47.5485 - KL_divergence_loss: 2.3774 - neg_log_likelihood: 83.5052 - val_loss: 85.2226 - val_squared_difference_loss: 45.1207 - val_KL_divergence_loss: 2.9312 - val_neg_log_likelihood: 82.2914\n",
      "Epoch 4/50\n",
      " - 1s - loss: 85.2950 - squared_difference_loss: 46.0209 - KL_divergence_loss: 2.5535 - neg_log_likelihood: 82.7414 - val_loss: 84.8268 - val_squared_difference_loss: 44.1136 - val_KL_divergence_loss: 3.0390 - val_neg_log_likelihood: 81.7878\n",
      "Epoch 5/50\n",
      " - 1s - loss: 84.9926 - squared_difference_loss: 45.2228 - KL_divergence_loss: 2.6502 - neg_log_likelihood: 82.3424 - val_loss: 84.4991 - val_squared_difference_loss: 43.4574 - val_KL_divergence_loss: 3.0393 - val_neg_log_likelihood: 81.4597\n",
      "Epoch 6/50\n",
      " - 1s - loss: 84.7935 - squared_difference_loss: 44.6844 - KL_divergence_loss: 2.7203 - neg_log_likelihood: 82.0732 - val_loss: 84.3638 - val_squared_difference_loss: 43.2283 - val_KL_divergence_loss: 3.0186 - val_neg_log_likelihood: 81.3452\n",
      "Epoch 7/50\n",
      " - 1s - loss: 84.6594 - squared_difference_loss: 44.3648 - KL_divergence_loss: 2.7460 - neg_log_likelihood: 81.9134 - val_loss: 84.2924 - val_squared_difference_loss: 43.1599 - val_KL_divergence_loss: 2.9814 - val_neg_log_likelihood: 81.3110\n",
      "Epoch 8/50\n",
      " - 1s - loss: 84.5079 - squared_difference_loss: 44.0247 - KL_divergence_loss: 2.7646 - neg_log_likelihood: 81.7434 - val_loss: 84.1004 - val_squared_difference_loss: 42.8767 - val_KL_divergence_loss: 2.9310 - val_neg_log_likelihood: 81.1694\n",
      "Epoch 9/50\n",
      " - 1s - loss: 84.3944 - squared_difference_loss: 43.7425 - KL_divergence_loss: 2.7922 - neg_log_likelihood: 81.6022 - val_loss: 83.9659 - val_squared_difference_loss: 42.5636 - val_KL_divergence_loss: 2.9531 - val_neg_log_likelihood: 81.0128\n",
      "Epoch 10/50\n",
      " - 1s - loss: 84.2986 - squared_difference_loss: 43.5226 - KL_divergence_loss: 2.8063 - neg_log_likelihood: 81.4923 - val_loss: 83.8863 - val_squared_difference_loss: 42.4467 - val_KL_divergence_loss: 2.9319 - val_neg_log_likelihood: 80.9544\n",
      "Epoch 11/50\n",
      " - 1s - loss: 84.1820 - squared_difference_loss: 43.2410 - KL_divergence_loss: 2.8304 - neg_log_likelihood: 81.3515 - val_loss: 83.8392 - val_squared_difference_loss: 42.3530 - val_KL_divergence_loss: 2.9317 - val_neg_log_likelihood: 80.9075\n",
      "Epoch 12/50\n",
      " - 1s - loss: 84.1084 - squared_difference_loss: 43.0093 - KL_divergence_loss: 2.8728 - neg_log_likelihood: 81.2357 - val_loss: 83.7276 - val_squared_difference_loss: 41.9296 - val_KL_divergence_loss: 3.0318 - val_neg_log_likelihood: 80.6958\n",
      "Epoch 13/50\n",
      " - 1s - loss: 84.0118 - squared_difference_loss: 42.8016 - KL_divergence_loss: 2.8800 - neg_log_likelihood: 81.1318 - val_loss: 83.7083 - val_squared_difference_loss: 41.9332 - val_KL_divergence_loss: 3.0107 - val_neg_log_likelihood: 80.6976\n",
      "Epoch 14/50\n",
      " - 1s - loss: 83.9431 - squared_difference_loss: 42.6161 - KL_divergence_loss: 2.9041 - neg_log_likelihood: 81.0391 - val_loss: 83.6748 - val_squared_difference_loss: 41.7058 - val_KL_divergence_loss: 3.0909 - val_neg_log_likelihood: 80.5839\n",
      "Epoch 15/50\n",
      " - 1s - loss: 83.8843 - squared_difference_loss: 42.4983 - KL_divergence_loss: 2.9041 - neg_log_likelihood: 80.9802 - val_loss: 83.5732 - val_squared_difference_loss: 41.5840 - val_KL_divergence_loss: 3.0502 - val_neg_log_likelihood: 80.5230\n",
      "Epoch 16/50\n",
      " - 1s - loss: 83.8722 - squared_difference_loss: 42.4158 - KL_divergence_loss: 2.9333 - neg_log_likelihood: 80.9389 - val_loss: 83.5617 - val_squared_difference_loss: 41.7346 - val_KL_divergence_loss: 2.9634 - val_neg_log_likelihood: 80.5983\n",
      "Epoch 17/50\n",
      " - 1s - loss: 83.8040 - squared_difference_loss: 42.2700 - KL_divergence_loss: 2.9380 - neg_log_likelihood: 80.8660 - val_loss: 83.4077 - val_squared_difference_loss: 41.4284 - val_KL_divergence_loss: 2.9625 - val_neg_log_likelihood: 80.4452\n",
      "Epoch 18/50\n",
      " - 1s - loss: 83.7663 - squared_difference_loss: 42.1938 - KL_divergence_loss: 2.9384 - neg_log_likelihood: 80.8279 - val_loss: 83.3727 - val_squared_difference_loss: 41.3385 - val_KL_divergence_loss: 2.9725 - val_neg_log_likelihood: 80.4002\n",
      "Epoch 19/50\n",
      " - 1s - loss: 83.7446 - squared_difference_loss: 42.1195 - KL_divergence_loss: 2.9539 - neg_log_likelihood: 80.7907 - val_loss: 83.3520 - val_squared_difference_loss: 41.2984 - val_KL_divergence_loss: 2.9718 - val_neg_log_likelihood: 80.3802\n",
      "Epoch 20/50\n",
      " - 1s - loss: 83.7049 - squared_difference_loss: 42.0825 - KL_divergence_loss: 2.9326 - neg_log_likelihood: 80.7723 - val_loss: 83.3818 - val_squared_difference_loss: 41.1327 - val_KL_divergence_loss: 3.0844 - val_neg_log_likelihood: 80.2974\n",
      "Epoch 21/50\n",
      " - 1s - loss: 83.6799 - squared_difference_loss: 41.9629 - KL_divergence_loss: 2.9674 - neg_log_likelihood: 80.7125 - val_loss: 83.3161 - val_squared_difference_loss: 41.1889 - val_KL_divergence_loss: 2.9907 - val_neg_log_likelihood: 80.3254\n",
      "Epoch 22/50\n",
      " - 1s - loss: 83.6785 - squared_difference_loss: 41.9515 - KL_divergence_loss: 2.9717 - neg_log_likelihood: 80.7068 - val_loss: 83.3651 - val_squared_difference_loss: 41.3090 - val_KL_divergence_loss: 2.9796 - val_neg_log_likelihood: 80.3855\n",
      "Epoch 23/50\n",
      " - 1s - loss: 83.6699 - squared_difference_loss: 41.9355 - KL_divergence_loss: 2.9712 - neg_log_likelihood: 80.6987 - val_loss: 83.3337 - val_squared_difference_loss: 41.1005 - val_KL_divergence_loss: 3.0525 - val_neg_log_likelihood: 80.2813\n",
      "Epoch 24/50\n",
      " - 1s - loss: 83.6185 - squared_difference_loss: 41.8167 - KL_divergence_loss: 2.9791 - neg_log_likelihood: 80.6394 - val_loss: 83.2852 - val_squared_difference_loss: 40.8415 - val_KL_divergence_loss: 3.1335 - val_neg_log_likelihood: 80.1518\n",
      "Epoch 25/50\n",
      " - 1s - loss: 83.6051 - squared_difference_loss: 41.8189 - KL_divergence_loss: 2.9647 - neg_log_likelihood: 80.6405 - val_loss: 83.3289 - val_squared_difference_loss: 40.8667 - val_KL_divergence_loss: 3.1646 - val_neg_log_likelihood: 80.1644\n",
      "Epoch 26/50\n",
      " - 1s - loss: 83.5866 - squared_difference_loss: 41.7513 - KL_divergence_loss: 2.9800 - neg_log_likelihood: 80.6067 - val_loss: 83.2304 - val_squared_difference_loss: 40.7389 - val_KL_divergence_loss: 3.1299 - val_neg_log_likelihood: 80.1005\n",
      "Epoch 27/50\n",
      " - 1s - loss: 83.5974 - squared_difference_loss: 41.7418 - KL_divergence_loss: 2.9955 - neg_log_likelihood: 80.6019 - val_loss: 83.2140 - val_squared_difference_loss: 40.8542 - val_KL_divergence_loss: 3.0559 - val_neg_log_likelihood: 80.1581\n",
      "Epoch 28/50\n",
      " - 1s - loss: 83.5145 - squared_difference_loss: 41.6260 - KL_divergence_loss: 2.9706 - neg_log_likelihood: 80.5440 - val_loss: 83.1885 - val_squared_difference_loss: 40.8408 - val_KL_divergence_loss: 3.0371 - val_neg_log_likelihood: 80.1514\n",
      "Epoch 29/50\n",
      " - 1s - loss: 83.5438 - squared_difference_loss: 41.6558 - KL_divergence_loss: 2.9849 - neg_log_likelihood: 80.5589 - val_loss: 83.1901 - val_squared_difference_loss: 40.6786 - val_KL_divergence_loss: 3.1198 - val_neg_log_likelihood: 80.0703\n",
      "Epoch 30/50\n",
      " - 1s - loss: 83.5117 - squared_difference_loss: 41.5673 - KL_divergence_loss: 2.9970 - neg_log_likelihood: 80.5147 - val_loss: 83.1691 - val_squared_difference_loss: 40.5490 - val_KL_divergence_loss: 3.1636 - val_neg_log_likelihood: 80.0055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      " - 1s - loss: 83.5029 - squared_difference_loss: 41.5290 - KL_divergence_loss: 3.0074 - neg_log_likelihood: 80.4955 - val_loss: 83.2954 - val_squared_difference_loss: 41.0098 - val_KL_divergence_loss: 3.0595 - val_neg_log_likelihood: 80.2359\n",
      "Epoch 32/50\n",
      " - 1s - loss: 83.5138 - squared_difference_loss: 41.5984 - KL_divergence_loss: 2.9836 - neg_log_likelihood: 80.5302 - val_loss: 83.1316 - val_squared_difference_loss: 40.5967 - val_KL_divergence_loss: 3.1022 - val_neg_log_likelihood: 80.0294\n",
      "Epoch 33/50\n",
      " - 1s - loss: 83.4891 - squared_difference_loss: 41.4918 - KL_divergence_loss: 3.0122 - neg_log_likelihood: 80.4769 - val_loss: 83.1332 - val_squared_difference_loss: 40.6652 - val_KL_divergence_loss: 3.0695 - val_neg_log_likelihood: 80.0636\n",
      "Epoch 34/50\n",
      " - 1s - loss: 83.4540 - squared_difference_loss: 41.4873 - KL_divergence_loss: 2.9794 - neg_log_likelihood: 80.4747 - val_loss: 83.1325 - val_squared_difference_loss: 40.5564 - val_KL_divergence_loss: 3.1233 - val_neg_log_likelihood: 80.0092\n",
      "Epoch 35/50\n",
      " - 1s - loss: 83.4527 - squared_difference_loss: 41.4377 - KL_divergence_loss: 3.0028 - neg_log_likelihood: 80.4499 - val_loss: 83.1374 - val_squared_difference_loss: 40.5356 - val_KL_divergence_loss: 3.1386 - val_neg_log_likelihood: 79.9988\n",
      "Epoch 36/50\n",
      " - 1s - loss: 83.4620 - squared_difference_loss: 41.4174 - KL_divergence_loss: 3.0223 - neg_log_likelihood: 80.4397 - val_loss: 83.1763 - val_squared_difference_loss: 40.5554 - val_KL_divergence_loss: 3.1676 - val_neg_log_likelihood: 80.0087\n",
      "Epoch 37/50\n",
      " - 1s - loss: 83.4428 - squared_difference_loss: 41.3874 - KL_divergence_loss: 3.0181 - neg_log_likelihood: 80.4247 - val_loss: 83.1389 - val_squared_difference_loss: 40.5863 - val_KL_divergence_loss: 3.1148 - val_neg_log_likelihood: 80.0241\n",
      "Epoch 38/50\n",
      " - 1s - loss: 83.4108 - squared_difference_loss: 41.3108 - KL_divergence_loss: 3.0244 - neg_log_likelihood: 80.3864 - val_loss: 83.0781 - val_squared_difference_loss: 40.4297 - val_KL_divergence_loss: 3.1322 - val_neg_log_likelihood: 79.9459\n",
      "Epoch 39/50\n",
      " - 1s - loss: 83.4209 - squared_difference_loss: 41.3647 - KL_divergence_loss: 3.0075 - neg_log_likelihood: 80.4134 - val_loss: 83.2084 - val_squared_difference_loss: 40.8337 - val_KL_divergence_loss: 3.0606 - val_neg_log_likelihood: 80.1479\n",
      "Epoch 40/50\n",
      " - 1s - loss: 83.4040 - squared_difference_loss: 41.2745 - KL_divergence_loss: 3.0358 - neg_log_likelihood: 80.3682 - val_loss: 83.0445 - val_squared_difference_loss: 40.5383 - val_KL_divergence_loss: 3.0444 - val_neg_log_likelihood: 80.0001\n",
      "Epoch 41/50\n",
      " - 1s - loss: 83.3706 - squared_difference_loss: 41.2769 - KL_divergence_loss: 3.0012 - neg_log_likelihood: 80.3695 - val_loss: 83.1025 - val_squared_difference_loss: 40.4993 - val_KL_divergence_loss: 3.1218 - val_neg_log_likelihood: 79.9806\n",
      "Epoch 42/50\n",
      " - 1s - loss: 83.3436 - squared_difference_loss: 41.1985 - KL_divergence_loss: 3.0133 - neg_log_likelihood: 80.3302 - val_loss: 83.0093 - val_squared_difference_loss: 40.5360 - val_KL_divergence_loss: 3.0103 - val_neg_log_likelihood: 79.9990\n",
      "Epoch 43/50\n",
      " - 1s - loss: 83.3650 - squared_difference_loss: 41.2284 - KL_divergence_loss: 3.0198 - neg_log_likelihood: 80.3452 - val_loss: 83.1031 - val_squared_difference_loss: 40.6809 - val_KL_divergence_loss: 3.0317 - val_neg_log_likelihood: 80.0715\n",
      "Epoch 44/50\n",
      " - 1s - loss: 83.3458 - squared_difference_loss: 41.1746 - KL_divergence_loss: 3.0275 - neg_log_likelihood: 80.3183 - val_loss: 83.0756 - val_squared_difference_loss: 40.7905 - val_KL_divergence_loss: 2.9493 - val_neg_log_likelihood: 80.1263\n",
      "Epoch 45/50\n",
      " - 1s - loss: 83.3297 - squared_difference_loss: 41.1517 - KL_divergence_loss: 3.0229 - neg_log_likelihood: 80.3069 - val_loss: 83.0549 - val_squared_difference_loss: 40.5293 - val_KL_divergence_loss: 3.0593 - val_neg_log_likelihood: 79.9956\n",
      "Epoch 46/50\n",
      " - 1s - loss: 83.3299 - squared_difference_loss: 41.0801 - KL_divergence_loss: 3.0588 - neg_log_likelihood: 80.2710 - val_loss: 83.0742 - val_squared_difference_loss: 40.6076 - val_KL_divergence_loss: 3.0394 - val_neg_log_likelihood: 80.0348\n",
      "Epoch 47/50\n",
      " - 1s - loss: 83.2656 - squared_difference_loss: 40.9816 - KL_divergence_loss: 3.0438 - neg_log_likelihood: 80.2218 - val_loss: 83.0280 - val_squared_difference_loss: 40.3199 - val_KL_divergence_loss: 3.1371 - val_neg_log_likelihood: 79.8909\n",
      "Epoch 48/50\n",
      " - 1s - loss: 83.2622 - squared_difference_loss: 40.9898 - KL_divergence_loss: 3.0363 - neg_log_likelihood: 80.2259 - val_loss: 83.0074 - val_squared_difference_loss: 40.5526 - val_KL_divergence_loss: 3.0001 - val_neg_log_likelihood: 80.0073\n",
      "Epoch 49/50\n",
      " - 1s - loss: 83.2693 - squared_difference_loss: 40.9740 - KL_divergence_loss: 3.0513 - neg_log_likelihood: 80.2180 - val_loss: 82.9938 - val_squared_difference_loss: 40.4510 - val_KL_divergence_loss: 3.0373 - val_neg_log_likelihood: 79.9565\n",
      "Epoch 50/50\n",
      " - 1s - loss: 83.2289 - squared_difference_loss: 40.9169 - KL_divergence_loss: 3.0394 - neg_log_likelihood: 80.1894 - val_loss: 83.0289 - val_squared_difference_loss: 40.4550 - val_KL_divergence_loss: 3.0704 - val_neg_log_likelihood: 79.9585\n",
      "\tAverage Validation Squared Error = 79.12155779191744\n",
      "Latent Var Num = 3\n",
      "Hidden Var Num = 30\n",
      "Simulation #3: VAE with 3 Latent Variable(s) and 30 Hidden Variable(s)...\n",
      "\tEvaluating model on Fold 1\n",
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 98.6874 - squared_difference_loss: 73.4731 - KL_divergence_loss: 2.2198 - neg_log_likelihood: 96.4676 - val_loss: 87.6696 - val_squared_difference_loss: 50.8355 - val_KL_divergence_loss: 2.5209 - val_neg_log_likelihood: 85.1488\n",
      "Epoch 2/50\n",
      " - 1s - loss: 86.4030 - squared_difference_loss: 49.1009 - KL_divergence_loss: 2.1216 - neg_log_likelihood: 84.2815 - val_loss: 85.5059 - val_squared_difference_loss: 45.6255 - val_KL_divergence_loss: 2.9621 - val_neg_log_likelihood: 82.5438\n",
      "Epoch 3/50\n",
      " - 1s - loss: 85.2709 - squared_difference_loss: 46.1228 - KL_divergence_loss: 2.4786 - neg_log_likelihood: 82.7924 - val_loss: 84.7301 - val_squared_difference_loss: 44.0759 - val_KL_divergence_loss: 2.9611 - val_neg_log_likelihood: 81.7690\n",
      "Epoch 4/50\n",
      " - 1s - loss: 84.8315 - squared_difference_loss: 44.9658 - KL_divergence_loss: 2.6176 - neg_log_likelihood: 82.2139 - val_loss: 84.4133 - val_squared_difference_loss: 43.4191 - val_KL_divergence_loss: 2.9728 - val_neg_log_likelihood: 81.4406\n",
      "Epoch 5/50\n",
      " - 1s - loss: 84.5845 - squared_difference_loss: 44.3574 - KL_divergence_loss: 2.6748 - neg_log_likelihood: 81.9097 - val_loss: 84.1753 - val_squared_difference_loss: 42.7270 - val_KL_divergence_loss: 3.0808 - val_neg_log_likelihood: 81.0945\n",
      "Epoch 6/50\n",
      " - 1s - loss: 84.3994 - squared_difference_loss: 43.8566 - KL_divergence_loss: 2.7401 - neg_log_likelihood: 81.6593 - val_loss: 83.9940 - val_squared_difference_loss: 42.3952 - val_KL_divergence_loss: 3.0654 - val_neg_log_likelihood: 80.9286\n",
      "Epoch 7/50\n",
      " - 1s - loss: 84.2531 - squared_difference_loss: 43.5262 - KL_divergence_loss: 2.7590 - neg_log_likelihood: 81.4941 - val_loss: 83.9147 - val_squared_difference_loss: 42.1762 - val_KL_divergence_loss: 3.0956 - val_neg_log_likelihood: 80.8191\n",
      "Epoch 8/50\n",
      " - 1s - loss: 84.1209 - squared_difference_loss: 43.1582 - KL_divergence_loss: 2.8108 - neg_log_likelihood: 81.3101 - val_loss: 83.9206 - val_squared_difference_loss: 42.3135 - val_KL_divergence_loss: 3.0328 - val_neg_log_likelihood: 80.8878\n",
      "Epoch 9/50\n",
      " - 1s - loss: 84.0994 - squared_difference_loss: 43.0763 - KL_divergence_loss: 2.8303 - neg_log_likelihood: 81.2691 - val_loss: 83.6811 - val_squared_difference_loss: 41.8156 - val_KL_divergence_loss: 3.0423 - val_neg_log_likelihood: 80.6388\n",
      "Epoch 10/50\n",
      " - 1s - loss: 84.0237 - squared_difference_loss: 42.9463 - KL_divergence_loss: 2.8196 - neg_log_likelihood: 81.2042 - val_loss: 83.7069 - val_squared_difference_loss: 41.6904 - val_KL_divergence_loss: 3.1307 - val_neg_log_likelihood: 80.5762\n",
      "Epoch 11/50\n",
      " - 1s - loss: 83.9696 - squared_difference_loss: 42.7829 - KL_divergence_loss: 2.8472 - neg_log_likelihood: 81.1224 - val_loss: 83.5851 - val_squared_difference_loss: 41.7639 - val_KL_divergence_loss: 2.9722 - val_neg_log_likelihood: 80.6129\n",
      "Epoch 12/50\n",
      " - 1s - loss: 83.8817 - squared_difference_loss: 42.6144 - KL_divergence_loss: 2.8435 - neg_log_likelihood: 81.0382 - val_loss: 83.5364 - val_squared_difference_loss: 41.3506 - val_KL_divergence_loss: 3.1301 - val_neg_log_likelihood: 80.4063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      " - 1s - loss: 83.8675 - squared_difference_loss: 42.5261 - KL_divergence_loss: 2.8735 - neg_log_likelihood: 80.9940 - val_loss: 83.5454 - val_squared_difference_loss: 41.3930 - val_KL_divergence_loss: 3.1179 - val_neg_log_likelihood: 80.4275\n",
      "Epoch 14/50\n",
      " - 1s - loss: 83.8107 - squared_difference_loss: 42.4240 - KL_divergence_loss: 2.8677 - neg_log_likelihood: 80.9430 - val_loss: 83.5180 - val_squared_difference_loss: 41.3877 - val_KL_divergence_loss: 3.0931 - val_neg_log_likelihood: 80.4249\n",
      "Epoch 15/50\n",
      " - 1s - loss: 83.7686 - squared_difference_loss: 42.3502 - KL_divergence_loss: 2.8624 - neg_log_likelihood: 80.9061 - val_loss: 83.5006 - val_squared_difference_loss: 41.6086 - val_KL_divergence_loss: 2.9653 - val_neg_log_likelihood: 80.5353\n",
      "Epoch 16/50\n",
      " - 1s - loss: 83.7262 - squared_difference_loss: 42.2706 - KL_divergence_loss: 2.8599 - neg_log_likelihood: 80.8663 - val_loss: 83.4755 - val_squared_difference_loss: 41.3607 - val_KL_divergence_loss: 3.0642 - val_neg_log_likelihood: 80.4114\n",
      "Epoch 17/50\n",
      " - 1s - loss: 83.6748 - squared_difference_loss: 42.1297 - KL_divergence_loss: 2.8789 - neg_log_likelihood: 80.7959 - val_loss: 83.6134 - val_squared_difference_loss: 41.8188 - val_KL_divergence_loss: 2.9730 - val_neg_log_likelihood: 80.6404\n",
      "Epoch 18/50\n",
      " - 1s - loss: 83.6234 - squared_difference_loss: 42.0481 - KL_divergence_loss: 2.8683 - neg_log_likelihood: 80.7550 - val_loss: 83.4134 - val_squared_difference_loss: 41.5802 - val_KL_divergence_loss: 2.8923 - val_neg_log_likelihood: 80.5211\n",
      "Epoch 19/50\n",
      " - 1s - loss: 83.6354 - squared_difference_loss: 41.9976 - KL_divergence_loss: 2.9056 - neg_log_likelihood: 80.7298 - val_loss: 83.3594 - val_squared_difference_loss: 41.2756 - val_KL_divergence_loss: 2.9906 - val_neg_log_likelihood: 80.3688\n",
      "Epoch 20/50\n",
      " - 1s - loss: 83.5556 - squared_difference_loss: 41.8684 - KL_divergence_loss: 2.8904 - neg_log_likelihood: 80.6652 - val_loss: 83.3726 - val_squared_difference_loss: 41.4817 - val_KL_divergence_loss: 2.9008 - val_neg_log_likelihood: 80.4719\n",
      "Epoch 21/50\n",
      " - 1s - loss: 83.4827 - squared_difference_loss: 41.6730 - KL_divergence_loss: 2.9152 - neg_log_likelihood: 80.5675 - val_loss: 83.2338 - val_squared_difference_loss: 41.1290 - val_KL_divergence_loss: 2.9383 - val_neg_log_likelihood: 80.2955\n",
      "Epoch 22/50\n",
      " - 1s - loss: 83.4557 - squared_difference_loss: 41.6167 - KL_divergence_loss: 2.9163 - neg_log_likelihood: 80.5394 - val_loss: 83.3990 - val_squared_difference_loss: 41.0592 - val_KL_divergence_loss: 3.1383 - val_neg_log_likelihood: 80.2606\n",
      "Epoch 23/50\n",
      " - 1s - loss: 83.4320 - squared_difference_loss: 41.5207 - KL_divergence_loss: 2.9407 - neg_log_likelihood: 80.4913 - val_loss: 83.3877 - val_squared_difference_loss: 41.4698 - val_KL_divergence_loss: 2.9218 - val_neg_log_likelihood: 80.4659\n",
      "Epoch 24/50\n",
      " - 1s - loss: 83.3377 - squared_difference_loss: 41.3330 - KL_divergence_loss: 2.9402 - neg_log_likelihood: 80.3975 - val_loss: 83.2807 - val_squared_difference_loss: 41.2059 - val_KL_divergence_loss: 2.9468 - val_neg_log_likelihood: 80.3339\n",
      "Epoch 25/50\n",
      " - 1s - loss: 83.3250 - squared_difference_loss: 41.3185 - KL_divergence_loss: 2.9347 - neg_log_likelihood: 80.3902 - val_loss: 83.3002 - val_squared_difference_loss: 40.8185 - val_KL_divergence_loss: 3.1599 - val_neg_log_likelihood: 80.1402\n",
      "Epoch 26/50\n",
      " - 1s - loss: 83.2992 - squared_difference_loss: 41.2219 - KL_divergence_loss: 2.9572 - neg_log_likelihood: 80.3420 - val_loss: 83.3763 - val_squared_difference_loss: 41.0768 - val_KL_divergence_loss: 3.1069 - val_neg_log_likelihood: 80.2694\n",
      "Epoch 27/50\n",
      " - 1s - loss: 83.3035 - squared_difference_loss: 41.2123 - KL_divergence_loss: 2.9663 - neg_log_likelihood: 80.3372 - val_loss: 83.1783 - val_squared_difference_loss: 40.7619 - val_KL_divergence_loss: 3.0663 - val_neg_log_likelihood: 80.1120\n",
      "Epoch 28/50\n",
      " - 1s - loss: 83.2195 - squared_difference_loss: 41.0467 - KL_divergence_loss: 2.9652 - neg_log_likelihood: 80.2543 - val_loss: 83.1443 - val_squared_difference_loss: 40.7785 - val_KL_divergence_loss: 3.0240 - val_neg_log_likelihood: 80.1203\n",
      "Epoch 29/50\n",
      " - 1s - loss: 83.2224 - squared_difference_loss: 41.0584 - KL_divergence_loss: 2.9622 - neg_log_likelihood: 80.2602 - val_loss: 83.2976 - val_squared_difference_loss: 40.9975 - val_KL_divergence_loss: 3.0678 - val_neg_log_likelihood: 80.2298\n",
      "Epoch 30/50\n",
      " - 1s - loss: 83.1713 - squared_difference_loss: 40.9190 - KL_divergence_loss: 2.9809 - neg_log_likelihood: 80.1905 - val_loss: 83.1214 - val_squared_difference_loss: 40.6294 - val_KL_divergence_loss: 3.0757 - val_neg_log_likelihood: 80.0457\n",
      "Epoch 31/50\n",
      " - 1s - loss: 83.1385 - squared_difference_loss: 40.8494 - KL_divergence_loss: 2.9828 - neg_log_likelihood: 80.1557 - val_loss: 83.1416 - val_squared_difference_loss: 40.7488 - val_KL_divergence_loss: 3.0362 - val_neg_log_likelihood: 80.1054\n",
      "Epoch 32/50\n",
      " - 1s - loss: 83.1373 - squared_difference_loss: 40.8557 - KL_divergence_loss: 2.9785 - neg_log_likelihood: 80.1588 - val_loss: 83.0885 - val_squared_difference_loss: 40.6072 - val_KL_divergence_loss: 3.0539 - val_neg_log_likelihood: 80.0346\n",
      "Epoch 33/50\n",
      " - 1s - loss: 83.1196 - squared_difference_loss: 40.8395 - KL_divergence_loss: 2.9688 - neg_log_likelihood: 80.1507 - val_loss: 83.0836 - val_squared_difference_loss: 40.5245 - val_KL_divergence_loss: 3.0903 - val_neg_log_likelihood: 79.9933\n",
      "Epoch 34/50\n",
      " - 1s - loss: 83.1010 - squared_difference_loss: 40.7687 - KL_divergence_loss: 2.9856 - neg_log_likelihood: 80.1154 - val_loss: 83.0041 - val_squared_difference_loss: 40.3898 - val_KL_divergence_loss: 3.0782 - val_neg_log_likelihood: 79.9259\n",
      "Epoch 35/50\n",
      " - 1s - loss: 83.0852 - squared_difference_loss: 40.7242 - KL_divergence_loss: 2.9921 - neg_log_likelihood: 80.0931 - val_loss: 83.0308 - val_squared_difference_loss: 40.4060 - val_KL_divergence_loss: 3.0968 - val_neg_log_likelihood: 79.9340\n",
      "Epoch 36/50\n",
      " - 1s - loss: 83.0905 - squared_difference_loss: 40.6994 - KL_divergence_loss: 3.0098 - neg_log_likelihood: 80.0807 - val_loss: 83.0480 - val_squared_difference_loss: 40.5076 - val_KL_divergence_loss: 3.0632 - val_neg_log_likelihood: 79.9848\n",
      "Epoch 37/50\n",
      " - 1s - loss: 83.0994 - squared_difference_loss: 40.7014 - KL_divergence_loss: 3.0177 - neg_log_likelihood: 80.0817 - val_loss: 82.9972 - val_squared_difference_loss: 40.3862 - val_KL_divergence_loss: 3.0731 - val_neg_log_likelihood: 79.9241\n",
      "Epoch 38/50\n",
      " - 1s - loss: 83.0826 - squared_difference_loss: 40.6398 - KL_divergence_loss: 3.0317 - neg_log_likelihood: 80.0509 - val_loss: 82.9104 - val_squared_difference_loss: 40.1283 - val_KL_divergence_loss: 3.1153 - val_neg_log_likelihood: 79.7952\n",
      "Epoch 39/50\n",
      " - 1s - loss: 83.0575 - squared_difference_loss: 40.6225 - KL_divergence_loss: 3.0153 - neg_log_likelihood: 80.0423 - val_loss: 82.9123 - val_squared_difference_loss: 40.2492 - val_KL_divergence_loss: 3.0567 - val_neg_log_likelihood: 79.8556\n",
      "Epoch 40/50\n",
      " - 1s - loss: 83.0677 - squared_difference_loss: 40.6151 - KL_divergence_loss: 3.0292 - neg_log_likelihood: 80.0386 - val_loss: 82.8735 - val_squared_difference_loss: 39.9755 - val_KL_divergence_loss: 3.1547 - val_neg_log_likelihood: 79.7187\n",
      "Epoch 41/50\n",
      " - 1s - loss: 83.0333 - squared_difference_loss: 40.5425 - KL_divergence_loss: 3.0311 - neg_log_likelihood: 80.0022 - val_loss: 82.9819 - val_squared_difference_loss: 40.3659 - val_KL_divergence_loss: 3.0679 - val_neg_log_likelihood: 79.9139\n",
      "Epoch 42/50\n",
      " - 1s - loss: 82.9916 - squared_difference_loss: 40.4845 - KL_divergence_loss: 3.0184 - neg_log_likelihood: 79.9733 - val_loss: 82.9397 - val_squared_difference_loss: 40.1916 - val_KL_divergence_loss: 3.1129 - val_neg_log_likelihood: 79.8268\n",
      "Epoch 43/50\n",
      " - 1s - loss: 83.0564 - squared_difference_loss: 40.6117 - KL_divergence_loss: 3.0196 - neg_log_likelihood: 80.0368 - val_loss: 82.8436 - val_squared_difference_loss: 39.9866 - val_KL_divergence_loss: 3.1193 - val_neg_log_likelihood: 79.7243\n",
      "Epoch 44/50\n",
      " - 1s - loss: 83.0199 - squared_difference_loss: 40.5137 - KL_divergence_loss: 3.0320 - neg_log_likelihood: 79.9879 - val_loss: 82.8641 - val_squared_difference_loss: 39.9530 - val_KL_divergence_loss: 3.1565 - val_neg_log_likelihood: 79.7075\n",
      "Epoch 45/50\n",
      " - 1s - loss: 83.0257 - squared_difference_loss: 40.5207 - KL_divergence_loss: 3.0344 - neg_log_likelihood: 79.9913 - val_loss: 82.9787 - val_squared_difference_loss: 39.8534 - val_KL_divergence_loss: 3.3210 - val_neg_log_likelihood: 79.6577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50\n",
      " - 1s - loss: 83.0176 - squared_difference_loss: 40.4763 - KL_divergence_loss: 3.0485 - neg_log_likelihood: 79.9691 - val_loss: 82.8475 - val_squared_difference_loss: 39.8062 - val_KL_divergence_loss: 3.2135 - val_neg_log_likelihood: 79.6341\n",
      "Epoch 47/50\n",
      " - 1s - loss: 82.9762 - squared_difference_loss: 40.3781 - KL_divergence_loss: 3.0562 - neg_log_likelihood: 79.9200 - val_loss: 82.8298 - val_squared_difference_loss: 40.0713 - val_KL_divergence_loss: 3.0631 - val_neg_log_likelihood: 79.7667\n",
      "Epoch 48/50\n",
      " - 1s - loss: 82.9715 - squared_difference_loss: 40.4042 - KL_divergence_loss: 3.0384 - neg_log_likelihood: 79.9331 - val_loss: 82.8466 - val_squared_difference_loss: 39.9647 - val_KL_divergence_loss: 3.1332 - val_neg_log_likelihood: 79.7134\n",
      "Epoch 49/50\n",
      " - 1s - loss: 82.9820 - squared_difference_loss: 40.3660 - KL_divergence_loss: 3.0679 - neg_log_likelihood: 79.9140 - val_loss: 82.8915 - val_squared_difference_loss: 40.2285 - val_KL_divergence_loss: 3.0463 - val_neg_log_likelihood: 79.8452\n",
      "Epoch 50/50\n",
      " - 1s - loss: 82.9615 - squared_difference_loss: 40.3417 - KL_divergence_loss: 3.0596 - neg_log_likelihood: 79.9019 - val_loss: 82.8388 - val_squared_difference_loss: 40.0077 - val_KL_divergence_loss: 3.1040 - val_neg_log_likelihood: 79.7349\n",
      "\tAverage Validation Squared Error = 78.64848257554382\n",
      "Latent Var Num = 3\n",
      "Hidden Var Num = 40\n",
      "Simulation #4: VAE with 3 Latent Variable(s) and 40 Hidden Variable(s)...\n",
      "\tEvaluating model on Fold 1\n",
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 94.8705 - squared_difference_loss: 66.0158 - KL_divergence_loss: 2.1316 - neg_log_likelihood: 92.7389 - val_loss: 86.1619 - val_squared_difference_loss: 47.3896 - val_KL_divergence_loss: 2.7360 - val_neg_log_likelihood: 83.4258\n",
      "Epoch 2/50\n",
      " - 1s - loss: 85.4426 - squared_difference_loss: 46.6225 - KL_divergence_loss: 2.4003 - neg_log_likelihood: 83.0423 - val_loss: 84.6487 - val_squared_difference_loss: 43.5168 - val_KL_divergence_loss: 3.1593 - val_neg_log_likelihood: 81.4894\n",
      "Epoch 3/50\n",
      " - 1s - loss: 84.6682 - squared_difference_loss: 44.5735 - KL_divergence_loss: 2.6504 - neg_log_likelihood: 82.0178 - val_loss: 84.1510 - val_squared_difference_loss: 42.3234 - val_KL_divergence_loss: 3.2583 - val_neg_log_likelihood: 80.8927\n",
      "Epoch 4/50\n",
      " - 1s - loss: 84.4051 - squared_difference_loss: 43.6915 - KL_divergence_loss: 2.8284 - neg_log_likelihood: 81.5767 - val_loss: 83.9786 - val_squared_difference_loss: 41.7413 - val_KL_divergence_loss: 3.3770 - val_neg_log_likelihood: 80.6016\n",
      "Epoch 5/50\n",
      " - 1s - loss: 84.2528 - squared_difference_loss: 43.2999 - KL_divergence_loss: 2.8718 - neg_log_likelihood: 81.3810 - val_loss: 83.8981 - val_squared_difference_loss: 41.8539 - val_KL_divergence_loss: 3.2402 - val_neg_log_likelihood: 80.6579\n",
      "Epoch 6/50\n",
      " - 1s - loss: 84.1472 - squared_difference_loss: 43.0373 - KL_divergence_loss: 2.8975 - neg_log_likelihood: 81.2497 - val_loss: 83.8143 - val_squared_difference_loss: 41.7251 - val_KL_divergence_loss: 3.2207 - val_neg_log_likelihood: 80.5935\n",
      "Epoch 7/50\n",
      " - 1s - loss: 84.0922 - squared_difference_loss: 42.8949 - KL_divergence_loss: 2.9138 - neg_log_likelihood: 81.1784 - val_loss: 83.7138 - val_squared_difference_loss: 41.5168 - val_KL_divergence_loss: 3.2244 - val_neg_log_likelihood: 80.4894\n",
      "Epoch 8/50\n",
      " - 1s - loss: 84.0239 - squared_difference_loss: 42.7060 - KL_divergence_loss: 2.9399 - neg_log_likelihood: 81.0840 - val_loss: 83.6665 - val_squared_difference_loss: 41.6305 - val_KL_divergence_loss: 3.1203 - val_neg_log_likelihood: 80.5462\n",
      "Epoch 9/50\n",
      " - 1s - loss: 83.9328 - squared_difference_loss: 42.5526 - KL_divergence_loss: 2.9255 - neg_log_likelihood: 81.0073 - val_loss: 83.6963 - val_squared_difference_loss: 41.7064 - val_KL_divergence_loss: 3.1121 - val_neg_log_likelihood: 80.5842\n",
      "Epoch 10/50\n",
      " - 1s - loss: 83.8731 - squared_difference_loss: 42.3660 - KL_divergence_loss: 2.9591 - neg_log_likelihood: 80.9140 - val_loss: 83.7061 - val_squared_difference_loss: 41.8046 - val_KL_divergence_loss: 3.0728 - val_neg_log_likelihood: 80.6333\n",
      "Epoch 11/50\n",
      " - 1s - loss: 83.7925 - squared_difference_loss: 42.2126 - KL_divergence_loss: 2.9552 - neg_log_likelihood: 80.8373 - val_loss: 83.7341 - val_squared_difference_loss: 41.8532 - val_KL_divergence_loss: 3.0765 - val_neg_log_likelihood: 80.6576\n",
      "Epoch 12/50\n",
      " - 1s - loss: 83.7467 - squared_difference_loss: 42.1623 - KL_divergence_loss: 2.9345 - neg_log_likelihood: 80.8122 - val_loss: 83.6148 - val_squared_difference_loss: 41.5206 - val_KL_divergence_loss: 3.1235 - val_neg_log_likelihood: 80.4913\n",
      "Epoch 13/50\n",
      " - 1s - loss: 83.6613 - squared_difference_loss: 41.9895 - KL_divergence_loss: 2.9355 - neg_log_likelihood: 80.7258 - val_loss: 83.6489 - val_squared_difference_loss: 41.4425 - val_KL_divergence_loss: 3.1966 - val_neg_log_likelihood: 80.4523\n",
      "Epoch 14/50\n",
      " - 1s - loss: 83.6368 - squared_difference_loss: 41.8403 - KL_divergence_loss: 2.9857 - neg_log_likelihood: 80.6511 - val_loss: 83.5968 - val_squared_difference_loss: 41.5770 - val_KL_divergence_loss: 3.0773 - val_neg_log_likelihood: 80.5195\n",
      "Epoch 15/50\n",
      " - 1s - loss: 83.5431 - squared_difference_loss: 41.7063 - KL_divergence_loss: 2.9590 - neg_log_likelihood: 80.5842 - val_loss: 83.4488 - val_squared_difference_loss: 41.0448 - val_KL_divergence_loss: 3.1954 - val_neg_log_likelihood: 80.2534\n",
      "Epoch 16/50\n",
      " - 1s - loss: 83.4674 - squared_difference_loss: 41.4949 - KL_divergence_loss: 2.9889 - neg_log_likelihood: 80.4785 - val_loss: 83.4594 - val_squared_difference_loss: 41.2810 - val_KL_divergence_loss: 3.0880 - val_neg_log_likelihood: 80.3715\n",
      "Epoch 17/50\n",
      " - 1s - loss: 83.4348 - squared_difference_loss: 41.4425 - KL_divergence_loss: 2.9825 - neg_log_likelihood: 80.4523 - val_loss: 83.4235 - val_squared_difference_loss: 41.3279 - val_KL_divergence_loss: 3.0285 - val_neg_log_likelihood: 80.3950\n",
      "Epoch 18/50\n",
      " - 1s - loss: 83.4290 - squared_difference_loss: 41.4289 - KL_divergence_loss: 2.9836 - neg_log_likelihood: 80.4454 - val_loss: 83.2775 - val_squared_difference_loss: 41.1438 - val_KL_divergence_loss: 2.9746 - val_neg_log_likelihood: 80.3029\n",
      "Epoch 19/50\n",
      " - 1s - loss: 83.4033 - squared_difference_loss: 41.3613 - KL_divergence_loss: 2.9916 - neg_log_likelihood: 80.4117 - val_loss: 83.1725 - val_squared_difference_loss: 40.8090 - val_KL_divergence_loss: 3.0370 - val_neg_log_likelihood: 80.1355\n",
      "Epoch 20/50\n",
      " - 1s - loss: 83.3578 - squared_difference_loss: 41.2280 - KL_divergence_loss: 3.0128 - neg_log_likelihood: 80.3450 - val_loss: 83.2525 - val_squared_difference_loss: 40.8902 - val_KL_divergence_loss: 3.0764 - val_neg_log_likelihood: 80.1761\n",
      "Epoch 21/50\n",
      " - 1s - loss: 83.3544 - squared_difference_loss: 41.2074 - KL_divergence_loss: 3.0197 - neg_log_likelihood: 80.3347 - val_loss: 83.1286 - val_squared_difference_loss: 40.7480 - val_KL_divergence_loss: 3.0236 - val_neg_log_likelihood: 80.1050\n",
      "Epoch 22/50\n",
      " - 1s - loss: 83.3315 - squared_difference_loss: 41.1381 - KL_divergence_loss: 3.0314 - neg_log_likelihood: 80.3000 - val_loss: 83.1432 - val_squared_difference_loss: 40.5955 - val_KL_divergence_loss: 3.1144 - val_neg_log_likelihood: 80.0288\n",
      "Epoch 23/50\n",
      " - 1s - loss: 83.2942 - squared_difference_loss: 41.0109 - KL_divergence_loss: 3.0577 - neg_log_likelihood: 80.2365 - val_loss: 83.0766 - val_squared_difference_loss: 40.6781 - val_KL_divergence_loss: 3.0066 - val_neg_log_likelihood: 80.0701\n",
      "Epoch 24/50\n",
      " - 1s - loss: 83.2283 - squared_difference_loss: 40.9461 - KL_divergence_loss: 3.0243 - neg_log_likelihood: 80.2041 - val_loss: 82.9789 - val_squared_difference_loss: 40.2112 - val_KL_divergence_loss: 3.1423 - val_neg_log_likelihood: 79.8366\n",
      "Epoch 25/50\n",
      " - 1s - loss: 83.2167 - squared_difference_loss: 40.8878 - KL_divergence_loss: 3.0418 - neg_log_likelihood: 80.1749 - val_loss: 83.0193 - val_squared_difference_loss: 40.3211 - val_KL_divergence_loss: 3.1278 - val_neg_log_likelihood: 79.8915\n",
      "Epoch 26/50\n",
      " - 1s - loss: 83.2531 - squared_difference_loss: 40.9077 - KL_divergence_loss: 3.0683 - neg_log_likelihood: 80.1849 - val_loss: 83.0258 - val_squared_difference_loss: 40.2919 - val_KL_divergence_loss: 3.1489 - val_neg_log_likelihood: 79.8769\n",
      "Epoch 27/50\n",
      " - 1s - loss: 83.1722 - squared_difference_loss: 40.7477 - KL_divergence_loss: 3.0674 - neg_log_likelihood: 80.1048 - val_loss: 83.0127 - val_squared_difference_loss: 40.3431 - val_KL_divergence_loss: 3.1101 - val_neg_log_likelihood: 79.9026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      " - 1s - loss: 83.1673 - squared_difference_loss: 40.7627 - KL_divergence_loss: 3.0549 - neg_log_likelihood: 80.1124 - val_loss: 83.0585 - val_squared_difference_loss: 40.0601 - val_KL_divergence_loss: 3.2975 - val_neg_log_likelihood: 79.7610\n",
      "Epoch 29/50\n",
      " - 1s - loss: 83.0900 - squared_difference_loss: 40.5127 - KL_divergence_loss: 3.1026 - neg_log_likelihood: 79.9874 - val_loss: 82.8778 - val_squared_difference_loss: 40.0291 - val_KL_divergence_loss: 3.1323 - val_neg_log_likelihood: 79.7456\n",
      "Epoch 30/50\n",
      " - 1s - loss: 83.0394 - squared_difference_loss: 40.4460 - KL_divergence_loss: 3.0854 - neg_log_likelihood: 79.9540 - val_loss: 82.9562 - val_squared_difference_loss: 40.0886 - val_KL_divergence_loss: 3.1809 - val_neg_log_likelihood: 79.7753\n",
      "Epoch 31/50\n",
      " - 1s - loss: 83.0986 - squared_difference_loss: 40.4926 - KL_divergence_loss: 3.1213 - neg_log_likelihood: 79.9773 - val_loss: 82.9018 - val_squared_difference_loss: 40.0812 - val_KL_divergence_loss: 3.1303 - val_neg_log_likelihood: 79.7716\n",
      "Epoch 32/50\n",
      " - 1s - loss: 83.0238 - squared_difference_loss: 40.3167 - KL_divergence_loss: 3.1344 - neg_log_likelihood: 79.8894 - val_loss: 82.9133 - val_squared_difference_loss: 40.1802 - val_KL_divergence_loss: 3.0922 - val_neg_log_likelihood: 79.8211\n",
      "Epoch 33/50\n",
      " - 1s - loss: 82.9911 - squared_difference_loss: 40.3111 - KL_divergence_loss: 3.1046 - neg_log_likelihood: 79.8866 - val_loss: 82.8152 - val_squared_difference_loss: 39.7456 - val_KL_divergence_loss: 3.2114 - val_neg_log_likelihood: 79.6038\n",
      "Epoch 34/50\n",
      " - 1s - loss: 82.9522 - squared_difference_loss: 40.1913 - KL_divergence_loss: 3.1255 - neg_log_likelihood: 79.8266 - val_loss: 82.8297 - val_squared_difference_loss: 39.9658 - val_KL_divergence_loss: 3.1158 - val_neg_log_likelihood: 79.7139\n",
      "Epoch 35/50\n",
      " - 1s - loss: 82.9510 - squared_difference_loss: 40.1776 - KL_divergence_loss: 3.1312 - neg_log_likelihood: 79.8198 - val_loss: 82.7966 - val_squared_difference_loss: 39.8241 - val_KL_divergence_loss: 3.1535 - val_neg_log_likelihood: 79.6431\n",
      "Epoch 36/50\n",
      " - 1s - loss: 82.8697 - squared_difference_loss: 40.0070 - KL_divergence_loss: 3.1352 - neg_log_likelihood: 79.7345 - val_loss: 82.7943 - val_squared_difference_loss: 39.8647 - val_KL_divergence_loss: 3.1310 - val_neg_log_likelihood: 79.6633\n",
      "Epoch 37/50\n",
      " - 1s - loss: 82.8789 - squared_difference_loss: 40.0287 - KL_divergence_loss: 3.1336 - neg_log_likelihood: 79.7454 - val_loss: 82.7214 - val_squared_difference_loss: 39.5234 - val_KL_divergence_loss: 3.2287 - val_neg_log_likelihood: 79.4927\n",
      "Epoch 38/50\n",
      " - 1s - loss: 82.8545 - squared_difference_loss: 39.9510 - KL_divergence_loss: 3.1480 - neg_log_likelihood: 79.7065 - val_loss: 82.8103 - val_squared_difference_loss: 39.8581 - val_KL_divergence_loss: 3.1503 - val_neg_log_likelihood: 79.6600\n",
      "Epoch 39/50\n",
      " - 1s - loss: 82.8671 - squared_difference_loss: 40.0101 - KL_divergence_loss: 3.1310 - neg_log_likelihood: 79.7361 - val_loss: 82.8650 - val_squared_difference_loss: 39.8236 - val_KL_divergence_loss: 3.2222 - val_neg_log_likelihood: 79.6428\n",
      "Epoch 40/50\n",
      " - 1s - loss: 82.8087 - squared_difference_loss: 39.8699 - KL_divergence_loss: 3.1428 - neg_log_likelihood: 79.6659 - val_loss: 82.7285 - val_squared_difference_loss: 39.6400 - val_KL_divergence_loss: 3.1775 - val_neg_log_likelihood: 79.5510\n",
      "Epoch 41/50\n",
      " - 1s - loss: 82.8016 - squared_difference_loss: 39.8566 - KL_divergence_loss: 3.1423 - neg_log_likelihood: 79.6593 - val_loss: 82.7939 - val_squared_difference_loss: 39.7447 - val_KL_divergence_loss: 3.1906 - val_neg_log_likelihood: 79.6033\n",
      "Epoch 42/50\n",
      " - 1s - loss: 82.8071 - squared_difference_loss: 39.8365 - KL_divergence_loss: 3.1578 - neg_log_likelihood: 79.6493 - val_loss: 82.7073 - val_squared_difference_loss: 39.4560 - val_KL_divergence_loss: 3.2483 - val_neg_log_likelihood: 79.4590\n",
      "Epoch 43/50\n",
      " - 1s - loss: 82.7841 - squared_difference_loss: 39.7619 - KL_divergence_loss: 3.1722 - neg_log_likelihood: 79.6120 - val_loss: 82.6658 - val_squared_difference_loss: 39.3872 - val_KL_divergence_loss: 3.2412 - val_neg_log_likelihood: 79.4246\n",
      "Epoch 44/50\n",
      " - 1s - loss: 82.7299 - squared_difference_loss: 39.6644 - KL_divergence_loss: 3.1667 - neg_log_likelihood: 79.5632 - val_loss: 82.7027 - val_squared_difference_loss: 39.4898 - val_KL_divergence_loss: 3.2268 - val_neg_log_likelihood: 79.4759\n",
      "Epoch 45/50\n",
      " - 1s - loss: 82.7315 - squared_difference_loss: 39.6561 - KL_divergence_loss: 3.1724 - neg_log_likelihood: 79.5591 - val_loss: 82.7040 - val_squared_difference_loss: 39.7888 - val_KL_divergence_loss: 3.0786 - val_neg_log_likelihood: 79.6254\n",
      "Epoch 46/50\n",
      " - 1s - loss: 82.7332 - squared_difference_loss: 39.7214 - KL_divergence_loss: 3.1415 - neg_log_likelihood: 79.5917 - val_loss: 82.6621 - val_squared_difference_loss: 39.3291 - val_KL_divergence_loss: 3.2666 - val_neg_log_likelihood: 79.3955\n",
      "Epoch 47/50\n",
      " - 1s - loss: 82.7351 - squared_difference_loss: 39.6567 - KL_divergence_loss: 3.1758 - neg_log_likelihood: 79.5593 - val_loss: 82.5927 - val_squared_difference_loss: 39.2246 - val_KL_divergence_loss: 3.2494 - val_neg_log_likelihood: 79.3433\n",
      "Epoch 48/50\n",
      " - 1s - loss: 82.7057 - squared_difference_loss: 39.5970 - KL_divergence_loss: 3.1762 - neg_log_likelihood: 79.5295 - val_loss: 82.6655 - val_squared_difference_loss: 39.6585 - val_KL_divergence_loss: 3.1053 - val_neg_log_likelihood: 79.5602\n",
      "Epoch 49/50\n",
      " - 1s - loss: 82.7025 - squared_difference_loss: 39.6041 - KL_divergence_loss: 3.1694 - neg_log_likelihood: 79.5331 - val_loss: 82.6574 - val_squared_difference_loss: 39.4415 - val_KL_divergence_loss: 3.2057 - val_neg_log_likelihood: 79.4517\n",
      "Epoch 50/50\n",
      " - 1s - loss: 82.6727 - squared_difference_loss: 39.5456 - KL_divergence_loss: 3.1689 - neg_log_likelihood: 79.5038 - val_loss: 82.6517 - val_squared_difference_loss: 39.3906 - val_KL_divergence_loss: 3.2254 - val_neg_log_likelihood: 79.4263\n",
      "\tAverage Validation Squared Error = 78.27118384348235\n",
      "Latent Var Num = 3\n",
      "Hidden Var Num = 50\n",
      "Simulation #5: VAE with 3 Latent Variable(s) and 50 Hidden Variable(s)...\n",
      "\tEvaluating model on Fold 1\n",
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 95.2902 - squared_difference_loss: 66.4848 - KL_divergence_loss: 2.3168 - neg_log_likelihood: 92.9734 - val_loss: 86.6800 - val_squared_difference_loss: 48.4382 - val_KL_divergence_loss: 2.7299 - val_neg_log_likelihood: 83.9501\n",
      "Epoch 2/50\n",
      " - 1s - loss: 85.6991 - squared_difference_loss: 47.2532 - KL_divergence_loss: 2.3415 - neg_log_likelihood: 83.3576 - val_loss: 84.9583 - val_squared_difference_loss: 44.0783 - val_KL_divergence_loss: 3.1882 - val_neg_log_likelihood: 81.7702\n",
      "Epoch 3/50\n",
      " - 1s - loss: 84.7730 - squared_difference_loss: 44.7037 - KL_divergence_loss: 2.6902 - neg_log_likelihood: 82.0828 - val_loss: 84.4522 - val_squared_difference_loss: 42.9968 - val_KL_divergence_loss: 3.2228 - val_neg_log_likelihood: 81.2294\n",
      "Epoch 4/50\n",
      " - 1s - loss: 84.4797 - squared_difference_loss: 43.8385 - KL_divergence_loss: 2.8295 - neg_log_likelihood: 81.6502 - val_loss: 84.2056 - val_squared_difference_loss: 42.4863 - val_KL_divergence_loss: 3.2315 - val_neg_log_likelihood: 80.9742\n",
      "Epoch 5/50\n",
      " - 1s - loss: 84.3063 - squared_difference_loss: 43.4309 - KL_divergence_loss: 2.8598 - neg_log_likelihood: 81.4465 - val_loss: 84.1696 - val_squared_difference_loss: 42.3298 - val_KL_divergence_loss: 3.2738 - val_neg_log_likelihood: 80.8959\n",
      "Epoch 6/50\n",
      " - 1s - loss: 84.1882 - squared_difference_loss: 43.1491 - KL_divergence_loss: 2.8826 - neg_log_likelihood: 81.3056 - val_loss: 84.0547 - val_squared_difference_loss: 42.1724 - val_KL_divergence_loss: 3.2375 - val_neg_log_likelihood: 80.8172\n",
      "Epoch 7/50\n",
      " - 1s - loss: 84.0669 - squared_difference_loss: 42.8604 - KL_divergence_loss: 2.9057 - neg_log_likelihood: 81.1612 - val_loss: 83.9025 - val_squared_difference_loss: 41.9288 - val_KL_divergence_loss: 3.2071 - val_neg_log_likelihood: 80.6954\n",
      "Epoch 8/50\n",
      " - 1s - loss: 83.9413 - squared_difference_loss: 42.5949 - KL_divergence_loss: 2.9129 - neg_log_likelihood: 81.0285 - val_loss: 83.7893 - val_squared_difference_loss: 42.0114 - val_KL_divergence_loss: 3.0526 - val_neg_log_likelihood: 80.7367\n",
      "Epoch 9/50\n",
      " - 1s - loss: 83.8831 - squared_difference_loss: 42.4674 - KL_divergence_loss: 2.9184 - neg_log_likelihood: 80.9647 - val_loss: 83.6396 - val_squared_difference_loss: 41.6748 - val_KL_divergence_loss: 3.0712 - val_neg_log_likelihood: 80.5684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      " - 1s - loss: 83.8064 - squared_difference_loss: 42.3031 - KL_divergence_loss: 2.9239 - neg_log_likelihood: 80.8825 - val_loss: 83.6721 - val_squared_difference_loss: 41.5173 - val_KL_divergence_loss: 3.1825 - val_neg_log_likelihood: 80.4896\n",
      "Epoch 11/50\n",
      " - 1s - loss: 83.7249 - squared_difference_loss: 42.0835 - KL_divergence_loss: 2.9522 - neg_log_likelihood: 80.7727 - val_loss: 83.9023 - val_squared_difference_loss: 42.2142 - val_KL_divergence_loss: 3.0642 - val_neg_log_likelihood: 80.8381\n",
      "Epoch 12/50\n",
      " - 1s - loss: 83.6681 - squared_difference_loss: 41.9485 - KL_divergence_loss: 2.9629 - neg_log_likelihood: 80.7053 - val_loss: 83.6217 - val_squared_difference_loss: 41.5299 - val_KL_divergence_loss: 3.1257 - val_neg_log_likelihood: 80.4960\n",
      "Epoch 13/50\n",
      " - 1s - loss: 83.6208 - squared_difference_loss: 41.8567 - KL_divergence_loss: 2.9615 - neg_log_likelihood: 80.6593 - val_loss: 83.5010 - val_squared_difference_loss: 41.3654 - val_KL_divergence_loss: 3.0873 - val_neg_log_likelihood: 80.4137\n",
      "Epoch 14/50\n",
      " - 1s - loss: 83.5232 - squared_difference_loss: 41.5678 - KL_divergence_loss: 3.0083 - neg_log_likelihood: 80.5149 - val_loss: 83.6852 - val_squared_difference_loss: 41.9097 - val_KL_divergence_loss: 2.9993 - val_neg_log_likelihood: 80.6859\n",
      "Epoch 15/50\n",
      " - 1s - loss: 83.4304 - squared_difference_loss: 41.3832 - KL_divergence_loss: 3.0078 - neg_log_likelihood: 80.4226 - val_loss: 83.4871 - val_squared_difference_loss: 41.4082 - val_KL_divergence_loss: 3.0520 - val_neg_log_likelihood: 80.4351\n",
      "Epoch 16/50\n",
      " - 1s - loss: 83.3597 - squared_difference_loss: 41.2602 - KL_divergence_loss: 2.9986 - neg_log_likelihood: 80.3611 - val_loss: 83.4859 - val_squared_difference_loss: 41.3662 - val_KL_divergence_loss: 3.0718 - val_neg_log_likelihood: 80.4141\n",
      "Epoch 17/50\n",
      " - 1s - loss: 83.3324 - squared_difference_loss: 41.1269 - KL_divergence_loss: 3.0379 - neg_log_likelihood: 80.2944 - val_loss: 83.4427 - val_squared_difference_loss: 41.2363 - val_KL_divergence_loss: 3.0935 - val_neg_log_likelihood: 80.3491\n",
      "Epoch 18/50\n",
      " - 1s - loss: 83.2710 - squared_difference_loss: 41.0536 - KL_divergence_loss: 3.0132 - neg_log_likelihood: 80.2578 - val_loss: 83.3262 - val_squared_difference_loss: 41.1123 - val_KL_divergence_loss: 3.0391 - val_neg_log_likelihood: 80.2872\n",
      "Epoch 19/50\n",
      " - 1s - loss: 83.2265 - squared_difference_loss: 40.8870 - KL_divergence_loss: 3.0520 - neg_log_likelihood: 80.1745 - val_loss: 83.1432 - val_squared_difference_loss: 40.4640 - val_KL_divergence_loss: 3.1802 - val_neg_log_likelihood: 79.9630\n",
      "Epoch 20/50\n",
      " - 1s - loss: 83.2244 - squared_difference_loss: 40.8372 - KL_divergence_loss: 3.0748 - neg_log_likelihood: 80.1496 - val_loss: 83.3017 - val_squared_difference_loss: 40.9148 - val_KL_divergence_loss: 3.1133 - val_neg_log_likelihood: 80.1884\n",
      "Epoch 21/50\n",
      " - 1s - loss: 83.1289 - squared_difference_loss: 40.6644 - KL_divergence_loss: 3.0657 - neg_log_likelihood: 80.0632 - val_loss: 83.1645 - val_squared_difference_loss: 40.5644 - val_KL_divergence_loss: 3.1513 - val_neg_log_likelihood: 80.0132\n",
      "Epoch 22/50\n",
      " - 1s - loss: 83.1094 - squared_difference_loss: 40.6025 - KL_divergence_loss: 3.0771 - neg_log_likelihood: 80.0323 - val_loss: 83.2366 - val_squared_difference_loss: 40.9358 - val_KL_divergence_loss: 3.0377 - val_neg_log_likelihood: 80.1989\n",
      "Epoch 23/50\n",
      " - 1s - loss: 83.0899 - squared_difference_loss: 40.5794 - KL_divergence_loss: 3.0692 - neg_log_likelihood: 80.0207 - val_loss: 83.0707 - val_squared_difference_loss: 40.3009 - val_KL_divergence_loss: 3.1892 - val_neg_log_likelihood: 79.8815\n",
      "Epoch 24/50\n",
      " - 1s - loss: 83.0457 - squared_difference_loss: 40.4485 - KL_divergence_loss: 3.0905 - neg_log_likelihood: 79.9552 - val_loss: 83.1717 - val_squared_difference_loss: 40.7284 - val_KL_divergence_loss: 3.0766 - val_neg_log_likelihood: 80.0952\n",
      "Epoch 25/50\n",
      " - 1s - loss: 83.0471 - squared_difference_loss: 40.4682 - KL_divergence_loss: 3.0820 - neg_log_likelihood: 79.9651 - val_loss: 82.9698 - val_squared_difference_loss: 40.2026 - val_KL_divergence_loss: 3.1375 - val_neg_log_likelihood: 79.8323\n",
      "Epoch 26/50\n",
      " - 1s - loss: 82.9769 - squared_difference_loss: 40.3191 - KL_divergence_loss: 3.0863 - neg_log_likelihood: 79.8906 - val_loss: 83.1171 - val_squared_difference_loss: 40.4544 - val_KL_divergence_loss: 3.1588 - val_neg_log_likelihood: 79.9582\n",
      "Epoch 27/50\n",
      " - 1s - loss: 82.9976 - squared_difference_loss: 40.3254 - KL_divergence_loss: 3.1039 - neg_log_likelihood: 79.8937 - val_loss: 82.9143 - val_squared_difference_loss: 40.1475 - val_KL_divergence_loss: 3.1096 - val_neg_log_likelihood: 79.8047\n",
      "Epoch 28/50\n",
      " - 1s - loss: 82.9545 - squared_difference_loss: 40.2242 - KL_divergence_loss: 3.1114 - neg_log_likelihood: 79.8431 - val_loss: 82.8918 - val_squared_difference_loss: 40.1141 - val_KL_divergence_loss: 3.1037 - val_neg_log_likelihood: 79.7880\n",
      "Epoch 29/50\n",
      " - 1s - loss: 82.9279 - squared_difference_loss: 40.1342 - KL_divergence_loss: 3.1298 - neg_log_likelihood: 79.7981 - val_loss: 82.8190 - val_squared_difference_loss: 39.9756 - val_KL_divergence_loss: 3.1002 - val_neg_log_likelihood: 79.7188\n",
      "Epoch 30/50\n",
      " - 1s - loss: 82.8673 - squared_difference_loss: 40.0438 - KL_divergence_loss: 3.1144 - neg_log_likelihood: 79.7529 - val_loss: 82.8393 - val_squared_difference_loss: 40.0844 - val_KL_divergence_loss: 3.0661 - val_neg_log_likelihood: 79.7732\n",
      "Epoch 31/50\n",
      " - 1s - loss: 82.8854 - squared_difference_loss: 40.0496 - KL_divergence_loss: 3.1296 - neg_log_likelihood: 79.7558 - val_loss: 82.9343 - val_squared_difference_loss: 40.0960 - val_KL_divergence_loss: 3.1553 - val_neg_log_likelihood: 79.7790\n",
      "Epoch 32/50\n",
      " - 1s - loss: 82.8993 - squared_difference_loss: 40.0766 - KL_divergence_loss: 3.1300 - neg_log_likelihood: 79.7693 - val_loss: 82.8816 - val_squared_difference_loss: 39.7328 - val_KL_divergence_loss: 3.2842 - val_neg_log_likelihood: 79.5974\n",
      "Epoch 33/50\n",
      " - 1s - loss: 82.8496 - squared_difference_loss: 39.9108 - KL_divergence_loss: 3.1632 - neg_log_likelihood: 79.6864 - val_loss: 82.8273 - val_squared_difference_loss: 39.9259 - val_KL_divergence_loss: 3.1333 - val_neg_log_likelihood: 79.6940\n",
      "Epoch 34/50\n",
      " - 1s - loss: 82.7936 - squared_difference_loss: 39.8670 - KL_divergence_loss: 3.1291 - neg_log_likelihood: 79.6645 - val_loss: 82.7704 - val_squared_difference_loss: 39.7739 - val_KL_divergence_loss: 3.1524 - val_neg_log_likelihood: 79.6180\n",
      "Epoch 35/50\n",
      " - 1s - loss: 82.7978 - squared_difference_loss: 39.8319 - KL_divergence_loss: 3.1508 - neg_log_likelihood: 79.6469 - val_loss: 82.7529 - val_squared_difference_loss: 39.8120 - val_KL_divergence_loss: 3.1158 - val_neg_log_likelihood: 79.6370\n",
      "Epoch 36/50\n",
      " - 1s - loss: 82.7847 - squared_difference_loss: 39.8130 - KL_divergence_loss: 3.1472 - neg_log_likelihood: 79.6375 - val_loss: 82.7452 - val_squared_difference_loss: 39.5657 - val_KL_divergence_loss: 3.2313 - val_neg_log_likelihood: 79.5138\n",
      "Epoch 37/50\n",
      " - 1s - loss: 82.7923 - squared_difference_loss: 39.8288 - KL_divergence_loss: 3.1468 - neg_log_likelihood: 79.6454 - val_loss: 82.7296 - val_squared_difference_loss: 40.0175 - val_KL_divergence_loss: 2.9898 - val_neg_log_likelihood: 79.7397\n",
      "Epoch 38/50\n",
      " - 1s - loss: 82.7497 - squared_difference_loss: 39.7462 - KL_divergence_loss: 3.1456 - neg_log_likelihood: 79.6041 - val_loss: 82.7305 - val_squared_difference_loss: 39.5596 - val_KL_divergence_loss: 3.2196 - val_neg_log_likelihood: 79.5108\n",
      "Epoch 39/50\n",
      " - 1s - loss: 82.7386 - squared_difference_loss: 39.6711 - KL_divergence_loss: 3.1721 - neg_log_likelihood: 79.5666 - val_loss: 82.7184 - val_squared_difference_loss: 39.5287 - val_KL_divergence_loss: 3.2231 - val_neg_log_likelihood: 79.4954\n",
      "Epoch 40/50\n",
      " - 1s - loss: 82.7493 - squared_difference_loss: 39.6712 - KL_divergence_loss: 3.1827 - neg_log_likelihood: 79.5666 - val_loss: 82.6430 - val_squared_difference_loss: 39.4235 - val_KL_divergence_loss: 3.2003 - val_neg_log_likelihood: 79.4427\n",
      "Epoch 41/50\n",
      " - 1s - loss: 82.7480 - squared_difference_loss: 39.6905 - KL_divergence_loss: 3.1718 - neg_log_likelihood: 79.5763 - val_loss: 82.7931 - val_squared_difference_loss: 39.6585 - val_KL_divergence_loss: 3.2329 - val_neg_log_likelihood: 79.5603\n",
      "Epoch 42/50\n",
      " - 1s - loss: 82.7734 - squared_difference_loss: 39.7281 - KL_divergence_loss: 3.1784 - neg_log_likelihood: 79.5950 - val_loss: 82.7874 - val_squared_difference_loss: 39.9791 - val_KL_divergence_loss: 3.0669 - val_neg_log_likelihood: 79.7206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50\n",
      " - 1s - loss: 82.6774 - squared_difference_loss: 39.5816 - KL_divergence_loss: 3.1555 - neg_log_likelihood: 79.5218 - val_loss: 82.7084 - val_squared_difference_loss: 39.6787 - val_KL_divergence_loss: 3.1381 - val_neg_log_likelihood: 79.5703\n",
      "Epoch 44/50\n",
      " - 1s - loss: 82.6652 - squared_difference_loss: 39.5255 - KL_divergence_loss: 3.1714 - neg_log_likelihood: 79.4938 - val_loss: 82.6305 - val_squared_difference_loss: 39.5680 - val_KL_divergence_loss: 3.1155 - val_neg_log_likelihood: 79.5150\n",
      "Epoch 45/50\n",
      " - 1s - loss: 82.6741 - squared_difference_loss: 39.5554 - KL_divergence_loss: 3.1654 - neg_log_likelihood: 79.5087 - val_loss: 82.6253 - val_squared_difference_loss: 39.3932 - val_KL_divergence_loss: 3.1978 - val_neg_log_likelihood: 79.4276\n",
      "Epoch 46/50\n",
      " - 1s - loss: 82.6793 - squared_difference_loss: 39.5142 - KL_divergence_loss: 3.1912 - neg_log_likelihood: 79.4881 - val_loss: 82.7085 - val_squared_difference_loss: 39.4785 - val_KL_divergence_loss: 3.2382 - val_neg_log_likelihood: 79.4703\n",
      "Epoch 47/50\n",
      " - 1s - loss: 82.7334 - squared_difference_loss: 39.6374 - KL_divergence_loss: 3.1837 - neg_log_likelihood: 79.5497 - val_loss: 82.6644 - val_squared_difference_loss: 39.4972 - val_KL_divergence_loss: 3.1848 - val_neg_log_likelihood: 79.4796\n",
      "Epoch 48/50\n",
      " - 1s - loss: 82.6661 - squared_difference_loss: 39.4860 - KL_divergence_loss: 3.1921 - neg_log_likelihood: 79.4740 - val_loss: 82.5966 - val_squared_difference_loss: 39.1826 - val_KL_divergence_loss: 3.2742 - val_neg_log_likelihood: 79.3223\n",
      "Epoch 49/50\n",
      " - 1s - loss: 82.6564 - squared_difference_loss: 39.4506 - KL_divergence_loss: 3.2001 - neg_log_likelihood: 79.4563 - val_loss: 82.6215 - val_squared_difference_loss: 39.3257 - val_KL_divergence_loss: 3.2277 - val_neg_log_likelihood: 79.3938\n",
      "Epoch 50/50\n",
      " - 1s - loss: 82.6340 - squared_difference_loss: 39.4106 - KL_divergence_loss: 3.1977 - neg_log_likelihood: 79.4363 - val_loss: 82.5873 - val_squared_difference_loss: 39.3596 - val_KL_divergence_loss: 3.1765 - val_neg_log_likelihood: 79.4108\n",
      "\tAverage Validation Squared Error = 78.2074656778095\n",
      "Latent Var Num = 3\n",
      "Hidden Var Num = 60\n",
      "Simulation #6: VAE with 3 Latent Variable(s) and 60 Hidden Variable(s)...\n",
      "\tEvaluating model on Fold 1\n",
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 93.7196 - squared_difference_loss: 63.6079 - KL_divergence_loss: 2.1846 - neg_log_likelihood: 91.5350 - val_loss: 86.4100 - val_squared_difference_loss: 47.9133 - val_KL_divergence_loss: 2.7224 - val_neg_log_likelihood: 83.6876\n",
      "Epoch 2/50\n",
      " - 1s - loss: 85.5340 - squared_difference_loss: 46.6734 - KL_divergence_loss: 2.4663 - neg_log_likelihood: 83.0677 - val_loss: 84.6562 - val_squared_difference_loss: 43.5746 - val_KL_divergence_loss: 3.1379 - val_neg_log_likelihood: 81.5183\n",
      "Epoch 3/50\n",
      " - 1s - loss: 84.6400 - squared_difference_loss: 44.3641 - KL_divergence_loss: 2.7269 - neg_log_likelihood: 81.9131 - val_loss: 84.1238 - val_squared_difference_loss: 42.1750 - val_KL_divergence_loss: 3.3054 - val_neg_log_likelihood: 80.8185\n",
      "Epoch 4/50\n",
      " - 1s - loss: 84.2756 - squared_difference_loss: 43.4360 - KL_divergence_loss: 2.8266 - neg_log_likelihood: 81.4490 - val_loss: 83.8999 - val_squared_difference_loss: 41.8309 - val_KL_divergence_loss: 3.2534 - val_neg_log_likelihood: 80.6465\n",
      "Epoch 5/50\n",
      " - 1s - loss: 84.1375 - squared_difference_loss: 43.0180 - KL_divergence_loss: 2.8975 - neg_log_likelihood: 81.2400 - val_loss: 83.8374 - val_squared_difference_loss: 41.6771 - val_KL_divergence_loss: 3.2678 - val_neg_log_likelihood: 80.5696\n",
      "Epoch 6/50\n",
      " - 1s - loss: 84.0127 - squared_difference_loss: 42.7597 - KL_divergence_loss: 2.9018 - neg_log_likelihood: 81.1109 - val_loss: 83.8654 - val_squared_difference_loss: 41.9021 - val_KL_divergence_loss: 3.1833 - val_neg_log_likelihood: 80.6820\n",
      "Epoch 7/50\n",
      " - 1s - loss: 83.9164 - squared_difference_loss: 42.5735 - KL_divergence_loss: 2.8986 - neg_log_likelihood: 81.0177 - val_loss: 83.7998 - val_squared_difference_loss: 41.9116 - val_KL_divergence_loss: 3.1130 - val_neg_log_likelihood: 80.6868\n",
      "Epoch 8/50\n",
      " - 1s - loss: 83.8426 - squared_difference_loss: 42.3981 - KL_divergence_loss: 2.9126 - neg_log_likelihood: 80.9301 - val_loss: 83.7467 - val_squared_difference_loss: 41.7190 - val_KL_divergence_loss: 3.1562 - val_neg_log_likelihood: 80.5905\n",
      "Epoch 9/50\n",
      " - 1s - loss: 83.7140 - squared_difference_loss: 42.1313 - KL_divergence_loss: 2.9173 - neg_log_likelihood: 80.7966 - val_loss: 83.6920 - val_squared_difference_loss: 41.4065 - val_KL_divergence_loss: 3.2578 - val_neg_log_likelihood: 80.4342\n",
      "Epoch 10/50\n",
      " - 1s - loss: 83.6474 - squared_difference_loss: 41.9570 - KL_divergence_loss: 2.9379 - neg_log_likelihood: 80.7095 - val_loss: 83.7603 - val_squared_difference_loss: 41.8306 - val_KL_divergence_loss: 3.1140 - val_neg_log_likelihood: 80.6463\n",
      "Epoch 11/50\n",
      " - 1s - loss: 83.5131 - squared_difference_loss: 41.6644 - KL_divergence_loss: 2.9499 - neg_log_likelihood: 80.5632 - val_loss: 83.4721 - val_squared_difference_loss: 40.9723 - val_KL_divergence_loss: 3.2549 - val_neg_log_likelihood: 80.2172\n",
      "Epoch 12/50\n",
      " - 1s - loss: 83.4341 - squared_difference_loss: 41.4575 - KL_divergence_loss: 2.9744 - neg_log_likelihood: 80.4598 - val_loss: 83.4394 - val_squared_difference_loss: 41.1548 - val_KL_divergence_loss: 3.1310 - val_neg_log_likelihood: 80.3084\n",
      "Epoch 13/50\n",
      " - 1s - loss: 83.3613 - squared_difference_loss: 41.3411 - KL_divergence_loss: 2.9598 - neg_log_likelihood: 80.4015 - val_loss: 83.4250 - val_squared_difference_loss: 40.9905 - val_KL_divergence_loss: 3.1987 - val_neg_log_likelihood: 80.2263\n",
      "Epoch 14/50\n",
      " - 1s - loss: 83.3026 - squared_difference_loss: 41.1785 - KL_divergence_loss: 2.9824 - neg_log_likelihood: 80.3203 - val_loss: 83.3369 - val_squared_difference_loss: 40.9224 - val_KL_divergence_loss: 3.1447 - val_neg_log_likelihood: 80.1922\n",
      "Epoch 15/50\n",
      " - 1s - loss: 83.2323 - squared_difference_loss: 41.0202 - KL_divergence_loss: 2.9912 - neg_log_likelihood: 80.2411 - val_loss: 83.3898 - val_squared_difference_loss: 40.9683 - val_KL_divergence_loss: 3.1746 - val_neg_log_likelihood: 80.2151\n",
      "Epoch 16/50\n",
      " - 1s - loss: 83.1953 - squared_difference_loss: 40.9283 - KL_divergence_loss: 3.0001 - neg_log_likelihood: 80.1951 - val_loss: 83.1288 - val_squared_difference_loss: 40.5498 - val_KL_divergence_loss: 3.1229 - val_neg_log_likelihood: 80.0059\n",
      "Epoch 17/50\n",
      " - 1s - loss: 83.1041 - squared_difference_loss: 40.6910 - KL_divergence_loss: 3.0276 - neg_log_likelihood: 80.0765 - val_loss: 83.1196 - val_squared_difference_loss: 40.7701 - val_KL_divergence_loss: 3.0035 - val_neg_log_likelihood: 80.1161\n",
      "Epoch 18/50\n",
      " - 1s - loss: 83.0934 - squared_difference_loss: 40.6747 - KL_divergence_loss: 3.0251 - neg_log_likelihood: 80.0684 - val_loss: 83.1901 - val_squared_difference_loss: 40.5100 - val_KL_divergence_loss: 3.2041 - val_neg_log_likelihood: 79.9860\n",
      "Epoch 19/50\n",
      " - 1s - loss: 83.0824 - squared_difference_loss: 40.6125 - KL_divergence_loss: 3.0452 - neg_log_likelihood: 80.0373 - val_loss: 82.9990 - val_squared_difference_loss: 40.3463 - val_KL_divergence_loss: 3.0949 - val_neg_log_likelihood: 79.9042\n",
      "Epoch 20/50\n",
      " - 1s - loss: 83.0578 - squared_difference_loss: 40.5195 - KL_divergence_loss: 3.0670 - neg_log_likelihood: 79.9908 - val_loss: 82.9044 - val_squared_difference_loss: 40.1999 - val_KL_divergence_loss: 3.0734 - val_neg_log_likelihood: 79.8310\n",
      "Epoch 21/50\n",
      " - 1s - loss: 83.0100 - squared_difference_loss: 40.4482 - KL_divergence_loss: 3.0549 - neg_log_likelihood: 79.9551 - val_loss: 82.8575 - val_squared_difference_loss: 40.1116 - val_KL_divergence_loss: 3.0707 - val_neg_log_likelihood: 79.7868\n",
      "Epoch 22/50\n",
      " - 1s - loss: 82.9680 - squared_difference_loss: 40.3256 - KL_divergence_loss: 3.0742 - neg_log_likelihood: 79.8938 - val_loss: 82.9382 - val_squared_difference_loss: 40.1456 - val_KL_divergence_loss: 3.1344 - val_neg_log_likelihood: 79.8038\n",
      "Epoch 23/50\n",
      " - 1s - loss: 82.9184 - squared_difference_loss: 40.2680 - KL_divergence_loss: 3.0534 - neg_log_likelihood: 79.8650 - val_loss: 82.8741 - val_squared_difference_loss: 40.0807 - val_KL_divergence_loss: 3.1028 - val_neg_log_likelihood: 79.7713\n",
      "Epoch 24/50\n",
      " - 1s - loss: 82.9105 - squared_difference_loss: 40.1784 - KL_divergence_loss: 3.0903 - neg_log_likelihood: 79.8202 - val_loss: 82.7898 - val_squared_difference_loss: 39.8552 - val_KL_divergence_loss: 3.1312 - val_neg_log_likelihood: 79.6586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      " - 1s - loss: 82.9209 - squared_difference_loss: 40.1612 - KL_divergence_loss: 3.1093 - neg_log_likelihood: 79.8116 - val_loss: 82.8682 - val_squared_difference_loss: 40.1311 - val_KL_divergence_loss: 3.0716 - val_neg_log_likelihood: 79.7965\n",
      "Epoch 26/50\n",
      " - 1s - loss: 82.8666 - squared_difference_loss: 40.0459 - KL_divergence_loss: 3.1126 - neg_log_likelihood: 79.7539 - val_loss: 82.8696 - val_squared_difference_loss: 39.9458 - val_KL_divergence_loss: 3.1658 - val_neg_log_likelihood: 79.7039\n",
      "Epoch 27/50\n",
      " - 1s - loss: 82.8131 - squared_difference_loss: 39.9148 - KL_divergence_loss: 3.1247 - neg_log_likelihood: 79.6884 - val_loss: 82.7667 - val_squared_difference_loss: 39.5596 - val_KL_divergence_loss: 3.2559 - val_neg_log_likelihood: 79.5108\n",
      "Epoch 28/50\n",
      " - 1s - loss: 82.8174 - squared_difference_loss: 39.9165 - KL_divergence_loss: 3.1282 - neg_log_likelihood: 79.6892 - val_loss: 82.7855 - val_squared_difference_loss: 39.8968 - val_KL_divergence_loss: 3.1061 - val_neg_log_likelihood: 79.6794\n",
      "Epoch 29/50\n",
      " - 1s - loss: 82.7803 - squared_difference_loss: 39.8811 - KL_divergence_loss: 3.1087 - neg_log_likelihood: 79.6715 - val_loss: 82.7236 - val_squared_difference_loss: 39.6844 - val_KL_divergence_loss: 3.1504 - val_neg_log_likelihood: 79.5732\n",
      "Epoch 30/50\n",
      " - 1s - loss: 82.7813 - squared_difference_loss: 39.8037 - KL_divergence_loss: 3.1485 - neg_log_likelihood: 79.6329 - val_loss: 82.7110 - val_squared_difference_loss: 39.6811 - val_KL_divergence_loss: 3.1394 - val_neg_log_likelihood: 79.5716\n",
      "Epoch 31/50\n",
      " - 1s - loss: 82.7509 - squared_difference_loss: 39.7626 - KL_divergence_loss: 3.1387 - neg_log_likelihood: 79.6123 - val_loss: 82.7153 - val_squared_difference_loss: 39.4317 - val_KL_divergence_loss: 3.2685 - val_neg_log_likelihood: 79.4468\n",
      "Epoch 32/50\n",
      " - 1s - loss: 82.7365 - squared_difference_loss: 39.6526 - KL_divergence_loss: 3.1792 - neg_log_likelihood: 79.5573 - val_loss: 82.6392 - val_squared_difference_loss: 39.5056 - val_KL_divergence_loss: 3.1554 - val_neg_log_likelihood: 79.4838\n",
      "Epoch 33/50\n",
      " - 1s - loss: 82.7191 - squared_difference_loss: 39.6824 - KL_divergence_loss: 3.1469 - neg_log_likelihood: 79.5722 - val_loss: 82.6506 - val_squared_difference_loss: 39.5657 - val_KL_divergence_loss: 3.1368 - val_neg_log_likelihood: 79.5139\n",
      "Epoch 34/50\n",
      " - 1s - loss: 82.6790 - squared_difference_loss: 39.5947 - KL_divergence_loss: 3.1506 - neg_log_likelihood: 79.5284 - val_loss: 82.7257 - val_squared_difference_loss: 39.6060 - val_KL_divergence_loss: 3.1917 - val_neg_log_likelihood: 79.5340\n",
      "Epoch 35/50\n",
      " - 1s - loss: 82.6957 - squared_difference_loss: 39.5754 - KL_divergence_loss: 3.1770 - neg_log_likelihood: 79.5187 - val_loss: 82.6581 - val_squared_difference_loss: 39.4052 - val_KL_divergence_loss: 3.2245 - val_neg_log_likelihood: 79.4336\n",
      "Epoch 36/50\n",
      " - 1s - loss: 82.6726 - squared_difference_loss: 39.5238 - KL_divergence_loss: 3.1797 - neg_log_likelihood: 79.4929 - val_loss: 82.6074 - val_squared_difference_loss: 39.4099 - val_KL_divergence_loss: 3.1714 - val_neg_log_likelihood: 79.4360\n",
      "Epoch 37/50\n",
      " - 1s - loss: 82.6555 - squared_difference_loss: 39.4912 - KL_divergence_loss: 3.1789 - neg_log_likelihood: 79.4766 - val_loss: 82.5994 - val_squared_difference_loss: 39.3559 - val_KL_divergence_loss: 3.1905 - val_neg_log_likelihood: 79.4090\n",
      "Epoch 38/50\n",
      " - 1s - loss: 82.6863 - squared_difference_loss: 39.4993 - KL_divergence_loss: 3.2057 - neg_log_likelihood: 79.4807 - val_loss: 82.6241 - val_squared_difference_loss: 39.4817 - val_KL_divergence_loss: 3.1523 - val_neg_log_likelihood: 79.4718\n",
      "Epoch 39/50\n",
      " - 1s - loss: 82.6402 - squared_difference_loss: 39.4564 - KL_divergence_loss: 3.1810 - neg_log_likelihood: 79.4592 - val_loss: 82.5882 - val_squared_difference_loss: 39.1989 - val_KL_divergence_loss: 3.2578 - val_neg_log_likelihood: 79.3304\n",
      "Epoch 40/50\n",
      " - 1s - loss: 82.6121 - squared_difference_loss: 39.3621 - KL_divergence_loss: 3.2000 - neg_log_likelihood: 79.4120 - val_loss: 82.5481 - val_squared_difference_loss: 39.1673 - val_KL_divergence_loss: 3.2335 - val_neg_log_likelihood: 79.3147\n",
      "Epoch 41/50\n",
      " - 1s - loss: 82.5906 - squared_difference_loss: 39.3353 - KL_divergence_loss: 3.1920 - neg_log_likelihood: 79.3986 - val_loss: 82.5527 - val_squared_difference_loss: 39.0783 - val_KL_divergence_loss: 3.2826 - val_neg_log_likelihood: 79.2701\n",
      "Epoch 42/50\n",
      " - 1s - loss: 82.5861 - squared_difference_loss: 39.3552 - KL_divergence_loss: 3.1775 - neg_log_likelihood: 79.4086 - val_loss: 82.5795 - val_squared_difference_loss: 39.1835 - val_KL_divergence_loss: 3.2568 - val_neg_log_likelihood: 79.3228\n",
      "Epoch 43/50\n",
      " - 1s - loss: 82.5475 - squared_difference_loss: 39.2364 - KL_divergence_loss: 3.1983 - neg_log_likelihood: 79.3492 - val_loss: 82.5679 - val_squared_difference_loss: 39.3169 - val_KL_divergence_loss: 3.1784 - val_neg_log_likelihood: 79.3895\n",
      "Epoch 44/50\n",
      " - 1s - loss: 82.5569 - squared_difference_loss: 39.2444 - KL_divergence_loss: 3.2037 - neg_log_likelihood: 79.3532 - val_loss: 82.5274 - val_squared_difference_loss: 39.0519 - val_KL_divergence_loss: 3.2704 - val_neg_log_likelihood: 79.2570\n",
      "Epoch 45/50\n",
      " - 1s - loss: 82.5622 - squared_difference_loss: 39.1910 - KL_divergence_loss: 3.2357 - neg_log_likelihood: 79.3265 - val_loss: 82.4970 - val_squared_difference_loss: 39.1530 - val_KL_divergence_loss: 3.1895 - val_neg_log_likelihood: 79.3075\n",
      "Epoch 46/50\n",
      " - 1s - loss: 82.5350 - squared_difference_loss: 39.1624 - KL_divergence_loss: 3.2228 - neg_log_likelihood: 79.3122 - val_loss: 82.4299 - val_squared_difference_loss: 38.9170 - val_KL_divergence_loss: 3.2404 - val_neg_log_likelihood: 79.1895\n",
      "Epoch 47/50\n",
      " - 1s - loss: 82.5449 - squared_difference_loss: 39.1823 - KL_divergence_loss: 3.2228 - neg_log_likelihood: 79.3221 - val_loss: 82.4155 - val_squared_difference_loss: 38.7863 - val_KL_divergence_loss: 3.2913 - val_neg_log_likelihood: 79.1242\n",
      "Epoch 48/50\n",
      " - 1s - loss: 82.5301 - squared_difference_loss: 39.1430 - KL_divergence_loss: 3.2276 - neg_log_likelihood: 79.3025 - val_loss: 82.4990 - val_squared_difference_loss: 39.2414 - val_KL_divergence_loss: 3.1473 - val_neg_log_likelihood: 79.3517\n",
      "Epoch 49/50\n",
      " - 1s - loss: 82.5589 - squared_difference_loss: 39.2376 - KL_divergence_loss: 3.2091 - neg_log_likelihood: 79.3498 - val_loss: 82.5993 - val_squared_difference_loss: 39.2147 - val_KL_divergence_loss: 3.2609 - val_neg_log_likelihood: 79.3384\n",
      "Epoch 50/50\n",
      " - 1s - loss: 82.5472 - squared_difference_loss: 39.1753 - KL_divergence_loss: 3.2286 - neg_log_likelihood: 79.3186 - val_loss: 82.5120 - val_squared_difference_loss: 38.9906 - val_KL_divergence_loss: 3.2857 - val_neg_log_likelihood: 79.2263\n",
      "\tAverage Validation Squared Error = 78.23416399484067\n",
      "Latent Var Num = 4\n",
      "Hidden Var Num = 10\n",
      "Simulation #7: VAE with 4 Latent Variable(s) and 10 Hidden Variable(s)...\n",
      "\tEvaluating model on Fold 1\n",
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 102.4613 - squared_difference_loss: 80.9662 - KL_divergence_loss: 2.2472 - neg_log_likelihood: 100.2141 - val_loss: 90.2317 - val_squared_difference_loss: 57.3029 - val_KL_divergence_loss: 1.8492 - val_neg_log_likelihood: 88.3824\n",
      "Epoch 2/50\n",
      " - 1s - loss: 88.4852 - squared_difference_loss: 54.5757 - KL_divergence_loss: 1.4663 - neg_log_likelihood: 87.0188 - val_loss: 87.2549 - val_squared_difference_loss: 50.3331 - val_KL_divergence_loss: 2.3573 - val_neg_log_likelihood: 84.8976\n",
      "Epoch 3/50\n",
      " - 1s - loss: 86.6065 - squared_difference_loss: 50.1764 - KL_divergence_loss: 1.7873 - neg_log_likelihood: 84.8192 - val_loss: 86.2015 - val_squared_difference_loss: 47.4812 - val_KL_divergence_loss: 2.7299 - val_neg_log_likelihood: 83.4716\n",
      "Epoch 4/50\n",
      " - 1s - loss: 85.8732 - squared_difference_loss: 48.0762 - KL_divergence_loss: 2.1042 - neg_log_likelihood: 83.7691 - val_loss: 85.5058 - val_squared_difference_loss: 46.2419 - val_KL_divergence_loss: 2.6539 - val_neg_log_likelihood: 82.8519\n",
      "Epoch 5/50\n",
      " - 1s - loss: 85.5556 - squared_difference_loss: 47.2374 - KL_divergence_loss: 2.2059 - neg_log_likelihood: 83.3497 - val_loss: 85.2107 - val_squared_difference_loss: 45.7365 - val_KL_divergence_loss: 2.6115 - val_neg_log_likelihood: 82.5992\n",
      "Epoch 6/50\n",
      " - 1s - loss: 85.3523 - squared_difference_loss: 46.6825 - KL_divergence_loss: 2.2800 - neg_log_likelihood: 83.0722 - val_loss: 84.9969 - val_squared_difference_loss: 45.5419 - val_KL_divergence_loss: 2.4950 - val_neg_log_likelihood: 82.5020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      " - 1s - loss: 85.2121 - squared_difference_loss: 46.3022 - KL_divergence_loss: 2.3300 - neg_log_likelihood: 82.8821 - val_loss: 84.8966 - val_squared_difference_loss: 45.2132 - val_KL_divergence_loss: 2.5591 - val_neg_log_likelihood: 82.3376\n",
      "Epoch 8/50\n",
      " - 1s - loss: 85.0979 - squared_difference_loss: 46.0010 - KL_divergence_loss: 2.3664 - neg_log_likelihood: 82.7315 - val_loss: 84.8162 - val_squared_difference_loss: 45.0376 - val_KL_divergence_loss: 2.5664 - val_neg_log_likelihood: 82.2498\n",
      "Epoch 9/50\n",
      " - 1s - loss: 85.0173 - squared_difference_loss: 45.7797 - KL_divergence_loss: 2.3964 - neg_log_likelihood: 82.6209 - val_loss: 84.6894 - val_squared_difference_loss: 44.6748 - val_KL_divergence_loss: 2.6210 - val_neg_log_likelihood: 82.0684\n",
      "Epoch 10/50\n",
      " - 1s - loss: 84.9655 - squared_difference_loss: 45.5608 - KL_divergence_loss: 2.4541 - neg_log_likelihood: 82.5114 - val_loss: 84.6370 - val_squared_difference_loss: 44.6277 - val_KL_divergence_loss: 2.5922 - val_neg_log_likelihood: 82.0449\n",
      "Epoch 11/50\n",
      " - 1s - loss: 84.8795 - squared_difference_loss: 45.3885 - KL_divergence_loss: 2.4542 - neg_log_likelihood: 82.4253 - val_loss: 84.5484 - val_squared_difference_loss: 44.3630 - val_KL_divergence_loss: 2.6359 - val_neg_log_likelihood: 81.9125\n",
      "Epoch 12/50\n",
      " - 1s - loss: 84.8365 - squared_difference_loss: 45.2200 - KL_divergence_loss: 2.4954 - neg_log_likelihood: 82.3410 - val_loss: 84.4626 - val_squared_difference_loss: 44.3110 - val_KL_divergence_loss: 2.5761 - val_neg_log_likelihood: 81.8865\n",
      "Epoch 13/50\n",
      " - 1s - loss: 84.7693 - squared_difference_loss: 45.0217 - KL_divergence_loss: 2.5274 - neg_log_likelihood: 82.2418 - val_loss: 84.4209 - val_squared_difference_loss: 44.0554 - val_KL_divergence_loss: 2.6622 - val_neg_log_likelihood: 81.7587\n",
      "Epoch 14/50\n",
      " - 1s - loss: 84.6920 - squared_difference_loss: 44.7601 - KL_divergence_loss: 2.5809 - neg_log_likelihood: 82.1111 - val_loss: 84.4590 - val_squared_difference_loss: 44.1429 - val_KL_divergence_loss: 2.6565 - val_neg_log_likelihood: 81.8024\n",
      "Epoch 15/50\n",
      " - 1s - loss: 84.6273 - squared_difference_loss: 44.5649 - KL_divergence_loss: 2.6138 - neg_log_likelihood: 82.0135 - val_loss: 84.3552 - val_squared_difference_loss: 43.6665 - val_KL_divergence_loss: 2.7910 - val_neg_log_likelihood: 81.5643\n",
      "Epoch 16/50\n",
      " - 1s - loss: 84.5383 - squared_difference_loss: 44.3081 - KL_divergence_loss: 2.6532 - neg_log_likelihood: 81.8850 - val_loss: 84.3166 - val_squared_difference_loss: 43.5236 - val_KL_divergence_loss: 2.8238 - val_neg_log_likelihood: 81.4928\n",
      "Epoch 17/50\n",
      " - 1s - loss: 84.4056 - squared_difference_loss: 43.9693 - KL_divergence_loss: 2.6900 - neg_log_likelihood: 81.7156 - val_loss: 84.3468 - val_squared_difference_loss: 43.5043 - val_KL_divergence_loss: 2.8636 - val_neg_log_likelihood: 81.4831\n",
      "Epoch 18/50\n",
      " - 1s - loss: 84.3475 - squared_difference_loss: 43.7948 - KL_divergence_loss: 2.7191 - neg_log_likelihood: 81.6284 - val_loss: 84.3101 - val_squared_difference_loss: 43.3929 - val_KL_divergence_loss: 2.8826 - val_neg_log_likelihood: 81.4274\n",
      "Epoch 19/50\n",
      " - 1s - loss: 84.2741 - squared_difference_loss: 43.5842 - KL_divergence_loss: 2.7510 - neg_log_likelihood: 81.5231 - val_loss: 84.2748 - val_squared_difference_loss: 43.4888 - val_KL_divergence_loss: 2.7994 - val_neg_log_likelihood: 81.4754\n",
      "Epoch 20/50\n",
      " - 1s - loss: 84.1555 - squared_difference_loss: 43.3316 - KL_divergence_loss: 2.7587 - neg_log_likelihood: 81.3968 - val_loss: 84.3459 - val_squared_difference_loss: 43.4698 - val_KL_divergence_loss: 2.8800 - val_neg_log_likelihood: 81.4659\n",
      "Epoch 21/50\n",
      " - 1s - loss: 84.0695 - squared_difference_loss: 43.1359 - KL_divergence_loss: 2.7706 - neg_log_likelihood: 81.2989 - val_loss: 84.3751 - val_squared_difference_loss: 43.4515 - val_KL_divergence_loss: 2.9184 - val_neg_log_likelihood: 81.4567\n",
      "Epoch 22/50\n",
      " - 1s - loss: 84.0036 - squared_difference_loss: 42.9548 - KL_divergence_loss: 2.7952 - neg_log_likelihood: 81.2084 - val_loss: 84.4212 - val_squared_difference_loss: 43.8099 - val_KL_divergence_loss: 2.7853 - val_neg_log_likelihood: 81.6359\n",
      "Epoch 23/50\n",
      " - 1s - loss: 83.9807 - squared_difference_loss: 42.9104 - KL_divergence_loss: 2.7945 - neg_log_likelihood: 81.1862 - val_loss: 84.3175 - val_squared_difference_loss: 43.4770 - val_KL_divergence_loss: 2.8480 - val_neg_log_likelihood: 81.4695\n",
      "Epoch 24/50\n",
      " - 1s - loss: 83.9293 - squared_difference_loss: 42.7734 - KL_divergence_loss: 2.8116 - neg_log_likelihood: 81.1177 - val_loss: 84.1556 - val_squared_difference_loss: 43.1977 - val_KL_divergence_loss: 2.8257 - val_neg_log_likelihood: 81.3298\n",
      "Epoch 25/50\n",
      " - 1s - loss: 83.9156 - squared_difference_loss: 42.7419 - KL_divergence_loss: 2.8136 - neg_log_likelihood: 81.1020 - val_loss: 84.0658 - val_squared_difference_loss: 43.0178 - val_KL_divergence_loss: 2.8259 - val_neg_log_likelihood: 81.2399\n",
      "Epoch 26/50\n",
      " - 1s - loss: 83.9012 - squared_difference_loss: 42.6903 - KL_divergence_loss: 2.8250 - neg_log_likelihood: 81.0761 - val_loss: 83.9703 - val_squared_difference_loss: 42.7191 - val_KL_divergence_loss: 2.8798 - val_neg_log_likelihood: 81.0906\n",
      "Epoch 27/50\n",
      " - 1s - loss: 83.8813 - squared_difference_loss: 42.6578 - KL_divergence_loss: 2.8214 - neg_log_likelihood: 81.0599 - val_loss: 84.0724 - val_squared_difference_loss: 42.9424 - val_KL_divergence_loss: 2.8702 - val_neg_log_likelihood: 81.2022\n",
      "Epoch 28/50\n",
      " - 1s - loss: 83.8531 - squared_difference_loss: 42.5479 - KL_divergence_loss: 2.8482 - neg_log_likelihood: 81.0049 - val_loss: 83.9182 - val_squared_difference_loss: 42.7826 - val_KL_divergence_loss: 2.7959 - val_neg_log_likelihood: 81.1223\n",
      "Epoch 29/50\n",
      " - 1s - loss: 83.8598 - squared_difference_loss: 42.6121 - KL_divergence_loss: 2.8228 - neg_log_likelihood: 81.0371 - val_loss: 83.8946 - val_squared_difference_loss: 42.7489 - val_KL_divergence_loss: 2.7892 - val_neg_log_likelihood: 81.1054\n",
      "Epoch 30/50\n",
      " - 1s - loss: 83.8529 - squared_difference_loss: 42.5515 - KL_divergence_loss: 2.8461 - neg_log_likelihood: 81.0068 - val_loss: 83.9453 - val_squared_difference_loss: 42.6813 - val_KL_divergence_loss: 2.8737 - val_neg_log_likelihood: 81.0716\n",
      "Epoch 31/50\n",
      " - 1s - loss: 83.8384 - squared_difference_loss: 42.5693 - KL_divergence_loss: 2.8228 - neg_log_likelihood: 81.0157 - val_loss: 83.8288 - val_squared_difference_loss: 42.4793 - val_KL_divergence_loss: 2.8581 - val_neg_log_likelihood: 80.9706\n",
      "Epoch 32/50\n",
      " - 1s - loss: 83.8083 - squared_difference_loss: 42.4865 - KL_divergence_loss: 2.8340 - neg_log_likelihood: 80.9742 - val_loss: 83.7321 - val_squared_difference_loss: 42.2598 - val_KL_divergence_loss: 2.8712 - val_neg_log_likelihood: 80.8609\n",
      "Epoch 33/50\n",
      " - 1s - loss: 83.8128 - squared_difference_loss: 42.4870 - KL_divergence_loss: 2.8383 - neg_log_likelihood: 80.9745 - val_loss: 84.1294 - val_squared_difference_loss: 43.0428 - val_KL_divergence_loss: 2.8770 - val_neg_log_likelihood: 81.2524\n",
      "Epoch 34/50\n",
      " - 1s - loss: 83.8140 - squared_difference_loss: 42.5096 - KL_divergence_loss: 2.8282 - neg_log_likelihood: 80.9858 - val_loss: 83.9558 - val_squared_difference_loss: 42.6694 - val_KL_divergence_loss: 2.8901 - val_neg_log_likelihood: 81.0657\n",
      "Epoch 35/50\n",
      " - 1s - loss: 83.7749 - squared_difference_loss: 42.4059 - KL_divergence_loss: 2.8409 - neg_log_likelihood: 80.9340 - val_loss: 83.8553 - val_squared_difference_loss: 42.5563 - val_KL_divergence_loss: 2.8461 - val_neg_log_likelihood: 81.0092\n",
      "Epoch 36/50\n",
      " - 1s - loss: 83.7646 - squared_difference_loss: 42.4417 - KL_divergence_loss: 2.8127 - neg_log_likelihood: 80.9519 - val_loss: 83.8890 - val_squared_difference_loss: 42.6036 - val_KL_divergence_loss: 2.8562 - val_neg_log_likelihood: 81.0328\n",
      "Epoch 37/50\n",
      " - 1s - loss: 83.7829 - squared_difference_loss: 42.4188 - KL_divergence_loss: 2.8425 - neg_log_likelihood: 80.9404 - val_loss: 84.1914 - val_squared_difference_loss: 43.2571 - val_KL_divergence_loss: 2.8318 - val_neg_log_likelihood: 81.3596\n",
      "Epoch 38/50\n",
      " - 1s - loss: 83.7805 - squared_difference_loss: 42.4353 - KL_divergence_loss: 2.8319 - neg_log_likelihood: 80.9486 - val_loss: 84.1498 - val_squared_difference_loss: 43.1497 - val_KL_divergence_loss: 2.8439 - val_neg_log_likelihood: 81.3059\n",
      "Epoch 39/50\n",
      " - 1s - loss: 83.7577 - squared_difference_loss: 42.4028 - KL_divergence_loss: 2.8253 - neg_log_likelihood: 80.9324 - val_loss: 84.3489 - val_squared_difference_loss: 43.6244 - val_KL_divergence_loss: 2.8057 - val_neg_log_likelihood: 81.5432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      " - 1s - loss: 83.7655 - squared_difference_loss: 42.4080 - KL_divergence_loss: 2.8304 - neg_log_likelihood: 80.9350 - val_loss: 84.1031 - val_squared_difference_loss: 43.0587 - val_KL_divergence_loss: 2.8427 - val_neg_log_likelihood: 81.2604\n",
      "Epoch 41/50\n",
      " - 1s - loss: 83.7449 - squared_difference_loss: 42.3419 - KL_divergence_loss: 2.8429 - neg_log_likelihood: 80.9020 - val_loss: 84.3119 - val_squared_difference_loss: 43.4846 - val_KL_divergence_loss: 2.8386 - val_neg_log_likelihood: 81.4733\n",
      "Epoch 42/50\n",
      " - 1s - loss: 83.7563 - squared_difference_loss: 42.3470 - KL_divergence_loss: 2.8518 - neg_log_likelihood: 80.9045 - val_loss: 84.1907 - val_squared_difference_loss: 43.2057 - val_KL_divergence_loss: 2.8568 - val_neg_log_likelihood: 81.3339\n",
      "\tAverage Validation Squared Error = 81.1393634832046\n",
      "Latent Var Num = 4\n",
      "Hidden Var Num = 20\n",
      "Simulation #8: VAE with 4 Latent Variable(s) and 20 Hidden Variable(s)...\n",
      "\tEvaluating model on Fold 1\n",
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 101.1975 - squared_difference_loss: 78.3469 - KL_divergence_loss: 2.2930 - neg_log_likelihood: 98.9045 - val_loss: 88.8933 - val_squared_difference_loss: 52.3909 - val_KL_divergence_loss: 2.9668 - val_neg_log_likelihood: 85.9265\n",
      "Epoch 2/50\n",
      " - 1s - loss: 86.9897 - squared_difference_loss: 50.4128 - KL_divergence_loss: 2.0523 - neg_log_likelihood: 84.9374 - val_loss: 85.7353 - val_squared_difference_loss: 45.0928 - val_KL_divergence_loss: 3.4579 - val_neg_log_likelihood: 82.2774\n",
      "Epoch 3/50\n",
      " - 1s - loss: 85.3354 - squared_difference_loss: 45.6502 - KL_divergence_loss: 2.7793 - neg_log_likelihood: 82.5561 - val_loss: 84.8916 - val_squared_difference_loss: 42.8124 - val_KL_divergence_loss: 3.7544 - val_neg_log_likelihood: 81.1372\n",
      "Epoch 4/50\n",
      " - 1s - loss: 84.7272 - squared_difference_loss: 44.0561 - KL_divergence_loss: 2.9681 - neg_log_likelihood: 81.7590 - val_loss: 84.3826 - val_squared_difference_loss: 42.1794 - val_KL_divergence_loss: 3.5619 - val_neg_log_likelihood: 80.8207\n",
      "Epoch 5/50\n",
      " - 1s - loss: 84.4010 - squared_difference_loss: 43.1848 - KL_divergence_loss: 3.0776 - neg_log_likelihood: 81.3234 - val_loss: 84.0960 - val_squared_difference_loss: 41.4645 - val_KL_divergence_loss: 3.6327 - val_neg_log_likelihood: 80.4633\n",
      "Epoch 6/50\n",
      " - 1s - loss: 84.1973 - squared_difference_loss: 42.4841 - KL_divergence_loss: 3.2243 - neg_log_likelihood: 80.9730 - val_loss: 83.9313 - val_squared_difference_loss: 41.0299 - val_KL_divergence_loss: 3.6853 - val_neg_log_likelihood: 80.2460\n",
      "Epoch 7/50\n",
      " - 1s - loss: 84.0383 - squared_difference_loss: 42.0496 - KL_divergence_loss: 3.2825 - neg_log_likelihood: 80.7558 - val_loss: 83.7141 - val_squared_difference_loss: 40.7048 - val_KL_divergence_loss: 3.6307 - val_neg_log_likelihood: 80.0834\n",
      "Epoch 8/50\n",
      " - 1s - loss: 83.8456 - squared_difference_loss: 41.5642 - KL_divergence_loss: 3.3325 - neg_log_likelihood: 80.5131 - val_loss: 83.5462 - val_squared_difference_loss: 40.2803 - val_KL_divergence_loss: 3.6751 - val_neg_log_likelihood: 79.8711\n",
      "Epoch 9/50\n",
      " - 1s - loss: 83.6934 - squared_difference_loss: 41.1758 - KL_divergence_loss: 3.3745 - neg_log_likelihood: 80.3189 - val_loss: 83.5300 - val_squared_difference_loss: 40.3331 - val_KL_divergence_loss: 3.6324 - val_neg_log_likelihood: 79.8975\n",
      "Epoch 10/50\n",
      " - 1s - loss: 83.5809 - squared_difference_loss: 40.9431 - KL_divergence_loss: 3.3783 - neg_log_likelihood: 80.2026 - val_loss: 83.4516 - val_squared_difference_loss: 40.0929 - val_KL_divergence_loss: 3.6742 - val_neg_log_likelihood: 79.7775\n",
      "Epoch 11/50\n",
      " - 1s - loss: 83.4689 - squared_difference_loss: 40.6599 - KL_divergence_loss: 3.4079 - neg_log_likelihood: 80.0610 - val_loss: 83.5289 - val_squared_difference_loss: 40.3929 - val_KL_divergence_loss: 3.6015 - val_neg_log_likelihood: 79.9275\n",
      "Epoch 12/50\n",
      " - 1s - loss: 83.3908 - squared_difference_loss: 40.5240 - KL_divergence_loss: 3.3978 - neg_log_likelihood: 79.9930 - val_loss: 83.3449 - val_squared_difference_loss: 40.1025 - val_KL_divergence_loss: 3.5626 - val_neg_log_likelihood: 79.7822\n",
      "Epoch 13/50\n",
      " - 1s - loss: 83.3227 - squared_difference_loss: 40.3607 - KL_divergence_loss: 3.4114 - neg_log_likelihood: 79.9113 - val_loss: 83.3198 - val_squared_difference_loss: 40.0152 - val_KL_divergence_loss: 3.5812 - val_neg_log_likelihood: 79.7386\n",
      "Epoch 14/50\n",
      " - 1s - loss: 83.2962 - squared_difference_loss: 40.2581 - KL_divergence_loss: 3.4361 - neg_log_likelihood: 79.8600 - val_loss: 83.2301 - val_squared_difference_loss: 39.8953 - val_KL_divergence_loss: 3.5515 - val_neg_log_likelihood: 79.6786\n",
      "Epoch 15/50\n",
      " - 1s - loss: 83.2513 - squared_difference_loss: 40.1633 - KL_divergence_loss: 3.4387 - neg_log_likelihood: 79.8126 - val_loss: 83.1704 - val_squared_difference_loss: 39.8753 - val_KL_divergence_loss: 3.5017 - val_neg_log_likelihood: 79.6687\n",
      "Epoch 16/50\n",
      " - 1s - loss: 83.2258 - squared_difference_loss: 40.1200 - KL_divergence_loss: 3.4347 - neg_log_likelihood: 79.7910 - val_loss: 83.1658 - val_squared_difference_loss: 39.9957 - val_KL_divergence_loss: 3.4370 - val_neg_log_likelihood: 79.7288\n",
      "Epoch 17/50\n",
      " - 1s - loss: 83.1907 - squared_difference_loss: 40.0470 - KL_divergence_loss: 3.4362 - neg_log_likelihood: 79.7545 - val_loss: 83.0482 - val_squared_difference_loss: 39.6096 - val_KL_divergence_loss: 3.5124 - val_neg_log_likelihood: 79.5358\n",
      "Epoch 18/50\n",
      " - 1s - loss: 83.1649 - squared_difference_loss: 39.9199 - KL_divergence_loss: 3.4740 - neg_log_likelihood: 79.6910 - val_loss: 83.0394 - val_squared_difference_loss: 39.8054 - val_KL_divergence_loss: 3.4057 - val_neg_log_likelihood: 79.6337\n",
      "Epoch 19/50\n",
      " - 1s - loss: 83.1312 - squared_difference_loss: 39.9325 - KL_divergence_loss: 3.4340 - neg_log_likelihood: 79.6973 - val_loss: 82.8717 - val_squared_difference_loss: 39.3959 - val_KL_divergence_loss: 3.4427 - val_neg_log_likelihood: 79.4290\n",
      "Epoch 20/50\n",
      " - 1s - loss: 83.0819 - squared_difference_loss: 39.8216 - KL_divergence_loss: 3.4401 - neg_log_likelihood: 79.6418 - val_loss: 82.9006 - val_squared_difference_loss: 39.3284 - val_KL_divergence_loss: 3.5054 - val_neg_log_likelihood: 79.3952\n",
      "Epoch 21/50\n",
      " - 1s - loss: 83.0728 - squared_difference_loss: 39.7911 - KL_divergence_loss: 3.4463 - neg_log_likelihood: 79.6265 - val_loss: 82.8460 - val_squared_difference_loss: 39.2181 - val_KL_divergence_loss: 3.5059 - val_neg_log_likelihood: 79.3401\n",
      "Epoch 22/50\n",
      " - 1s - loss: 83.0690 - squared_difference_loss: 39.7241 - KL_divergence_loss: 3.4760 - neg_log_likelihood: 79.5930 - val_loss: 82.8396 - val_squared_difference_loss: 39.3116 - val_KL_divergence_loss: 3.4528 - val_neg_log_likelihood: 79.3868\n",
      "Epoch 23/50\n",
      " - 1s - loss: 83.0616 - squared_difference_loss: 39.7337 - KL_divergence_loss: 3.4637 - neg_log_likelihood: 79.5979 - val_loss: 82.8265 - val_squared_difference_loss: 39.2114 - val_KL_divergence_loss: 3.4897 - val_neg_log_likelihood: 79.3367\n",
      "Epoch 24/50\n",
      " - 1s - loss: 83.0353 - squared_difference_loss: 39.6585 - KL_divergence_loss: 3.4750 - neg_log_likelihood: 79.5603 - val_loss: 82.7705 - val_squared_difference_loss: 38.9310 - val_KL_divergence_loss: 3.5740 - val_neg_log_likelihood: 79.1965\n",
      "Epoch 25/50\n",
      " - 1s - loss: 82.9930 - squared_difference_loss: 39.5799 - KL_divergence_loss: 3.4720 - neg_log_likelihood: 79.5210 - val_loss: 82.7285 - val_squared_difference_loss: 38.9958 - val_KL_divergence_loss: 3.4996 - val_neg_log_likelihood: 79.2289\n",
      "Epoch 26/50\n",
      " - 1s - loss: 82.9467 - squared_difference_loss: 39.5372 - KL_divergence_loss: 3.4471 - neg_log_likelihood: 79.4996 - val_loss: 82.7691 - val_squared_difference_loss: 38.9768 - val_KL_divergence_loss: 3.5497 - val_neg_log_likelihood: 79.2194\n",
      "Epoch 27/50\n",
      " - 1s - loss: 82.9512 - squared_difference_loss: 39.4476 - KL_divergence_loss: 3.4964 - neg_log_likelihood: 79.4548 - val_loss: 82.7215 - val_squared_difference_loss: 38.9184 - val_KL_divergence_loss: 3.5313 - val_neg_log_likelihood: 79.1902\n",
      "Epoch 28/50\n",
      " - 1s - loss: 82.8935 - squared_difference_loss: 39.3623 - KL_divergence_loss: 3.4813 - neg_log_likelihood: 79.4122 - val_loss: 82.7502 - val_squared_difference_loss: 38.9382 - val_KL_divergence_loss: 3.5501 - val_neg_log_likelihood: 79.2001\n",
      "Epoch 29/50\n",
      " - 1s - loss: 82.8952 - squared_difference_loss: 39.3408 - KL_divergence_loss: 3.4938 - neg_log_likelihood: 79.4014 - val_loss: 82.7874 - val_squared_difference_loss: 38.9776 - val_KL_divergence_loss: 3.5676 - val_neg_log_likelihood: 79.2198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50\n",
      " - 1s - loss: 82.8876 - squared_difference_loss: 39.3055 - KL_divergence_loss: 3.5038 - neg_log_likelihood: 79.3838 - val_loss: 82.7590 - val_squared_difference_loss: 39.0312 - val_KL_divergence_loss: 3.5124 - val_neg_log_likelihood: 79.2466\n",
      "Epoch 31/50\n",
      " - 1s - loss: 82.8303 - squared_difference_loss: 39.1780 - KL_divergence_loss: 3.5103 - neg_log_likelihood: 79.3200 - val_loss: 82.7387 - val_squared_difference_loss: 38.7588 - val_KL_divergence_loss: 3.6283 - val_neg_log_likelihood: 79.1104\n",
      "Epoch 32/50\n",
      " - 1s - loss: 82.7981 - squared_difference_loss: 39.0787 - KL_divergence_loss: 3.5278 - neg_log_likelihood: 79.2703 - val_loss: 82.9562 - val_squared_difference_loss: 39.6233 - val_KL_divergence_loss: 3.4136 - val_neg_log_likelihood: 79.5427\n",
      "Epoch 33/50\n",
      " - 1s - loss: 82.7490 - squared_difference_loss: 39.0450 - KL_divergence_loss: 3.4955 - neg_log_likelihood: 79.2535 - val_loss: 82.8097 - val_squared_difference_loss: 38.8694 - val_KL_divergence_loss: 3.6440 - val_neg_log_likelihood: 79.1657\n",
      "Epoch 34/50\n",
      " - 1s - loss: 82.7601 - squared_difference_loss: 39.0001 - KL_divergence_loss: 3.5290 - neg_log_likelihood: 79.2310 - val_loss: 82.7453 - val_squared_difference_loss: 38.9767 - val_KL_divergence_loss: 3.5259 - val_neg_log_likelihood: 79.2194\n",
      "Epoch 35/50\n",
      " - 1s - loss: 82.7232 - squared_difference_loss: 38.8908 - KL_divergence_loss: 3.5468 - neg_log_likelihood: 79.1764 - val_loss: 82.9204 - val_squared_difference_loss: 39.1151 - val_KL_divergence_loss: 3.6318 - val_neg_log_likelihood: 79.2885\n",
      "Epoch 36/50\n",
      " - 1s - loss: 82.7215 - squared_difference_loss: 38.8798 - KL_divergence_loss: 3.5506 - neg_log_likelihood: 79.1709 - val_loss: 82.9303 - val_squared_difference_loss: 39.2744 - val_KL_divergence_loss: 3.5621 - val_neg_log_likelihood: 79.3682\n",
      "Epoch 37/50\n",
      " - 1s - loss: 82.6860 - squared_difference_loss: 38.7804 - KL_divergence_loss: 3.5648 - neg_log_likelihood: 79.1212 - val_loss: 82.7801 - val_squared_difference_loss: 38.9839 - val_KL_divergence_loss: 3.5572 - val_neg_log_likelihood: 79.2230\n",
      "\tAverage Validation Squared Error = 77.66888639652989\n",
      "Latent Var Num = 4\n",
      "Hidden Var Num = 30\n",
      "Simulation #9: VAE with 4 Latent Variable(s) and 30 Hidden Variable(s)...\n",
      "\tEvaluating model on Fold 1\n",
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 98.2494 - squared_difference_loss: 72.0095 - KL_divergence_loss: 2.5136 - neg_log_likelihood: 95.7357 - val_loss: 87.4730 - val_squared_difference_loss: 49.5487 - val_KL_divergence_loss: 2.9677 - val_neg_log_likelihood: 84.5053\n",
      "Epoch 2/50\n",
      " - 1s - loss: 86.0274 - squared_difference_loss: 47.6278 - KL_divergence_loss: 2.4825 - neg_log_likelihood: 83.5449 - val_loss: 84.9960 - val_squared_difference_loss: 42.7655 - val_KL_divergence_loss: 3.8822 - val_neg_log_likelihood: 81.1137\n",
      "Epoch 3/50\n",
      " - 1s - loss: 84.7679 - squared_difference_loss: 44.0012 - KL_divergence_loss: 3.0363 - neg_log_likelihood: 81.7316 - val_loss: 84.4657 - val_squared_difference_loss: 41.9252 - val_KL_divergence_loss: 3.7721 - val_neg_log_likelihood: 80.6936\n",
      "Epoch 4/50\n",
      " - 1s - loss: 84.4075 - squared_difference_loss: 43.0848 - KL_divergence_loss: 3.1341 - neg_log_likelihood: 81.2734 - val_loss: 84.1348 - val_squared_difference_loss: 41.5264 - val_KL_divergence_loss: 3.6406 - val_neg_log_likelihood: 80.4942\n",
      "Epoch 5/50\n",
      " - 1s - loss: 84.1831 - squared_difference_loss: 42.4098 - KL_divergence_loss: 3.2472 - neg_log_likelihood: 80.9359 - val_loss: 83.9862 - val_squared_difference_loss: 41.1230 - val_KL_divergence_loss: 3.6937 - val_neg_log_likelihood: 80.2925\n",
      "Epoch 6/50\n",
      " - 1s - loss: 83.9639 - squared_difference_loss: 41.7816 - KL_divergence_loss: 3.3421 - neg_log_likelihood: 80.6218 - val_loss: 83.7362 - val_squared_difference_loss: 40.8059 - val_KL_divergence_loss: 3.6022 - val_neg_log_likelihood: 80.1339\n",
      "Epoch 7/50\n",
      " - 1s - loss: 83.7879 - squared_difference_loss: 41.3774 - KL_divergence_loss: 3.3682 - neg_log_likelihood: 80.4197 - val_loss: 83.5972 - val_squared_difference_loss: 40.5228 - val_KL_divergence_loss: 3.6048 - val_neg_log_likelihood: 79.9924\n",
      "Epoch 8/50\n",
      " - 1s - loss: 83.6677 - squared_difference_loss: 41.0768 - KL_divergence_loss: 3.3983 - neg_log_likelihood: 80.2694 - val_loss: 83.4481 - val_squared_difference_loss: 40.2283 - val_KL_divergence_loss: 3.6029 - val_neg_log_likelihood: 79.8451\n",
      "Epoch 9/50\n",
      " - 1s - loss: 83.5366 - squared_difference_loss: 40.7988 - KL_divergence_loss: 3.4062 - neg_log_likelihood: 80.1304 - val_loss: 83.4424 - val_squared_difference_loss: 40.2902 - val_KL_divergence_loss: 3.5663 - val_neg_log_likelihood: 79.8761\n",
      "Epoch 10/50\n",
      " - 1s - loss: 83.4319 - squared_difference_loss: 40.5617 - KL_divergence_loss: 3.4200 - neg_log_likelihood: 80.0119 - val_loss: 83.4051 - val_squared_difference_loss: 40.1072 - val_KL_divergence_loss: 3.6205 - val_neg_log_likelihood: 79.7846\n",
      "Epoch 11/50\n",
      " - 1s - loss: 83.3379 - squared_difference_loss: 40.2955 - KL_divergence_loss: 3.4591 - neg_log_likelihood: 79.8787 - val_loss: 83.3952 - val_squared_difference_loss: 39.9051 - val_KL_divergence_loss: 3.7117 - val_neg_log_likelihood: 79.6836\n",
      "Epoch 12/50\n",
      " - 1s - loss: 83.2698 - squared_difference_loss: 40.1706 - KL_divergence_loss: 3.4535 - neg_log_likelihood: 79.8163 - val_loss: 83.2887 - val_squared_difference_loss: 39.7187 - val_KL_divergence_loss: 3.6983 - val_neg_log_likelihood: 79.5904\n",
      "Epoch 13/50\n",
      " - 1s - loss: 83.2211 - squared_difference_loss: 40.0492 - KL_divergence_loss: 3.4655 - neg_log_likelihood: 79.7556 - val_loss: 83.5089 - val_squared_difference_loss: 40.1223 - val_KL_divergence_loss: 3.7168 - val_neg_log_likelihood: 79.7921\n",
      "Epoch 14/50\n",
      " - 1s - loss: 83.1676 - squared_difference_loss: 39.9514 - KL_divergence_loss: 3.4609 - neg_log_likelihood: 79.7067 - val_loss: 83.2072 - val_squared_difference_loss: 39.7543 - val_KL_divergence_loss: 3.5990 - val_neg_log_likelihood: 79.6082\n",
      "Epoch 15/50\n",
      " - 1s - loss: 83.1423 - squared_difference_loss: 39.9171 - KL_divergence_loss: 3.4527 - neg_log_likelihood: 79.6896 - val_loss: 83.0537 - val_squared_difference_loss: 39.3262 - val_KL_divergence_loss: 3.6597 - val_neg_log_likelihood: 79.3941\n",
      "Epoch 16/50\n",
      " - 1s - loss: 83.0653 - squared_difference_loss: 39.6830 - KL_divergence_loss: 3.4928 - neg_log_likelihood: 79.5725 - val_loss: 83.0042 - val_squared_difference_loss: 39.3826 - val_KL_divergence_loss: 3.5819 - val_neg_log_likelihood: 79.4223\n",
      "Epoch 17/50\n",
      " - 1s - loss: 83.0651 - squared_difference_loss: 39.6967 - KL_divergence_loss: 3.4858 - neg_log_likelihood: 79.5794 - val_loss: 82.9879 - val_squared_difference_loss: 39.4463 - val_KL_divergence_loss: 3.5338 - val_neg_log_likelihood: 79.4542\n",
      "Epoch 18/50\n",
      " - 1s - loss: 83.0601 - squared_difference_loss: 39.6455 - KL_divergence_loss: 3.5064 - neg_log_likelihood: 79.5537 - val_loss: 82.9570 - val_squared_difference_loss: 39.1861 - val_KL_divergence_loss: 3.6329 - val_neg_log_likelihood: 79.3241\n",
      "Epoch 19/50\n",
      " - 1s - loss: 82.9928 - squared_difference_loss: 39.5140 - KL_divergence_loss: 3.5048 - neg_log_likelihood: 79.4880 - val_loss: 82.8457 - val_squared_difference_loss: 39.0176 - val_KL_divergence_loss: 3.6059 - val_neg_log_likelihood: 79.2398\n",
      "Epoch 20/50\n",
      " - 1s - loss: 82.9768 - squared_difference_loss: 39.4504 - KL_divergence_loss: 3.5206 - neg_log_likelihood: 79.4562 - val_loss: 82.8987 - val_squared_difference_loss: 39.0068 - val_KL_divergence_loss: 3.6643 - val_neg_log_likelihood: 79.2344\n",
      "Epoch 21/50\n",
      " - 1s - loss: 82.9331 - squared_difference_loss: 39.3472 - KL_divergence_loss: 3.5285 - neg_log_likelihood: 79.4046 - val_loss: 82.7536 - val_squared_difference_loss: 38.7212 - val_KL_divergence_loss: 3.6620 - val_neg_log_likelihood: 79.0916\n",
      "Epoch 22/50\n",
      " - 1s - loss: 82.8492 - squared_difference_loss: 39.1459 - KL_divergence_loss: 3.5452 - neg_log_likelihood: 79.3039 - val_loss: 82.7455 - val_squared_difference_loss: 38.8650 - val_KL_divergence_loss: 3.5820 - val_neg_log_likelihood: 79.1635\n",
      "Epoch 23/50\n",
      " - 1s - loss: 82.8523 - squared_difference_loss: 39.0965 - KL_divergence_loss: 3.5730 - neg_log_likelihood: 79.2792 - val_loss: 82.6821 - val_squared_difference_loss: 38.7838 - val_KL_divergence_loss: 3.5592 - val_neg_log_likelihood: 79.1229\n",
      "Epoch 24/50\n",
      " - 1s - loss: 82.7480 - squared_difference_loss: 38.9354 - KL_divergence_loss: 3.5493 - neg_log_likelihood: 79.1987 - val_loss: 82.5891 - val_squared_difference_loss: 38.5032 - val_KL_divergence_loss: 3.6065 - val_neg_log_likelihood: 78.9826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      " - 1s - loss: 82.7449 - squared_difference_loss: 38.8913 - KL_divergence_loss: 3.5682 - neg_log_likelihood: 79.1767 - val_loss: 82.7719 - val_squared_difference_loss: 39.1273 - val_KL_divergence_loss: 3.4773 - val_neg_log_likelihood: 79.2946\n",
      "Epoch 26/50\n",
      " - 1s - loss: 82.7700 - squared_difference_loss: 38.9301 - KL_divergence_loss: 3.5739 - neg_log_likelihood: 79.1960 - val_loss: 82.6889 - val_squared_difference_loss: 38.7882 - val_KL_divergence_loss: 3.5637 - val_neg_log_likelihood: 79.1251\n",
      "Epoch 27/50\n",
      " - 1s - loss: 82.6962 - squared_difference_loss: 38.7639 - KL_divergence_loss: 3.5832 - neg_log_likelihood: 79.1130 - val_loss: 82.5831 - val_squared_difference_loss: 38.4407 - val_KL_divergence_loss: 3.6318 - val_neg_log_likelihood: 78.9513\n",
      "Epoch 28/50\n",
      " - 1s - loss: 82.7043 - squared_difference_loss: 38.7301 - KL_divergence_loss: 3.6083 - neg_log_likelihood: 79.0960 - val_loss: 82.5466 - val_squared_difference_loss: 38.2877 - val_KL_divergence_loss: 3.6718 - val_neg_log_likelihood: 78.8749\n",
      "Epoch 29/50\n",
      " - 1s - loss: 82.6488 - squared_difference_loss: 38.6399 - KL_divergence_loss: 3.5979 - neg_log_likelihood: 79.0509 - val_loss: 82.4706 - val_squared_difference_loss: 38.1805 - val_KL_divergence_loss: 3.6494 - val_neg_log_likelihood: 78.8213\n",
      "Epoch 30/50\n",
      " - 1s - loss: 82.6198 - squared_difference_loss: 38.6194 - KL_divergence_loss: 3.5791 - neg_log_likelihood: 79.0407 - val_loss: 82.4721 - val_squared_difference_loss: 38.0836 - val_KL_divergence_loss: 3.6993 - val_neg_log_likelihood: 78.7728\n",
      "Epoch 31/50\n",
      " - 1s - loss: 82.6119 - squared_difference_loss: 38.5255 - KL_divergence_loss: 3.6182 - neg_log_likelihood: 78.9938 - val_loss: 82.4738 - val_squared_difference_loss: 38.2173 - val_KL_divergence_loss: 3.6341 - val_neg_log_likelihood: 78.8396\n",
      "Epoch 32/50\n",
      " - 1s - loss: 82.6128 - squared_difference_loss: 38.5636 - KL_divergence_loss: 3.5999 - neg_log_likelihood: 79.0128 - val_loss: 82.4393 - val_squared_difference_loss: 38.1267 - val_KL_divergence_loss: 3.6450 - val_neg_log_likelihood: 78.7943\n",
      "Epoch 33/50\n",
      " - 1s - loss: 82.5465 - squared_difference_loss: 38.3965 - KL_divergence_loss: 3.6172 - neg_log_likelihood: 78.9293 - val_loss: 82.3371 - val_squared_difference_loss: 37.8939 - val_KL_divergence_loss: 3.6592 - val_neg_log_likelihood: 78.6780\n",
      "Epoch 34/50\n",
      " - 1s - loss: 82.5436 - squared_difference_loss: 38.3825 - KL_divergence_loss: 3.6214 - neg_log_likelihood: 78.9222 - val_loss: 82.4902 - val_squared_difference_loss: 38.3873 - val_KL_divergence_loss: 3.5655 - val_neg_log_likelihood: 78.9247\n",
      "Epoch 35/50\n",
      " - 1s - loss: 82.5555 - squared_difference_loss: 38.4569 - KL_divergence_loss: 3.5961 - neg_log_likelihood: 78.9595 - val_loss: 82.3361 - val_squared_difference_loss: 37.7971 - val_KL_divergence_loss: 3.7065 - val_neg_log_likelihood: 78.6296\n",
      "Epoch 36/50\n",
      " - 1s - loss: 82.5093 - squared_difference_loss: 38.3169 - KL_divergence_loss: 3.6199 - neg_log_likelihood: 78.8894 - val_loss: 82.4155 - val_squared_difference_loss: 37.7938 - val_KL_divergence_loss: 3.7876 - val_neg_log_likelihood: 78.6279\n",
      "Epoch 37/50\n",
      " - 1s - loss: 82.4869 - squared_difference_loss: 38.2328 - KL_divergence_loss: 3.6395 - neg_log_likelihood: 78.8474 - val_loss: 82.3737 - val_squared_difference_loss: 37.8224 - val_KL_divergence_loss: 3.7315 - val_neg_log_likelihood: 78.6422\n",
      "Epoch 38/50\n",
      " - 1s - loss: 82.4932 - squared_difference_loss: 38.2732 - KL_divergence_loss: 3.6256 - neg_log_likelihood: 78.8676 - val_loss: 82.3234 - val_squared_difference_loss: 37.7023 - val_KL_divergence_loss: 3.7412 - val_neg_log_likelihood: 78.5821\n",
      "Epoch 39/50\n",
      " - 1s - loss: 82.4674 - squared_difference_loss: 38.2499 - KL_divergence_loss: 3.6114 - neg_log_likelihood: 78.8560 - val_loss: 82.3342 - val_squared_difference_loss: 37.8795 - val_KL_divergence_loss: 3.6635 - val_neg_log_likelihood: 78.6708\n",
      "Epoch 40/50\n",
      " - 1s - loss: 82.3877 - squared_difference_loss: 38.1321 - KL_divergence_loss: 3.5907 - neg_log_likelihood: 78.7971 - val_loss: 82.2677 - val_squared_difference_loss: 37.6935 - val_KL_divergence_loss: 3.6899 - val_neg_log_likelihood: 78.5778\n",
      "Epoch 41/50\n",
      " - 1s - loss: 82.3957 - squared_difference_loss: 38.0901 - KL_divergence_loss: 3.6196 - neg_log_likelihood: 78.7760 - val_loss: 82.2652 - val_squared_difference_loss: 37.5995 - val_KL_divergence_loss: 3.7344 - val_neg_log_likelihood: 78.5308\n",
      "Epoch 42/50\n",
      " - 1s - loss: 82.3945 - squared_difference_loss: 38.0278 - KL_divergence_loss: 3.6496 - neg_log_likelihood: 78.7449 - val_loss: 82.3484 - val_squared_difference_loss: 37.5227 - val_KL_divergence_loss: 3.8560 - val_neg_log_likelihood: 78.4924\n",
      "Epoch 43/50\n",
      " - 1s - loss: 82.3651 - squared_difference_loss: 38.0319 - KL_divergence_loss: 3.6182 - neg_log_likelihood: 78.7470 - val_loss: 82.2654 - val_squared_difference_loss: 37.3505 - val_KL_divergence_loss: 3.8592 - val_neg_log_likelihood: 78.4062\n",
      "Epoch 44/50\n",
      " - 1s - loss: 82.3489 - squared_difference_loss: 37.9765 - KL_divergence_loss: 3.6297 - neg_log_likelihood: 78.7193 - val_loss: 82.2793 - val_squared_difference_loss: 37.6035 - val_KL_divergence_loss: 3.7466 - val_neg_log_likelihood: 78.5328\n",
      "Epoch 45/50\n",
      " - 1s - loss: 82.3309 - squared_difference_loss: 37.9328 - KL_divergence_loss: 3.6334 - neg_log_likelihood: 78.6974 - val_loss: 82.2820 - val_squared_difference_loss: 37.7642 - val_KL_divergence_loss: 3.6689 - val_neg_log_likelihood: 78.6131\n",
      "Epoch 46/50\n",
      " - 1s - loss: 82.3134 - squared_difference_loss: 37.9111 - KL_divergence_loss: 3.6269 - neg_log_likelihood: 78.6866 - val_loss: 82.2808 - val_squared_difference_loss: 37.5220 - val_KL_divergence_loss: 3.7888 - val_neg_log_likelihood: 78.4920\n",
      "Epoch 47/50\n",
      " - 1s - loss: 82.3367 - squared_difference_loss: 37.9093 - KL_divergence_loss: 3.6510 - neg_log_likelihood: 78.6857 - val_loss: 82.2223 - val_squared_difference_loss: 37.6749 - val_KL_divergence_loss: 3.6539 - val_neg_log_likelihood: 78.5684\n",
      "Epoch 48/50\n",
      " - 1s - loss: 82.3360 - squared_difference_loss: 37.9417 - KL_divergence_loss: 3.6341 - neg_log_likelihood: 78.7018 - val_loss: 82.2201 - val_squared_difference_loss: 37.6246 - val_KL_divergence_loss: 3.6768 - val_neg_log_likelihood: 78.5433\n",
      "Epoch 49/50\n",
      " - 1s - loss: 82.3030 - squared_difference_loss: 37.8932 - KL_divergence_loss: 3.6254 - neg_log_likelihood: 78.6776 - val_loss: 82.2641 - val_squared_difference_loss: 37.6003 - val_KL_divergence_loss: 3.7330 - val_neg_log_likelihood: 78.5311\n",
      "Epoch 50/50\n",
      " - 1s - loss: 82.2865 - squared_difference_loss: 37.8519 - KL_divergence_loss: 3.6295 - neg_log_likelihood: 78.6570 - val_loss: 82.2890 - val_squared_difference_loss: 37.6014 - val_KL_divergence_loss: 3.7573 - val_neg_log_likelihood: 78.5317\n",
      "\tAverage Validation Squared Error = 76.85704558044822\n",
      "Latent Var Num = 4\n",
      "Hidden Var Num = 40\n",
      "Simulation #10: VAE with 4 Latent Variable(s) and 40 Hidden Variable(s)...\n",
      "\tEvaluating model on Fold 1\n",
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/50\n",
      " - 14s - loss: 95.8897 - squared_difference_loss: 67.1140 - KL_divergence_loss: 2.6017 - neg_log_likelihood: 93.2880 - val_loss: 87.1498 - val_squared_difference_loss: 48.4472 - val_KL_divergence_loss: 3.1952 - val_neg_log_likelihood: 83.9546\n",
      "Epoch 2/50\n",
      " - 1s - loss: 85.6300 - squared_difference_loss: 46.3149 - KL_divergence_loss: 2.7415 - neg_log_likelihood: 82.8885 - val_loss: 84.7480 - val_squared_difference_loss: 42.5519 - val_KL_divergence_loss: 3.7411 - val_neg_log_likelihood: 81.0069\n",
      "Epoch 3/50\n",
      " - 1s - loss: 84.5243 - squared_difference_loss: 43.3690 - KL_divergence_loss: 3.1088 - neg_log_likelihood: 81.4155 - val_loss: 84.0943 - val_squared_difference_loss: 41.5284 - val_KL_divergence_loss: 3.5991 - val_neg_log_likelihood: 80.4952\n",
      "Epoch 4/50\n",
      " - 1s - loss: 84.1265 - squared_difference_loss: 42.2772 - KL_divergence_loss: 3.2569 - neg_log_likelihood: 80.8696 - val_loss: 83.7572 - val_squared_difference_loss: 40.6154 - val_KL_divergence_loss: 3.7185 - val_neg_log_likelihood: 80.0387\n",
      "Epoch 5/50\n",
      " - 1s - loss: 83.9011 - squared_difference_loss: 41.5933 - KL_divergence_loss: 3.3734 - neg_log_likelihood: 80.5277 - val_loss: 83.6537 - val_squared_difference_loss: 40.2654 - val_KL_divergence_loss: 3.7900 - val_neg_log_likelihood: 79.8637\n",
      "Epoch 6/50\n",
      " - 1s - loss: 83.7122 - squared_difference_loss: 41.0530 - KL_divergence_loss: 3.4548 - neg_log_likelihood: 80.2575 - val_loss: 83.5485 - val_squared_difference_loss: 39.9764 - val_KL_divergence_loss: 3.8293 - val_neg_log_likelihood: 79.7192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      " - 1s - loss: 83.5464 - squared_difference_loss: 40.6300 - KL_divergence_loss: 3.5004 - neg_log_likelihood: 80.0460 - val_loss: 83.4482 - val_squared_difference_loss: 40.0077 - val_KL_divergence_loss: 3.7134 - val_neg_log_likelihood: 79.7348\n",
      "Epoch 8/50\n",
      " - 1s - loss: 83.4114 - squared_difference_loss: 40.3485 - KL_divergence_loss: 3.5062 - neg_log_likelihood: 79.9053 - val_loss: 83.5113 - val_squared_difference_loss: 39.9878 - val_KL_divergence_loss: 3.7864 - val_neg_log_likelihood: 79.7249\n",
      "Epoch 9/50\n",
      " - 1s - loss: 83.3014 - squared_difference_loss: 40.0508 - KL_divergence_loss: 3.5450 - neg_log_likelihood: 79.7564 - val_loss: 83.3537 - val_squared_difference_loss: 39.7301 - val_KL_divergence_loss: 3.7576 - val_neg_log_likelihood: 79.5961\n",
      "Epoch 10/50\n",
      " - 1s - loss: 83.1895 - squared_difference_loss: 39.8201 - KL_divergence_loss: 3.5485 - neg_log_likelihood: 79.6411 - val_loss: 83.3554 - val_squared_difference_loss: 39.7506 - val_KL_divergence_loss: 3.7491 - val_neg_log_likelihood: 79.6063\n",
      "Epoch 11/50\n",
      " - 1s - loss: 83.1518 - squared_difference_loss: 39.7871 - KL_divergence_loss: 3.5273 - neg_log_likelihood: 79.6245 - val_loss: 83.2538 - val_squared_difference_loss: 39.5779 - val_KL_divergence_loss: 3.7339 - val_neg_log_likelihood: 79.5199\n",
      "Epoch 12/50\n",
      " - 1s - loss: 83.0471 - squared_difference_loss: 39.4641 - KL_divergence_loss: 3.5840 - neg_log_likelihood: 79.4631 - val_loss: 83.1912 - val_squared_difference_loss: 39.5031 - val_KL_divergence_loss: 3.7087 - val_neg_log_likelihood: 79.4825\n",
      "Epoch 13/50\n",
      " - 1s - loss: 82.9540 - squared_difference_loss: 39.3196 - KL_divergence_loss: 3.5632 - neg_log_likelihood: 79.3908 - val_loss: 83.0997 - val_squared_difference_loss: 39.2990 - val_KL_divergence_loss: 3.7192 - val_neg_log_likelihood: 79.3805\n",
      "Epoch 14/50\n",
      " - 1s - loss: 82.9202 - squared_difference_loss: 39.2364 - KL_divergence_loss: 3.5710 - neg_log_likelihood: 79.3492 - val_loss: 83.0970 - val_squared_difference_loss: 39.2251 - val_KL_divergence_loss: 3.7535 - val_neg_log_likelihood: 79.3435\n",
      "Epoch 15/50\n",
      " - 1s - loss: 82.8714 - squared_difference_loss: 39.1002 - KL_divergence_loss: 3.5903 - neg_log_likelihood: 79.2811 - val_loss: 82.9992 - val_squared_difference_loss: 39.2626 - val_KL_divergence_loss: 3.6369 - val_neg_log_likelihood: 79.3623\n",
      "Epoch 16/50\n",
      " - 1s - loss: 82.7901 - squared_difference_loss: 38.9687 - KL_divergence_loss: 3.5747 - neg_log_likelihood: 79.2154 - val_loss: 83.0255 - val_squared_difference_loss: 39.2480 - val_KL_divergence_loss: 3.6705 - val_neg_log_likelihood: 79.3550\n",
      "Epoch 17/50\n",
      " - 1s - loss: 82.7475 - squared_difference_loss: 38.8911 - KL_divergence_loss: 3.5710 - neg_log_likelihood: 79.1765 - val_loss: 82.8827 - val_squared_difference_loss: 38.7649 - val_KL_divergence_loss: 3.7693 - val_neg_log_likelihood: 79.1135\n",
      "Epoch 18/50\n",
      " - 1s - loss: 82.7143 - squared_difference_loss: 38.7251 - KL_divergence_loss: 3.6208 - neg_log_likelihood: 79.0936 - val_loss: 82.7955 - val_squared_difference_loss: 38.7882 - val_KL_divergence_loss: 3.6704 - val_neg_log_likelihood: 79.1251\n",
      "Epoch 19/50\n",
      " - 1s - loss: 82.6827 - squared_difference_loss: 38.6651 - KL_divergence_loss: 3.6191 - neg_log_likelihood: 79.0636 - val_loss: 82.8044 - val_squared_difference_loss: 38.6534 - val_KL_divergence_loss: 3.7467 - val_neg_log_likelihood: 79.0577\n",
      "Epoch 20/50\n",
      " - 1s - loss: 82.6455 - squared_difference_loss: 38.6444 - KL_divergence_loss: 3.5923 - neg_log_likelihood: 79.0532 - val_loss: 82.6825 - val_squared_difference_loss: 38.6906 - val_KL_divergence_loss: 3.6062 - val_neg_log_likelihood: 79.0763\n",
      "Epoch 21/50\n",
      " - 1s - loss: 82.6261 - squared_difference_loss: 38.5441 - KL_divergence_loss: 3.6231 - neg_log_likelihood: 79.0030 - val_loss: 82.7038 - val_squared_difference_loss: 38.2297 - val_KL_divergence_loss: 3.8580 - val_neg_log_likelihood: 78.8458\n",
      "Epoch 22/50\n",
      " - 1s - loss: 82.6207 - squared_difference_loss: 38.4860 - KL_divergence_loss: 3.6467 - neg_log_likelihood: 78.9740 - val_loss: 82.6347 - val_squared_difference_loss: 38.3164 - val_KL_divergence_loss: 3.7455 - val_neg_log_likelihood: 78.8892\n",
      "Epoch 23/50\n",
      " - 1s - loss: 82.5549 - squared_difference_loss: 38.4344 - KL_divergence_loss: 3.6067 - neg_log_likelihood: 78.9482 - val_loss: 82.5693 - val_squared_difference_loss: 38.0007 - val_KL_divergence_loss: 3.8379 - val_neg_log_likelihood: 78.7314\n",
      "Epoch 24/50\n",
      " - 1s - loss: 82.5418 - squared_difference_loss: 38.3436 - KL_divergence_loss: 3.6390 - neg_log_likelihood: 78.9028 - val_loss: 82.6643 - val_squared_difference_loss: 38.5472 - val_KL_divergence_loss: 3.6597 - val_neg_log_likelihood: 79.0046\n",
      "Epoch 25/50\n",
      " - 1s - loss: 82.5247 - squared_difference_loss: 38.3513 - KL_divergence_loss: 3.6180 - neg_log_likelihood: 78.9067 - val_loss: 82.4541 - val_squared_difference_loss: 38.1653 - val_KL_divergence_loss: 3.6405 - val_neg_log_likelihood: 78.8136\n",
      "Epoch 26/50\n",
      " - 1s - loss: 82.4766 - squared_difference_loss: 38.2240 - KL_divergence_loss: 3.6336 - neg_log_likelihood: 78.8430 - val_loss: 82.4345 - val_squared_difference_loss: 38.0833 - val_KL_divergence_loss: 3.6618 - val_neg_log_likelihood: 78.7726\n",
      "Epoch 27/50\n",
      " - 1s - loss: 82.4338 - squared_difference_loss: 38.1582 - KL_divergence_loss: 3.6237 - neg_log_likelihood: 78.8101 - val_loss: 82.4960 - val_squared_difference_loss: 38.4534 - val_KL_divergence_loss: 3.5383 - val_neg_log_likelihood: 78.9577\n",
      "Epoch 28/50\n",
      " - 1s - loss: 82.4404 - squared_difference_loss: 38.1834 - KL_divergence_loss: 3.6177 - neg_log_likelihood: 78.8227 - val_loss: 82.4998 - val_squared_difference_loss: 38.2261 - val_KL_divergence_loss: 3.6558 - val_neg_log_likelihood: 78.8441\n",
      "Epoch 29/50\n",
      " - 1s - loss: 82.4244 - squared_difference_loss: 38.0684 - KL_divergence_loss: 3.6592 - neg_log_likelihood: 78.7652 - val_loss: 82.3834 - val_squared_difference_loss: 37.8438 - val_KL_divergence_loss: 3.7305 - val_neg_log_likelihood: 78.6529\n",
      "Epoch 30/50\n",
      " - 1s - loss: 82.4137 - squared_difference_loss: 38.0541 - KL_divergence_loss: 3.6557 - neg_log_likelihood: 78.7581 - val_loss: 82.3522 - val_squared_difference_loss: 37.8293 - val_KL_divergence_loss: 3.7066 - val_neg_log_likelihood: 78.6456\n",
      "Epoch 31/50\n",
      " - 1s - loss: 82.4051 - squared_difference_loss: 38.0195 - KL_divergence_loss: 3.6643 - neg_log_likelihood: 78.7408 - val_loss: 82.3743 - val_squared_difference_loss: 37.7287 - val_KL_divergence_loss: 3.7789 - val_neg_log_likelihood: 78.5953\n",
      "Epoch 32/50\n",
      " - 1s - loss: 82.3815 - squared_difference_loss: 37.9583 - KL_divergence_loss: 3.6713 - neg_log_likelihood: 78.7102 - val_loss: 82.3400 - val_squared_difference_loss: 37.8321 - val_KL_divergence_loss: 3.6930 - val_neg_log_likelihood: 78.6470\n",
      "Epoch 33/50\n",
      " - 1s - loss: 82.3586 - squared_difference_loss: 37.9141 - KL_divergence_loss: 3.6706 - neg_log_likelihood: 78.6880 - val_loss: 82.3076 - val_squared_difference_loss: 37.5838 - val_KL_divergence_loss: 3.7847 - val_neg_log_likelihood: 78.5229\n",
      "Epoch 34/50\n",
      " - 1s - loss: 82.3644 - squared_difference_loss: 37.8734 - KL_divergence_loss: 3.6967 - neg_log_likelihood: 78.6677 - val_loss: 82.3623 - val_squared_difference_loss: 37.8580 - val_KL_divergence_loss: 3.7023 - val_neg_log_likelihood: 78.6600\n",
      "Epoch 35/50\n",
      " - 1s - loss: 82.3920 - squared_difference_loss: 37.9618 - KL_divergence_loss: 3.6801 - neg_log_likelihood: 78.7119 - val_loss: 82.3835 - val_squared_difference_loss: 37.8015 - val_KL_divergence_loss: 3.7517 - val_neg_log_likelihood: 78.6318\n",
      "Epoch 36/50\n",
      " - 1s - loss: 82.3090 - squared_difference_loss: 37.8153 - KL_divergence_loss: 3.6704 - neg_log_likelihood: 78.6386 - val_loss: 82.2873 - val_squared_difference_loss: 37.5100 - val_KL_divergence_loss: 3.8013 - val_neg_log_likelihood: 78.4860\n",
      "Epoch 37/50\n",
      " - 1s - loss: 82.3149 - squared_difference_loss: 37.7330 - KL_divergence_loss: 3.7175 - neg_log_likelihood: 78.5975 - val_loss: 82.2774 - val_squared_difference_loss: 37.6763 - val_KL_divergence_loss: 3.7083 - val_neg_log_likelihood: 78.5691\n",
      "Epoch 38/50\n",
      " - 1s - loss: 82.3302 - squared_difference_loss: 37.7631 - KL_divergence_loss: 3.7177 - neg_log_likelihood: 78.6125 - val_loss: 82.2601 - val_squared_difference_loss: 37.6934 - val_KL_divergence_loss: 3.6824 - val_neg_log_likelihood: 78.5777\n",
      "Epoch 39/50\n",
      " - 1s - loss: 82.2491 - squared_difference_loss: 37.6260 - KL_divergence_loss: 3.7051 - neg_log_likelihood: 78.5440 - val_loss: 82.2348 - val_squared_difference_loss: 37.5208 - val_KL_divergence_loss: 3.7434 - val_neg_log_likelihood: 78.4914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      " - 1s - loss: 82.2464 - squared_difference_loss: 37.6336 - KL_divergence_loss: 3.6986 - neg_log_likelihood: 78.5478 - val_loss: 82.1509 - val_squared_difference_loss: 37.1351 - val_KL_divergence_loss: 3.8523 - val_neg_log_likelihood: 78.2986\n",
      "Epoch 41/50\n",
      " - 1s - loss: 82.2246 - squared_difference_loss: 37.5420 - KL_divergence_loss: 3.7226 - neg_log_likelihood: 78.5020 - val_loss: 82.1994 - val_squared_difference_loss: 37.6717 - val_KL_divergence_loss: 3.6326 - val_neg_log_likelihood: 78.5669\n",
      "Epoch 42/50\n",
      " - 1s - loss: 82.2242 - squared_difference_loss: 37.5615 - KL_divergence_loss: 3.7125 - neg_log_likelihood: 78.5117 - val_loss: 82.2988 - val_squared_difference_loss: 37.5038 - val_KL_divergence_loss: 3.8159 - val_neg_log_likelihood: 78.4829\n",
      "Epoch 43/50\n",
      " - 1s - loss: 82.2360 - squared_difference_loss: 37.5081 - KL_divergence_loss: 3.7509 - neg_log_likelihood: 78.4850 - val_loss: 82.2937 - val_squared_difference_loss: 37.9190 - val_KL_divergence_loss: 3.6032 - val_neg_log_likelihood: 78.6905\n",
      "Epoch 44/50\n",
      " - 1s - loss: 82.2043 - squared_difference_loss: 37.5443 - KL_divergence_loss: 3.7012 - neg_log_likelihood: 78.5031 - val_loss: 82.1852 - val_squared_difference_loss: 37.2188 - val_KL_divergence_loss: 3.8448 - val_neg_log_likelihood: 78.3404\n",
      "Epoch 45/50\n",
      " - 1s - loss: 82.1659 - squared_difference_loss: 37.3827 - KL_divergence_loss: 3.7436 - neg_log_likelihood: 78.4223 - val_loss: 82.2374 - val_squared_difference_loss: 37.4769 - val_KL_divergence_loss: 3.7680 - val_neg_log_likelihood: 78.4694\n",
      "Epoch 46/50\n",
      " - 1s - loss: 82.1756 - squared_difference_loss: 37.4561 - KL_divergence_loss: 3.7166 - neg_log_likelihood: 78.4590 - val_loss: 82.1309 - val_squared_difference_loss: 37.2145 - val_KL_divergence_loss: 3.7926 - val_neg_log_likelihood: 78.3382\n",
      "Epoch 47/50\n",
      " - 1s - loss: 82.1994 - squared_difference_loss: 37.4697 - KL_divergence_loss: 3.7336 - neg_log_likelihood: 78.4658 - val_loss: 82.2140 - val_squared_difference_loss: 37.3410 - val_KL_divergence_loss: 3.8125 - val_neg_log_likelihood: 78.4015\n",
      "Epoch 48/50\n",
      " - 1s - loss: 82.1614 - squared_difference_loss: 37.3923 - KL_divergence_loss: 3.7343 - neg_log_likelihood: 78.4271 - val_loss: 82.1541 - val_squared_difference_loss: 37.2536 - val_KL_divergence_loss: 3.7963 - val_neg_log_likelihood: 78.3578\n",
      "Epoch 49/50\n",
      " - 1s - loss: 82.1497 - squared_difference_loss: 37.3771 - KL_divergence_loss: 3.7302 - neg_log_likelihood: 78.4196 - val_loss: 82.1520 - val_squared_difference_loss: 37.5315 - val_KL_divergence_loss: 3.6552 - val_neg_log_likelihood: 78.4968\n",
      "Epoch 50/50\n",
      " - 1s - loss: 82.1510 - squared_difference_loss: 37.3991 - KL_divergence_loss: 3.7205 - neg_log_likelihood: 78.4306 - val_loss: 82.1273 - val_squared_difference_loss: 37.2160 - val_KL_divergence_loss: 3.7883 - val_neg_log_likelihood: 78.3390\n",
      "\tAverage Validation Squared Error = 76.79060998796962\n",
      "Latent Var Num = 4\n",
      "Hidden Var Num = 50\n",
      "Simulation #11: VAE with 4 Latent Variable(s) and 50 Hidden Variable(s)...\n",
      "\tEvaluating model on Fold 1\n",
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/50\n",
      " - 14s - loss: 96.0903 - squared_difference_loss: 67.7899 - KL_divergence_loss: 2.4643 - neg_log_likelihood: 93.6260 - val_loss: 86.5296 - val_squared_difference_loss: 47.1296 - val_KL_divergence_loss: 3.2338 - val_neg_log_likelihood: 83.2958\n",
      "Epoch 2/50\n",
      " - 1s - loss: 85.2776 - squared_difference_loss: 45.3080 - KL_divergence_loss: 2.8926 - neg_log_likelihood: 82.3850 - val_loss: 84.2258 - val_squared_difference_loss: 41.2004 - val_KL_divergence_loss: 3.8947 - val_neg_log_likelihood: 80.3312\n",
      "Epoch 3/50\n",
      " - 1s - loss: 84.1945 - squared_difference_loss: 42.3845 - KL_divergence_loss: 3.2712 - neg_log_likelihood: 80.9233 - val_loss: 83.7126 - val_squared_difference_loss: 40.1658 - val_KL_divergence_loss: 3.8987 - val_neg_log_likelihood: 79.8139\n",
      "Epoch 4/50\n",
      " - 1s - loss: 83.9219 - squared_difference_loss: 41.6453 - KL_divergence_loss: 3.3682 - neg_log_likelihood: 80.5537 - val_loss: 83.4864 - val_squared_difference_loss: 39.5640 - val_KL_divergence_loss: 3.9734 - val_neg_log_likelihood: 79.5130\n",
      "Epoch 5/50\n",
      " - 1s - loss: 83.7602 - squared_difference_loss: 41.1550 - KL_divergence_loss: 3.4517 - neg_log_likelihood: 80.3085 - val_loss: 83.3054 - val_squared_difference_loss: 39.5766 - val_KL_divergence_loss: 3.7861 - val_neg_log_likelihood: 79.5193\n",
      "Epoch 6/50\n",
      " - 1s - loss: 83.5743 - squared_difference_loss: 40.6988 - KL_divergence_loss: 3.4939 - neg_log_likelihood: 80.0804 - val_loss: 83.2076 - val_squared_difference_loss: 39.2723 - val_KL_divergence_loss: 3.8405 - val_neg_log_likelihood: 79.3671\n",
      "Epoch 7/50\n",
      " - 1s - loss: 83.4463 - squared_difference_loss: 40.4061 - KL_divergence_loss: 3.5123 - neg_log_likelihood: 79.9340 - val_loss: 83.1013 - val_squared_difference_loss: 39.2127 - val_KL_divergence_loss: 3.7639 - val_neg_log_likelihood: 79.3374\n",
      "Epoch 8/50\n",
      " - 1s - loss: 83.3755 - squared_difference_loss: 40.1530 - KL_divergence_loss: 3.5680 - neg_log_likelihood: 79.8075 - val_loss: 83.1235 - val_squared_difference_loss: 39.3038 - val_KL_divergence_loss: 3.7406 - val_neg_log_likelihood: 79.3829\n",
      "Epoch 9/50\n",
      " - 1s - loss: 83.2277 - squared_difference_loss: 39.8232 - KL_divergence_loss: 3.5851 - neg_log_likelihood: 79.6426 - val_loss: 83.0920 - val_squared_difference_loss: 39.3143 - val_KL_divergence_loss: 3.7038 - val_neg_log_likelihood: 79.3881\n",
      "Epoch 10/50\n",
      " - 1s - loss: 83.1161 - squared_difference_loss: 39.5688 - KL_divergence_loss: 3.6007 - neg_log_likelihood: 79.5154 - val_loss: 82.9502 - val_squared_difference_loss: 38.9332 - val_KL_divergence_loss: 3.7527 - val_neg_log_likelihood: 79.1976\n",
      "Epoch 11/50\n",
      " - 1s - loss: 83.0834 - squared_difference_loss: 39.4926 - KL_divergence_loss: 3.6061 - neg_log_likelihood: 79.4773 - val_loss: 82.8731 - val_squared_difference_loss: 38.8117 - val_KL_divergence_loss: 3.7362 - val_neg_log_likelihood: 79.1369\n",
      "Epoch 12/50\n",
      " - 1s - loss: 82.9559 - squared_difference_loss: 39.2092 - KL_divergence_loss: 3.6203 - neg_log_likelihood: 79.3356 - val_loss: 82.9616 - val_squared_difference_loss: 39.1149 - val_KL_divergence_loss: 3.6732 - val_neg_log_likelihood: 79.2884\n",
      "Epoch 13/50\n",
      " - 1s - loss: 82.8953 - squared_difference_loss: 39.0990 - KL_divergence_loss: 3.6148 - neg_log_likelihood: 79.2805 - val_loss: 82.8580 - val_squared_difference_loss: 38.5794 - val_KL_divergence_loss: 3.8373 - val_neg_log_likelihood: 79.0207\n",
      "Epoch 14/50\n",
      " - 1s - loss: 82.8576 - squared_difference_loss: 38.9337 - KL_divergence_loss: 3.6597 - neg_log_likelihood: 79.1979 - val_loss: 82.7743 - val_squared_difference_loss: 38.6733 - val_KL_divergence_loss: 3.7066 - val_neg_log_likelihood: 79.0677\n",
      "Epoch 15/50\n",
      " - 1s - loss: 82.7471 - squared_difference_loss: 38.7602 - KL_divergence_loss: 3.6360 - neg_log_likelihood: 79.1111 - val_loss: 82.7329 - val_squared_difference_loss: 38.3415 - val_KL_divergence_loss: 3.8312 - val_neg_log_likelihood: 78.9017\n",
      "Epoch 16/50\n",
      " - 1s - loss: 82.7610 - squared_difference_loss: 38.6836 - KL_divergence_loss: 3.6882 - neg_log_likelihood: 79.0728 - val_loss: 82.6652 - val_squared_difference_loss: 38.3533 - val_KL_divergence_loss: 3.7576 - val_neg_log_likelihood: 78.9077\n",
      "Epoch 17/50\n",
      " - 1s - loss: 82.6541 - squared_difference_loss: 38.5452 - KL_divergence_loss: 3.6505 - neg_log_likelihood: 79.0036 - val_loss: 82.6809 - val_squared_difference_loss: 38.2609 - val_KL_divergence_loss: 3.8194 - val_neg_log_likelihood: 78.8614\n",
      "Epoch 18/50\n",
      " - 1s - loss: 82.6682 - squared_difference_loss: 38.4784 - KL_divergence_loss: 3.6980 - neg_log_likelihood: 78.9702 - val_loss: 82.6253 - val_squared_difference_loss: 38.0478 - val_KL_divergence_loss: 3.8704 - val_neg_log_likelihood: 78.7549\n",
      "Epoch 19/50\n",
      " - 1s - loss: 82.5779 - squared_difference_loss: 38.3261 - KL_divergence_loss: 3.6839 - neg_log_likelihood: 78.8941 - val_loss: 82.5630 - val_squared_difference_loss: 38.0557 - val_KL_divergence_loss: 3.8041 - val_neg_log_likelihood: 78.7589\n",
      "Epoch 20/50\n",
      " - 1s - loss: 82.4642 - squared_difference_loss: 38.0946 - KL_divergence_loss: 3.6859 - neg_log_likelihood: 78.7783 - val_loss: 82.5068 - val_squared_difference_loss: 37.8168 - val_KL_divergence_loss: 3.8674 - val_neg_log_likelihood: 78.6394\n",
      "Epoch 21/50\n",
      " - 1s - loss: 82.5357 - squared_difference_loss: 38.1860 - KL_divergence_loss: 3.7117 - neg_log_likelihood: 78.8240 - val_loss: 82.4998 - val_squared_difference_loss: 37.8381 - val_KL_divergence_loss: 3.8497 - val_neg_log_likelihood: 78.6501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n",
      " - 1s - loss: 82.4529 - squared_difference_loss: 37.9991 - KL_divergence_loss: 3.7224 - neg_log_likelihood: 78.7305 - val_loss: 82.4439 - val_squared_difference_loss: 38.0819 - val_KL_divergence_loss: 3.6719 - val_neg_log_likelihood: 78.7720\n",
      "Epoch 23/50\n",
      " - 1s - loss: 82.4432 - squared_difference_loss: 38.0036 - KL_divergence_loss: 3.7104 - neg_log_likelihood: 78.7328 - val_loss: 82.4371 - val_squared_difference_loss: 37.5043 - val_KL_divergence_loss: 3.9539 - val_neg_log_likelihood: 78.4832\n",
      "Epoch 24/50\n",
      " - 1s - loss: 82.3816 - squared_difference_loss: 37.7948 - KL_divergence_loss: 3.7532 - neg_log_likelihood: 78.6284 - val_loss: 82.3626 - val_squared_difference_loss: 37.7826 - val_KL_divergence_loss: 3.7403 - val_neg_log_likelihood: 78.6223\n",
      "Epoch 25/50\n",
      " - 1s - loss: 82.3822 - squared_difference_loss: 37.8635 - KL_divergence_loss: 3.7194 - neg_log_likelihood: 78.6628 - val_loss: 82.2907 - val_squared_difference_loss: 37.5180 - val_KL_divergence_loss: 3.8007 - val_neg_log_likelihood: 78.4900\n",
      "Epoch 26/50\n",
      " - 1s - loss: 82.3231 - squared_difference_loss: 37.7220 - KL_divergence_loss: 3.7311 - neg_log_likelihood: 78.5920 - val_loss: 82.2473 - val_squared_difference_loss: 37.5865 - val_KL_divergence_loss: 3.7231 - val_neg_log_likelihood: 78.5242\n",
      "Epoch 27/50\n",
      " - 1s - loss: 82.2857 - squared_difference_loss: 37.6563 - KL_divergence_loss: 3.7266 - neg_log_likelihood: 78.5592 - val_loss: 82.2984 - val_squared_difference_loss: 37.4554 - val_KL_divergence_loss: 3.8396 - val_neg_log_likelihood: 78.4587\n",
      "Epoch 28/50\n",
      " - 1s - loss: 82.3080 - squared_difference_loss: 37.6823 - KL_divergence_loss: 3.7359 - neg_log_likelihood: 78.5721 - val_loss: 82.3020 - val_squared_difference_loss: 37.3400 - val_KL_divergence_loss: 3.9010 - val_neg_log_likelihood: 78.4010\n",
      "Epoch 29/50\n",
      " - 1s - loss: 82.3203 - squared_difference_loss: 37.6628 - KL_divergence_loss: 3.7579 - neg_log_likelihood: 78.5624 - val_loss: 82.2744 - val_squared_difference_loss: 37.4024 - val_KL_divergence_loss: 3.8422 - val_neg_log_likelihood: 78.4322\n",
      "Epoch 30/50\n",
      " - 1s - loss: 82.2559 - squared_difference_loss: 37.5946 - KL_divergence_loss: 3.7276 - neg_log_likelihood: 78.5283 - val_loss: 82.1759 - val_squared_difference_loss: 37.0068 - val_KL_divergence_loss: 3.9415 - val_neg_log_likelihood: 78.2344\n",
      "Epoch 31/50\n",
      " - 1s - loss: 82.2249 - squared_difference_loss: 37.4698 - KL_divergence_loss: 3.7590 - neg_log_likelihood: 78.4659 - val_loss: 82.2333 - val_squared_difference_loss: 37.6205 - val_KL_divergence_loss: 3.6921 - val_neg_log_likelihood: 78.5412\n",
      "Epoch 32/50\n",
      " - 1s - loss: 82.2603 - squared_difference_loss: 37.5665 - KL_divergence_loss: 3.7460 - neg_log_likelihood: 78.5142 - val_loss: 82.1493 - val_squared_difference_loss: 37.2253 - val_KL_divergence_loss: 3.8056 - val_neg_log_likelihood: 78.3437\n",
      "Epoch 33/50\n",
      " - 1s - loss: 82.2091 - squared_difference_loss: 37.4281 - KL_divergence_loss: 3.7641 - neg_log_likelihood: 78.4450 - val_loss: 82.1986 - val_squared_difference_loss: 37.3525 - val_KL_divergence_loss: 3.7913 - val_neg_log_likelihood: 78.4073\n",
      "Epoch 34/50\n",
      " - 1s - loss: 82.2174 - squared_difference_loss: 37.3633 - KL_divergence_loss: 3.8048 - neg_log_likelihood: 78.4127 - val_loss: 82.0646 - val_squared_difference_loss: 37.0087 - val_KL_divergence_loss: 3.8292 - val_neg_log_likelihood: 78.2354\n",
      "Epoch 35/50\n",
      " - 1s - loss: 82.1439 - squared_difference_loss: 37.3229 - KL_divergence_loss: 3.7514 - neg_log_likelihood: 78.3925 - val_loss: 82.1185 - val_squared_difference_loss: 36.8261 - val_KL_divergence_loss: 3.9744 - val_neg_log_likelihood: 78.1440\n",
      "Epoch 36/50\n",
      " - 1s - loss: 82.2024 - squared_difference_loss: 37.3820 - KL_divergence_loss: 3.7804 - neg_log_likelihood: 78.4220 - val_loss: 82.0839 - val_squared_difference_loss: 36.9778 - val_KL_divergence_loss: 3.8640 - val_neg_log_likelihood: 78.2199\n",
      "Epoch 37/50\n",
      " - 1s - loss: 82.1676 - squared_difference_loss: 37.3102 - KL_divergence_loss: 3.7815 - neg_log_likelihood: 78.3861 - val_loss: 82.0846 - val_squared_difference_loss: 36.7949 - val_KL_divergence_loss: 3.9562 - val_neg_log_likelihood: 78.1284\n",
      "Epoch 38/50\n",
      " - 1s - loss: 82.1782 - squared_difference_loss: 37.2715 - KL_divergence_loss: 3.8114 - neg_log_likelihood: 78.3667 - val_loss: 82.1720 - val_squared_difference_loss: 37.2840 - val_KL_divergence_loss: 3.7990 - val_neg_log_likelihood: 78.3730\n",
      "Epoch 39/50\n",
      " - 1s - loss: 82.1276 - squared_difference_loss: 37.2575 - KL_divergence_loss: 3.7678 - neg_log_likelihood: 78.3597 - val_loss: 82.0659 - val_squared_difference_loss: 36.8191 - val_KL_divergence_loss: 3.9253 - val_neg_log_likelihood: 78.1405\n",
      "Epoch 40/50\n",
      " - 1s - loss: 82.0914 - squared_difference_loss: 37.0939 - KL_divergence_loss: 3.8134 - neg_log_likelihood: 78.2779 - val_loss: 82.0166 - val_squared_difference_loss: 36.7579 - val_KL_divergence_loss: 3.9066 - val_neg_log_likelihood: 78.1099\n",
      "Epoch 41/50\n",
      " - 1s - loss: 82.0789 - squared_difference_loss: 37.0901 - KL_divergence_loss: 3.8028 - neg_log_likelihood: 78.2761 - val_loss: 82.1014 - val_squared_difference_loss: 37.2228 - val_KL_divergence_loss: 3.7590 - val_neg_log_likelihood: 78.3424\n",
      "Epoch 42/50\n",
      " - 1s - loss: 82.0982 - squared_difference_loss: 37.0808 - KL_divergence_loss: 3.8268 - neg_log_likelihood: 78.2714 - val_loss: 81.9544 - val_squared_difference_loss: 36.8080 - val_KL_divergence_loss: 3.8194 - val_neg_log_likelihood: 78.1350\n",
      "Epoch 43/50\n",
      " - 1s - loss: 82.0444 - squared_difference_loss: 37.0234 - KL_divergence_loss: 3.8017 - neg_log_likelihood: 78.2427 - val_loss: 81.9391 - val_squared_difference_loss: 36.8796 - val_KL_divergence_loss: 3.7683 - val_neg_log_likelihood: 78.1708\n",
      "Epoch 44/50\n",
      " - 1s - loss: 82.0583 - squared_difference_loss: 37.0508 - KL_divergence_loss: 3.8019 - neg_log_likelihood: 78.2564 - val_loss: 82.0113 - val_squared_difference_loss: 36.9520 - val_KL_divergence_loss: 3.8043 - val_neg_log_likelihood: 78.2070\n",
      "Epoch 45/50\n",
      " - 1s - loss: 82.0616 - squared_difference_loss: 37.0115 - KL_divergence_loss: 3.8248 - neg_log_likelihood: 78.2368 - val_loss: 82.0085 - val_squared_difference_loss: 36.8044 - val_KL_divergence_loss: 3.8753 - val_neg_log_likelihood: 78.1332\n",
      "Epoch 46/50\n",
      " - 1s - loss: 82.0094 - squared_difference_loss: 36.9206 - KL_divergence_loss: 3.8181 - neg_log_likelihood: 78.1913 - val_loss: 81.9561 - val_squared_difference_loss: 36.5070 - val_KL_divergence_loss: 3.9716 - val_neg_log_likelihood: 77.9845\n",
      "Epoch 47/50\n",
      " - 1s - loss: 82.0397 - squared_difference_loss: 37.0063 - KL_divergence_loss: 3.8056 - neg_log_likelihood: 78.2342 - val_loss: 82.0547 - val_squared_difference_loss: 36.7543 - val_KL_divergence_loss: 3.9466 - val_neg_log_likelihood: 78.1082\n",
      "Epoch 48/50\n",
      " - 1s - loss: 82.0422 - squared_difference_loss: 36.9849 - KL_divergence_loss: 3.8187 - neg_log_likelihood: 78.2235 - val_loss: 81.9395 - val_squared_difference_loss: 36.6829 - val_KL_divergence_loss: 3.8670 - val_neg_log_likelihood: 78.0725\n",
      "Epoch 49/50\n",
      " - 1s - loss: 82.0092 - squared_difference_loss: 36.9341 - KL_divergence_loss: 3.8111 - neg_log_likelihood: 78.1981 - val_loss: 81.9432 - val_squared_difference_loss: 36.6689 - val_KL_divergence_loss: 3.8777 - val_neg_log_likelihood: 78.0655\n",
      "Epoch 50/50\n",
      " - 1s - loss: 82.0370 - squared_difference_loss: 36.9378 - KL_divergence_loss: 3.8371 - neg_log_likelihood: 78.1999 - val_loss: 81.9298 - val_squared_difference_loss: 36.7465 - val_KL_divergence_loss: 3.8255 - val_neg_log_likelihood: 78.1043\n",
      "\tAverage Validation Squared Error = 76.65136334299953\n",
      "Latent Var Num = 4\n",
      "Hidden Var Num = 60\n",
      "Simulation #12: VAE with 4 Latent Variable(s) and 60 Hidden Variable(s)...\n",
      "\tEvaluating model on Fold 1\n",
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/50\n",
      " - 14s - loss: 95.2261 - squared_difference_loss: 65.3082 - KL_divergence_loss: 2.8409 - neg_log_likelihood: 92.3851 - val_loss: 86.2939 - val_squared_difference_loss: 46.8119 - val_KL_divergence_loss: 3.1569 - val_neg_log_likelihood: 83.1369\n",
      "Epoch 2/50\n",
      " - 1s - loss: 85.2352 - squared_difference_loss: 45.1898 - KL_divergence_loss: 2.9093 - neg_log_likelihood: 82.3259 - val_loss: 84.3300 - val_squared_difference_loss: 41.6023 - val_KL_divergence_loss: 3.7979 - val_neg_log_likelihood: 80.5321\n",
      "Epoch 3/50\n",
      " - 1s - loss: 84.2834 - squared_difference_loss: 42.5289 - KL_divergence_loss: 3.2880 - neg_log_likelihood: 80.9954 - val_loss: 83.9291 - val_squared_difference_loss: 40.4216 - val_KL_divergence_loss: 3.9873 - val_neg_log_likelihood: 79.9418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      " - 1s - loss: 83.9656 - squared_difference_loss: 41.6592 - KL_divergence_loss: 3.4050 - neg_log_likelihood: 80.5606 - val_loss: 83.6893 - val_squared_difference_loss: 40.2308 - val_KL_divergence_loss: 3.8429 - val_neg_log_likelihood: 79.8464\n",
      "Epoch 5/50\n",
      " - 2s - loss: 83.7857 - squared_difference_loss: 41.1570 - KL_divergence_loss: 3.4762 - neg_log_likelihood: 80.3095 - val_loss: 83.5893 - val_squared_difference_loss: 40.0666 - val_KL_divergence_loss: 3.8250 - val_neg_log_likelihood: 79.7643\n",
      "Epoch 6/50\n",
      " - 1s - loss: 83.6530 - squared_difference_loss: 40.8865 - KL_divergence_loss: 3.4787 - neg_log_likelihood: 80.1742 - val_loss: 83.4784 - val_squared_difference_loss: 39.9118 - val_KL_divergence_loss: 3.7915 - val_neg_log_likelihood: 79.6869\n",
      "Epoch 7/50\n",
      " - 1s - loss: 83.5337 - squared_difference_loss: 40.6242 - KL_divergence_loss: 3.4906 - neg_log_likelihood: 80.0431 - val_loss: 83.3687 - val_squared_difference_loss: 40.0099 - val_KL_divergence_loss: 3.6328 - val_neg_log_likelihood: 79.7359\n",
      "Epoch 8/50\n",
      " - 1s - loss: 83.4258 - squared_difference_loss: 40.3720 - KL_divergence_loss: 3.5088 - neg_log_likelihood: 79.9170 - val_loss: 83.3298 - val_squared_difference_loss: 39.5884 - val_KL_divergence_loss: 3.8046 - val_neg_log_likelihood: 79.5252\n",
      "Epoch 9/50\n",
      " - 1s - loss: 83.2609 - squared_difference_loss: 40.0074 - KL_divergence_loss: 3.5262 - neg_log_likelihood: 79.7347 - val_loss: 83.3347 - val_squared_difference_loss: 39.7046 - val_KL_divergence_loss: 3.7514 - val_neg_log_likelihood: 79.5833\n",
      "Epoch 10/50\n",
      " - 1s - loss: 83.1885 - squared_difference_loss: 39.8341 - KL_divergence_loss: 3.5404 - neg_log_likelihood: 79.6481 - val_loss: 83.2386 - val_squared_difference_loss: 39.5321 - val_KL_divergence_loss: 3.7415 - val_neg_log_likelihood: 79.4971\n",
      "Epoch 11/50\n",
      " - 1s - loss: 83.1029 - squared_difference_loss: 39.6303 - KL_divergence_loss: 3.5567 - neg_log_likelihood: 79.5462 - val_loss: 83.2658 - val_squared_difference_loss: 39.6620 - val_KL_divergence_loss: 3.7038 - val_neg_log_likelihood: 79.5620\n",
      "Epoch 12/50\n",
      " - 1s - loss: 83.0547 - squared_difference_loss: 39.5089 - KL_divergence_loss: 3.5692 - neg_log_likelihood: 79.4855 - val_loss: 83.1975 - val_squared_difference_loss: 39.5376 - val_KL_divergence_loss: 3.6977 - val_neg_log_likelihood: 79.4998\n",
      "Epoch 13/50\n",
      " - 1s - loss: 82.9651 - squared_difference_loss: 39.2660 - KL_divergence_loss: 3.6011 - neg_log_likelihood: 79.3640 - val_loss: 83.2225 - val_squared_difference_loss: 39.6114 - val_KL_divergence_loss: 3.6858 - val_neg_log_likelihood: 79.5367\n",
      "Epoch 14/50\n",
      " - 1s - loss: 82.9119 - squared_difference_loss: 39.2128 - KL_divergence_loss: 3.5746 - neg_log_likelihood: 79.3374 - val_loss: 83.1652 - val_squared_difference_loss: 39.0973 - val_KL_divergence_loss: 3.8855 - val_neg_log_likelihood: 79.2797\n",
      "Epoch 15/50\n",
      " - 1s - loss: 82.8457 - squared_difference_loss: 39.0051 - KL_divergence_loss: 3.6122 - neg_log_likelihood: 79.2336 - val_loss: 82.9620 - val_squared_difference_loss: 39.1370 - val_KL_divergence_loss: 3.6625 - val_neg_log_likelihood: 79.2995\n",
      "Epoch 16/50\n",
      " - 1s - loss: 82.7760 - squared_difference_loss: 38.8772 - KL_divergence_loss: 3.6063 - neg_log_likelihood: 79.1696 - val_loss: 82.9172 - val_squared_difference_loss: 38.9098 - val_KL_divergence_loss: 3.7313 - val_neg_log_likelihood: 79.1859\n",
      "Epoch 17/50\n",
      " - 1s - loss: 82.7198 - squared_difference_loss: 38.7781 - KL_divergence_loss: 3.5997 - neg_log_likelihood: 79.1200 - val_loss: 82.8836 - val_squared_difference_loss: 38.8871 - val_KL_divergence_loss: 3.7090 - val_neg_log_likelihood: 79.1746\n",
      "Epoch 18/50\n",
      " - 1s - loss: 82.7190 - squared_difference_loss: 38.7427 - KL_divergence_loss: 3.6167 - neg_log_likelihood: 79.1023 - val_loss: 82.8775 - val_squared_difference_loss: 38.9221 - val_KL_divergence_loss: 3.6854 - val_neg_log_likelihood: 79.1921\n",
      "Epoch 19/50\n",
      " - 1s - loss: 82.6635 - squared_difference_loss: 38.5763 - KL_divergence_loss: 3.6444 - neg_log_likelihood: 79.0192 - val_loss: 82.8458 - val_squared_difference_loss: 38.6391 - val_KL_divergence_loss: 3.7952 - val_neg_log_likelihood: 79.0506\n",
      "Epoch 20/50\n",
      " - 1s - loss: 82.6137 - squared_difference_loss: 38.5017 - KL_divergence_loss: 3.6318 - neg_log_likelihood: 78.9818 - val_loss: 82.7916 - val_squared_difference_loss: 38.4928 - val_KL_divergence_loss: 3.8142 - val_neg_log_likelihood: 78.9774\n",
      "Epoch 21/50\n",
      " - 1s - loss: 82.5910 - squared_difference_loss: 38.4446 - KL_divergence_loss: 3.6377 - neg_log_likelihood: 78.9533 - val_loss: 82.7118 - val_squared_difference_loss: 38.4989 - val_KL_divergence_loss: 3.7314 - val_neg_log_likelihood: 78.9804\n",
      "Epoch 22/50\n",
      " - 1s - loss: 82.5466 - squared_difference_loss: 38.3897 - KL_divergence_loss: 3.6208 - neg_log_likelihood: 78.9259 - val_loss: 82.6079 - val_squared_difference_loss: 38.2712 - val_KL_divergence_loss: 3.7413 - val_neg_log_likelihood: 78.8666\n",
      "Epoch 23/50\n",
      " - 1s - loss: 82.5336 - squared_difference_loss: 38.3325 - KL_divergence_loss: 3.6364 - neg_log_likelihood: 78.8972 - val_loss: 82.5357 - val_squared_difference_loss: 38.3171 - val_KL_divergence_loss: 3.6462 - val_neg_log_likelihood: 78.8895\n",
      "Epoch 24/50\n",
      " - 1s - loss: 82.4748 - squared_difference_loss: 38.1450 - KL_divergence_loss: 3.6713 - neg_log_likelihood: 78.8035 - val_loss: 82.5063 - val_squared_difference_loss: 38.1671 - val_KL_divergence_loss: 3.6917 - val_neg_log_likelihood: 78.8146\n",
      "Epoch 25/50\n",
      " - 1s - loss: 82.5245 - squared_difference_loss: 38.3015 - KL_divergence_loss: 3.6427 - neg_log_likelihood: 78.8818 - val_loss: 82.6600 - val_squared_difference_loss: 38.2502 - val_KL_divergence_loss: 3.8039 - val_neg_log_likelihood: 78.8561\n",
      "Epoch 26/50\n",
      " - 1s - loss: 82.5383 - squared_difference_loss: 38.2273 - KL_divergence_loss: 3.6937 - neg_log_likelihood: 78.8446 - val_loss: 82.4965 - val_squared_difference_loss: 37.8891 - val_KL_divergence_loss: 3.8210 - val_neg_log_likelihood: 78.6756\n",
      "Epoch 27/50\n",
      " - 1s - loss: 82.4575 - squared_difference_loss: 38.1122 - KL_divergence_loss: 3.6704 - neg_log_likelihood: 78.7871 - val_loss: 82.4871 - val_squared_difference_loss: 37.9728 - val_KL_divergence_loss: 3.7698 - val_neg_log_likelihood: 78.7174\n",
      "Epoch 28/50\n",
      " - 1s - loss: 82.3993 - squared_difference_loss: 38.0327 - KL_divergence_loss: 3.6519 - neg_log_likelihood: 78.7474 - val_loss: 82.4373 - val_squared_difference_loss: 37.8102 - val_KL_divergence_loss: 3.8012 - val_neg_log_likelihood: 78.6361\n",
      "Epoch 29/50\n",
      " - 1s - loss: 82.3707 - squared_difference_loss: 37.9105 - KL_divergence_loss: 3.6844 - neg_log_likelihood: 78.6863 - val_loss: 82.4628 - val_squared_difference_loss: 38.0929 - val_KL_divergence_loss: 3.6854 - val_neg_log_likelihood: 78.7774\n",
      "Epoch 30/50\n",
      " - 1s - loss: 82.4139 - squared_difference_loss: 38.0399 - KL_divergence_loss: 3.6630 - neg_log_likelihood: 78.7510 - val_loss: 82.3755 - val_squared_difference_loss: 37.6790 - val_KL_divergence_loss: 3.8050 - val_neg_log_likelihood: 78.5705\n",
      "Epoch 31/50\n",
      " - 1s - loss: 82.3309 - squared_difference_loss: 37.8383 - KL_divergence_loss: 3.6808 - neg_log_likelihood: 78.6502 - val_loss: 82.4206 - val_squared_difference_loss: 37.7898 - val_KL_divergence_loss: 3.7947 - val_neg_log_likelihood: 78.6259\n",
      "Epoch 32/50\n",
      " - 1s - loss: 82.3693 - squared_difference_loss: 37.8522 - KL_divergence_loss: 3.7122 - neg_log_likelihood: 78.6571 - val_loss: 82.3308 - val_squared_difference_loss: 37.7203 - val_KL_divergence_loss: 3.7397 - val_neg_log_likelihood: 78.5912\n",
      "Epoch 33/50\n",
      " - 1s - loss: 82.3486 - squared_difference_loss: 37.8078 - KL_divergence_loss: 3.7137 - neg_log_likelihood: 78.6349 - val_loss: 82.3312 - val_squared_difference_loss: 37.6001 - val_KL_divergence_loss: 3.8002 - val_neg_log_likelihood: 78.5310\n",
      "Epoch 34/50\n",
      " - 1s - loss: 82.2942 - squared_difference_loss: 37.7137 - KL_divergence_loss: 3.7063 - neg_log_likelihood: 78.5879 - val_loss: 82.2337 - val_squared_difference_loss: 37.6196 - val_KL_divergence_loss: 3.6929 - val_neg_log_likelihood: 78.5408\n",
      "Epoch 35/50\n",
      " - 1s - loss: 82.2919 - squared_difference_loss: 37.7125 - KL_divergence_loss: 3.7046 - neg_log_likelihood: 78.5873 - val_loss: 82.3589 - val_squared_difference_loss: 37.6068 - val_KL_divergence_loss: 3.8245 - val_neg_log_likelihood: 78.5344\n",
      "Epoch 36/50\n",
      " - 1s - loss: 82.2762 - squared_difference_loss: 37.6089 - KL_divergence_loss: 3.7408 - neg_log_likelihood: 78.5355 - val_loss: 82.2429 - val_squared_difference_loss: 37.5522 - val_KL_divergence_loss: 3.7358 - val_neg_log_likelihood: 78.5071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50\n",
      " - 1s - loss: 82.2518 - squared_difference_loss: 37.6302 - KL_divergence_loss: 3.7057 - neg_log_likelihood: 78.5461 - val_loss: 82.2841 - val_squared_difference_loss: 37.3199 - val_KL_divergence_loss: 3.8932 - val_neg_log_likelihood: 78.3909\n",
      "Epoch 38/50\n",
      " - 1s - loss: 82.2706 - squared_difference_loss: 37.6200 - KL_divergence_loss: 3.7296 - neg_log_likelihood: 78.5410 - val_loss: 82.2837 - val_squared_difference_loss: 37.7225 - val_KL_divergence_loss: 3.6914 - val_neg_log_likelihood: 78.5922\n",
      "Epoch 39/50\n",
      " - 1s - loss: 82.2146 - squared_difference_loss: 37.5744 - KL_divergence_loss: 3.6964 - neg_log_likelihood: 78.5182 - val_loss: 82.4693 - val_squared_difference_loss: 37.4964 - val_KL_divergence_loss: 3.9902 - val_neg_log_likelihood: 78.4792\n",
      "Epoch 40/50\n",
      " - 1s - loss: 82.2612 - squared_difference_loss: 37.5779 - KL_divergence_loss: 3.7413 - neg_log_likelihood: 78.5199 - val_loss: 82.2193 - val_squared_difference_loss: 37.3202 - val_KL_divergence_loss: 3.8282 - val_neg_log_likelihood: 78.3911\n",
      "Epoch 41/50\n",
      " - 1s - loss: 82.2068 - squared_difference_loss: 37.4573 - KL_divergence_loss: 3.7471 - neg_log_likelihood: 78.4596 - val_loss: 82.1605 - val_squared_difference_loss: 37.3035 - val_KL_divergence_loss: 3.7778 - val_neg_log_likelihood: 78.3827\n",
      "Epoch 42/50\n",
      " - 1s - loss: 82.1964 - squared_difference_loss: 37.3765 - KL_divergence_loss: 3.7772 - neg_log_likelihood: 78.4193 - val_loss: 82.1219 - val_squared_difference_loss: 37.4229 - val_KL_divergence_loss: 3.6795 - val_neg_log_likelihood: 78.4424\n",
      "Epoch 43/50\n",
      " - 1s - loss: 82.1528 - squared_difference_loss: 37.3819 - KL_divergence_loss: 3.7309 - neg_log_likelihood: 78.4219 - val_loss: 82.1922 - val_squared_difference_loss: 37.3745 - val_KL_divergence_loss: 3.7740 - val_neg_log_likelihood: 78.4183\n",
      "Epoch 44/50\n",
      " - 1s - loss: 82.1228 - squared_difference_loss: 37.2814 - KL_divergence_loss: 3.7511 - neg_log_likelihood: 78.3717 - val_loss: 82.1092 - val_squared_difference_loss: 36.9972 - val_KL_divergence_loss: 3.8797 - val_neg_log_likelihood: 78.2296\n",
      "Epoch 45/50\n",
      " - 1s - loss: 82.1259 - squared_difference_loss: 37.2577 - KL_divergence_loss: 3.7661 - neg_log_likelihood: 78.3599 - val_loss: 82.1213 - val_squared_difference_loss: 37.1105 - val_KL_divergence_loss: 3.8351 - val_neg_log_likelihood: 78.2863\n",
      "Epoch 46/50\n",
      " - 1s - loss: 82.1140 - squared_difference_loss: 37.2210 - KL_divergence_loss: 3.7725 - neg_log_likelihood: 78.3415 - val_loss: 82.0789 - val_squared_difference_loss: 37.0016 - val_KL_divergence_loss: 3.8471 - val_neg_log_likelihood: 78.2318\n",
      "Epoch 47/50\n",
      " - 1s - loss: 82.1403 - squared_difference_loss: 37.2795 - KL_divergence_loss: 3.7696 - neg_log_likelihood: 78.3707 - val_loss: 82.1189 - val_squared_difference_loss: 37.1904 - val_KL_divergence_loss: 3.7927 - val_neg_log_likelihood: 78.3262\n",
      "Epoch 48/50\n",
      " - 1s - loss: 82.1269 - squared_difference_loss: 37.2613 - KL_divergence_loss: 3.7653 - neg_log_likelihood: 78.3617 - val_loss: 82.0758 - val_squared_difference_loss: 36.9196 - val_KL_divergence_loss: 3.8850 - val_neg_log_likelihood: 78.1908\n",
      "Epoch 49/50\n",
      " - 1s - loss: 82.0888 - squared_difference_loss: 37.1690 - KL_divergence_loss: 3.7733 - neg_log_likelihood: 78.3155 - val_loss: 82.0891 - val_squared_difference_loss: 37.1489 - val_KL_divergence_loss: 3.7836 - val_neg_log_likelihood: 78.3055\n",
      "Epoch 50/50\n",
      " - 1s - loss: 82.1036 - squared_difference_loss: 37.1807 - KL_divergence_loss: 3.7823 - neg_log_likelihood: 78.3214 - val_loss: 82.1467 - val_squared_difference_loss: 37.2321 - val_KL_divergence_loss: 3.7996 - val_neg_log_likelihood: 78.3471\n",
      "\tAverage Validation Squared Error = 76.9247654557925\n",
      "Latent Var Num = 5\n",
      "Hidden Var Num = 10\n",
      "Simulation #13: VAE with 5 Latent Variable(s) and 10 Hidden Variable(s)...\n",
      "\tEvaluating model on Fold 1\n",
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/50\n",
      " - 14s - loss: 108.8871 - squared_difference_loss: 91.6314 - KL_divergence_loss: 3.3403 - neg_log_likelihood: 105.5467 - val_loss: 91.2947 - val_squared_difference_loss: 56.7231 - val_KL_divergence_loss: 3.2022 - val_neg_log_likelihood: 88.0926\n",
      "Epoch 2/50\n",
      " - 1s - loss: 88.2426 - squared_difference_loss: 53.0039 - KL_divergence_loss: 2.0096 - neg_log_likelihood: 86.2329 - val_loss: 86.9570 - val_squared_difference_loss: 48.8505 - val_KL_divergence_loss: 2.8007 - val_neg_log_likelihood: 84.1563\n",
      "Epoch 3/50\n",
      " - 1s - loss: 86.5746 - squared_difference_loss: 49.7988 - KL_divergence_loss: 1.9441 - neg_log_likelihood: 84.6304 - val_loss: 86.1325 - val_squared_difference_loss: 47.4115 - val_KL_divergence_loss: 2.6957 - val_neg_log_likelihood: 83.4367\n",
      "Epoch 4/50\n",
      " - 1s - loss: 86.1260 - squared_difference_loss: 48.7101 - KL_divergence_loss: 2.0400 - neg_log_likelihood: 84.0861 - val_loss: 85.6872 - val_squared_difference_loss: 46.7921 - val_KL_divergence_loss: 2.5601 - val_neg_log_likelihood: 83.1271\n",
      "Epoch 5/50\n",
      " - 1s - loss: 85.8847 - squared_difference_loss: 48.1103 - KL_divergence_loss: 2.0985 - neg_log_likelihood: 83.7862 - val_loss: 85.3990 - val_squared_difference_loss: 46.2096 - val_KL_divergence_loss: 2.5631 - val_neg_log_likelihood: 82.8358\n",
      "Epoch 6/50\n",
      " - 1s - loss: 85.6440 - squared_difference_loss: 47.3306 - KL_divergence_loss: 2.2477 - neg_log_likelihood: 83.3963 - val_loss: 85.1311 - val_squared_difference_loss: 45.3120 - val_KL_divergence_loss: 2.7441 - val_neg_log_likelihood: 82.3870\n",
      "Epoch 7/50\n",
      " - 1s - loss: 85.3288 - squared_difference_loss: 46.1830 - KL_divergence_loss: 2.5063 - neg_log_likelihood: 82.8225 - val_loss: 84.8053 - val_squared_difference_loss: 44.2448 - val_KL_divergence_loss: 2.9519 - val_neg_log_likelihood: 81.8534\n",
      "Epoch 8/50\n",
      " - 1s - loss: 85.0141 - squared_difference_loss: 45.0736 - KL_divergence_loss: 2.7463 - neg_log_likelihood: 82.2678 - val_loss: 84.5648 - val_squared_difference_loss: 43.4143 - val_KL_divergence_loss: 3.1266 - val_neg_log_likelihood: 81.4382\n",
      "Epoch 9/50\n",
      " - 1s - loss: 84.7493 - squared_difference_loss: 44.1981 - KL_divergence_loss: 2.9192 - neg_log_likelihood: 81.8301 - val_loss: 84.4156 - val_squared_difference_loss: 42.8839 - val_KL_divergence_loss: 3.2427 - val_neg_log_likelihood: 81.1730\n",
      "Epoch 10/50\n",
      " - 1s - loss: 84.5308 - squared_difference_loss: 43.5103 - KL_divergence_loss: 3.0446 - neg_log_likelihood: 81.4862 - val_loss: 84.3373 - val_squared_difference_loss: 42.8871 - val_KL_divergence_loss: 3.1628 - val_neg_log_likelihood: 81.1745\n",
      "Epoch 11/50\n",
      " - 1s - loss: 84.3919 - squared_difference_loss: 43.1204 - KL_divergence_loss: 3.1007 - neg_log_likelihood: 81.2912 - val_loss: 84.4326 - val_squared_difference_loss: 43.2111 - val_KL_divergence_loss: 3.0961 - val_neg_log_likelihood: 81.3365\n",
      "Epoch 12/50\n",
      " - 1s - loss: 84.2924 - squared_difference_loss: 42.8709 - KL_divergence_loss: 3.1259 - neg_log_likelihood: 81.1665 - val_loss: 84.2900 - val_squared_difference_loss: 42.7921 - val_KL_divergence_loss: 3.1630 - val_neg_log_likelihood: 81.1270\n",
      "Epoch 13/50\n",
      " - 1s - loss: 84.2508 - squared_difference_loss: 42.6800 - KL_divergence_loss: 3.1799 - neg_log_likelihood: 81.0710 - val_loss: 84.2358 - val_squared_difference_loss: 42.5885 - val_KL_divergence_loss: 3.2105 - val_neg_log_likelihood: 81.0252\n",
      "Epoch 14/50\n",
      " - 1s - loss: 84.1850 - squared_difference_loss: 42.5110 - KL_divergence_loss: 3.1985 - neg_log_likelihood: 80.9865 - val_loss: 84.2295 - val_squared_difference_loss: 42.7023 - val_KL_divergence_loss: 3.1473 - val_neg_log_likelihood: 81.0822\n",
      "Epoch 15/50\n",
      " - 1s - loss: 84.1536 - squared_difference_loss: 42.3671 - KL_divergence_loss: 3.2390 - neg_log_likelihood: 80.9146 - val_loss: 84.0504 - val_squared_difference_loss: 42.2272 - val_KL_divergence_loss: 3.2058 - val_neg_log_likelihood: 80.8446\n",
      "Epoch 16/50\n",
      " - 1s - loss: 84.0935 - squared_difference_loss: 42.2434 - KL_divergence_loss: 3.2408 - neg_log_likelihood: 80.8527 - val_loss: 83.9832 - val_squared_difference_loss: 42.1049 - val_KL_divergence_loss: 3.1998 - val_neg_log_likelihood: 80.7834\n",
      "Epoch 17/50\n",
      " - 1s - loss: 84.0578 - squared_difference_loss: 42.1648 - KL_divergence_loss: 3.2444 - neg_log_likelihood: 80.8134 - val_loss: 83.8562 - val_squared_difference_loss: 41.7003 - val_KL_divergence_loss: 3.2751 - val_neg_log_likelihood: 80.5812\n",
      "Epoch 18/50\n",
      " - 1s - loss: 84.0012 - squared_difference_loss: 41.9346 - KL_divergence_loss: 3.3029 - neg_log_likelihood: 80.6983 - val_loss: 83.8897 - val_squared_difference_loss: 41.8619 - val_KL_divergence_loss: 3.2278 - val_neg_log_likelihood: 80.6619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50\n",
      " - 1s - loss: 83.9700 - squared_difference_loss: 41.8156 - KL_divergence_loss: 3.3312 - neg_log_likelihood: 80.6388 - val_loss: 83.8025 - val_squared_difference_loss: 41.3596 - val_KL_divergence_loss: 3.3917 - val_neg_log_likelihood: 80.4108\n",
      "Epoch 20/50\n",
      " - 1s - loss: 83.9383 - squared_difference_loss: 41.6889 - KL_divergence_loss: 3.3628 - neg_log_likelihood: 80.5755 - val_loss: 83.7506 - val_squared_difference_loss: 41.1530 - val_KL_divergence_loss: 3.4431 - val_neg_log_likelihood: 80.3075\n",
      "Epoch 21/50\n",
      " - 1s - loss: 83.8904 - squared_difference_loss: 41.5256 - KL_divergence_loss: 3.3966 - neg_log_likelihood: 80.4938 - val_loss: 83.7438 - val_squared_difference_loss: 41.1848 - val_KL_divergence_loss: 3.4204 - val_neg_log_likelihood: 80.3234\n",
      "Epoch 22/50\n",
      " - 1s - loss: 83.8407 - squared_difference_loss: 41.4456 - KL_divergence_loss: 3.3869 - neg_log_likelihood: 80.4538 - val_loss: 83.5777 - val_squared_difference_loss: 40.7190 - val_KL_divergence_loss: 3.4872 - val_neg_log_likelihood: 80.0905\n",
      "Epoch 23/50\n",
      " - 1s - loss: 83.8136 - squared_difference_loss: 41.3163 - KL_divergence_loss: 3.4244 - neg_log_likelihood: 80.3892 - val_loss: 83.5506 - val_squared_difference_loss: 40.7519 - val_KL_divergence_loss: 3.4436 - val_neg_log_likelihood: 80.1070\n",
      "Epoch 24/50\n",
      " - 1s - loss: 83.8027 - squared_difference_loss: 41.2292 - KL_divergence_loss: 3.4571 - neg_log_likelihood: 80.3456 - val_loss: 83.5057 - val_squared_difference_loss: 40.4519 - val_KL_divergence_loss: 3.5488 - val_neg_log_likelihood: 79.9570\n",
      "Epoch 25/50\n",
      " - 1s - loss: 83.7658 - squared_difference_loss: 41.1293 - KL_divergence_loss: 3.4702 - neg_log_likelihood: 80.2956 - val_loss: 83.4784 - val_squared_difference_loss: 40.3604 - val_KL_divergence_loss: 3.5672 - val_neg_log_likelihood: 79.9112\n",
      "Epoch 26/50\n",
      " - 1s - loss: 83.7023 - squared_difference_loss: 40.9819 - KL_divergence_loss: 3.4803 - neg_log_likelihood: 80.2219 - val_loss: 83.4125 - val_squared_difference_loss: 40.3126 - val_KL_divergence_loss: 3.5252 - val_neg_log_likelihood: 79.8873\n",
      "Epoch 27/50\n",
      " - 1s - loss: 83.6766 - squared_difference_loss: 40.8785 - KL_divergence_loss: 3.5064 - neg_log_likelihood: 80.1702 - val_loss: 83.3193 - val_squared_difference_loss: 39.9873 - val_KL_divergence_loss: 3.5947 - val_neg_log_likelihood: 79.7247\n",
      "Epoch 28/50\n",
      " - 1s - loss: 83.6518 - squared_difference_loss: 40.7528 - KL_divergence_loss: 3.5444 - neg_log_likelihood: 80.1074 - val_loss: 83.3048 - val_squared_difference_loss: 40.0026 - val_KL_divergence_loss: 3.5725 - val_neg_log_likelihood: 79.7323\n",
      "Epoch 29/50\n",
      " - 1s - loss: 83.6086 - squared_difference_loss: 40.6350 - KL_divergence_loss: 3.5601 - neg_log_likelihood: 80.0485 - val_loss: 83.3069 - val_squared_difference_loss: 39.9131 - val_KL_divergence_loss: 3.6193 - val_neg_log_likelihood: 79.6876\n",
      "Epoch 30/50\n",
      " - 1s - loss: 83.5768 - squared_difference_loss: 40.6033 - KL_divergence_loss: 3.5441 - neg_log_likelihood: 80.0327 - val_loss: 83.2054 - val_squared_difference_loss: 39.6080 - val_KL_divergence_loss: 3.6704 - val_neg_log_likelihood: 79.5350\n",
      "Epoch 31/50\n",
      " - 1s - loss: 83.5691 - squared_difference_loss: 40.4661 - KL_divergence_loss: 3.6050 - neg_log_likelihood: 79.9641 - val_loss: 83.1773 - val_squared_difference_loss: 39.6183 - val_KL_divergence_loss: 3.6372 - val_neg_log_likelihood: 79.5401\n",
      "Epoch 32/50\n",
      " - 1s - loss: 83.5612 - squared_difference_loss: 40.4966 - KL_divergence_loss: 3.5819 - neg_log_likelihood: 79.9793 - val_loss: 83.1112 - val_squared_difference_loss: 39.3574 - val_KL_divergence_loss: 3.7015 - val_neg_log_likelihood: 79.4097\n",
      "Epoch 33/50\n",
      " - 1s - loss: 83.5215 - squared_difference_loss: 40.3403 - KL_divergence_loss: 3.6203 - neg_log_likelihood: 79.9012 - val_loss: 83.1344 - val_squared_difference_loss: 39.5525 - val_KL_divergence_loss: 3.6271 - val_neg_log_likelihood: 79.5073\n",
      "Epoch 34/50\n",
      " - 1s - loss: 83.4757 - squared_difference_loss: 40.2489 - KL_divergence_loss: 3.6202 - neg_log_likelihood: 79.8555 - val_loss: 83.1776 - val_squared_difference_loss: 39.4090 - val_KL_divergence_loss: 3.7421 - val_neg_log_likelihood: 79.4355\n",
      "Epoch 35/50\n",
      " - 1s - loss: 83.4344 - squared_difference_loss: 40.1663 - KL_divergence_loss: 3.6203 - neg_log_likelihood: 79.8142 - val_loss: 83.0529 - val_squared_difference_loss: 39.3634 - val_KL_divergence_loss: 3.6402 - val_neg_log_likelihood: 79.4127\n",
      "Epoch 36/50\n",
      " - 1s - loss: 83.4063 - squared_difference_loss: 40.0646 - KL_divergence_loss: 3.6430 - neg_log_likelihood: 79.7633 - val_loss: 82.9989 - val_squared_difference_loss: 38.9136 - val_KL_divergence_loss: 3.8111 - val_neg_log_likelihood: 79.1878\n",
      "Epoch 37/50\n",
      " - 1s - loss: 83.3906 - squared_difference_loss: 40.0587 - KL_divergence_loss: 3.6302 - neg_log_likelihood: 79.7603 - val_loss: 83.0469 - val_squared_difference_loss: 39.2141 - val_KL_divergence_loss: 3.7088 - val_neg_log_likelihood: 79.3381\n",
      "Epoch 38/50\n",
      " - 1s - loss: 83.3733 - squared_difference_loss: 39.9578 - KL_divergence_loss: 3.6634 - neg_log_likelihood: 79.7099 - val_loss: 83.0500 - val_squared_difference_loss: 39.0561 - val_KL_divergence_loss: 3.7909 - val_neg_log_likelihood: 79.2590\n",
      "Epoch 39/50\n",
      " - 1s - loss: 83.3251 - squared_difference_loss: 39.8972 - KL_divergence_loss: 3.6455 - neg_log_likelihood: 79.6796 - val_loss: 83.0247 - val_squared_difference_loss: 39.0970 - val_KL_divergence_loss: 3.7453 - val_neg_log_likelihood: 79.2795\n",
      "Epoch 40/50\n",
      " - 1s - loss: 83.3344 - squared_difference_loss: 39.9187 - KL_divergence_loss: 3.6440 - neg_log_likelihood: 79.6903 - val_loss: 83.0133 - val_squared_difference_loss: 39.0146 - val_KL_divergence_loss: 3.7750 - val_neg_log_likelihood: 79.2383\n",
      "Epoch 41/50\n",
      " - 1s - loss: 83.3165 - squared_difference_loss: 39.8509 - KL_divergence_loss: 3.6601 - neg_log_likelihood: 79.6564 - val_loss: 82.9356 - val_squared_difference_loss: 38.9465 - val_KL_divergence_loss: 3.7313 - val_neg_log_likelihood: 79.2042\n",
      "Epoch 42/50\n",
      " - 1s - loss: 83.3206 - squared_difference_loss: 39.8305 - KL_divergence_loss: 3.6743 - neg_log_likelihood: 79.6463 - val_loss: 82.9187 - val_squared_difference_loss: 38.9234 - val_KL_divergence_loss: 3.7260 - val_neg_log_likelihood: 79.1927\n",
      "Epoch 43/50\n",
      " - 1s - loss: 83.2936 - squared_difference_loss: 39.7961 - KL_divergence_loss: 3.6645 - neg_log_likelihood: 79.6291 - val_loss: 82.9217 - val_squared_difference_loss: 38.9819 - val_KL_divergence_loss: 3.6997 - val_neg_log_likelihood: 79.2220\n",
      "Epoch 44/50\n",
      " - 1s - loss: 83.2803 - squared_difference_loss: 39.8164 - KL_divergence_loss: 3.6411 - neg_log_likelihood: 79.6392 - val_loss: 82.9370 - val_squared_difference_loss: 38.8615 - val_KL_divergence_loss: 3.7753 - val_neg_log_likelihood: 79.1617\n",
      "Epoch 45/50\n",
      " - 1s - loss: 83.2760 - squared_difference_loss: 39.7372 - KL_divergence_loss: 3.6764 - neg_log_likelihood: 79.5996 - val_loss: 82.8899 - val_squared_difference_loss: 38.9440 - val_KL_divergence_loss: 3.6869 - val_neg_log_likelihood: 79.2030\n",
      "Epoch 46/50\n",
      " - 1s - loss: 83.2757 - squared_difference_loss: 39.7302 - KL_divergence_loss: 3.6796 - neg_log_likelihood: 79.5961 - val_loss: 82.8402 - val_squared_difference_loss: 38.6694 - val_KL_divergence_loss: 3.7745 - val_neg_log_likelihood: 79.0657\n",
      "Epoch 47/50\n",
      " - 1s - loss: 83.2589 - squared_difference_loss: 39.6672 - KL_divergence_loss: 3.6943 - neg_log_likelihood: 79.5646 - val_loss: 82.8448 - val_squared_difference_loss: 38.8170 - val_KL_divergence_loss: 3.7053 - val_neg_log_likelihood: 79.1395\n",
      "Epoch 48/50\n",
      " - 1s - loss: 83.2273 - squared_difference_loss: 39.6033 - KL_divergence_loss: 3.6946 - neg_log_likelihood: 79.5327 - val_loss: 82.8827 - val_squared_difference_loss: 38.7611 - val_KL_divergence_loss: 3.7711 - val_neg_log_likelihood: 79.1116\n",
      "Epoch 49/50\n",
      " - 1s - loss: 83.2089 - squared_difference_loss: 39.5607 - KL_divergence_loss: 3.6975 - neg_log_likelihood: 79.5114 - val_loss: 82.9847 - val_squared_difference_loss: 38.9000 - val_KL_divergence_loss: 3.8038 - val_neg_log_likelihood: 79.1810\n",
      "Epoch 50/50\n",
      " - 1s - loss: 83.2028 - squared_difference_loss: 39.5269 - KL_divergence_loss: 3.7083 - neg_log_likelihood: 79.4944 - val_loss: 82.9160 - val_squared_difference_loss: 38.7251 - val_KL_divergence_loss: 3.8224 - val_neg_log_likelihood: 79.0936\n",
      "\tAverage Validation Squared Error = 78.32582932654942\n",
      "Latent Var Num = 5\n",
      "Hidden Var Num = 20\n",
      "Simulation #14: VAE with 5 Latent Variable(s) and 20 Hidden Variable(s)...\n",
      "\tEvaluating model on Fold 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/50\n",
      " - 14s - loss: 101.5148 - squared_difference_loss: 77.8872 - KL_divergence_loss: 2.8402 - neg_log_likelihood: 98.6746 - val_loss: 89.0522 - val_squared_difference_loss: 52.0815 - val_KL_divergence_loss: 3.2804 - val_neg_log_likelihood: 85.7718\n",
      "Epoch 2/50\n",
      " - 1s - loss: 86.9669 - squared_difference_loss: 50.1726 - KL_divergence_loss: 2.1496 - neg_log_likelihood: 84.8173 - val_loss: 85.8092 - val_squared_difference_loss: 46.1574 - val_KL_divergence_loss: 2.9995 - val_neg_log_likelihood: 82.8097\n",
      "Epoch 3/50\n",
      " - 1s - loss: 85.6602 - squared_difference_loss: 46.6728 - KL_divergence_loss: 2.5928 - neg_log_likelihood: 83.0674 - val_loss: 84.9789 - val_squared_difference_loss: 43.9801 - val_KL_divergence_loss: 3.2579 - val_neg_log_likelihood: 81.7210\n",
      "Epoch 4/50\n",
      " - 1s - loss: 85.0490 - squared_difference_loss: 44.8139 - KL_divergence_loss: 2.9111 - neg_log_likelihood: 82.1379 - val_loss: 84.5226 - val_squared_difference_loss: 42.3117 - val_KL_divergence_loss: 3.6357 - val_neg_log_likelihood: 80.8868\n",
      "Epoch 5/50\n",
      " - 1s - loss: 84.6206 - squared_difference_loss: 43.5012 - KL_divergence_loss: 3.1390 - neg_log_likelihood: 81.4816 - val_loss: 84.1120 - val_squared_difference_loss: 41.4570 - val_KL_divergence_loss: 3.6525 - val_neg_log_likelihood: 80.4595\n",
      "Epoch 6/50\n",
      " - 1s - loss: 84.3266 - squared_difference_loss: 42.6502 - KL_divergence_loss: 3.2705 - neg_log_likelihood: 81.0561 - val_loss: 83.9850 - val_squared_difference_loss: 41.0127 - val_KL_divergence_loss: 3.7476 - val_neg_log_likelihood: 80.2373\n",
      "Epoch 7/50\n",
      " - 1s - loss: 84.0749 - squared_difference_loss: 41.9234 - KL_divergence_loss: 3.3822 - neg_log_likelihood: 80.6927 - val_loss: 83.6970 - val_squared_difference_loss: 40.3622 - val_KL_divergence_loss: 3.7849 - val_neg_log_likelihood: 79.9121\n",
      "Epoch 8/50\n",
      " - 1s - loss: 83.8316 - squared_difference_loss: 41.3179 - KL_divergence_loss: 3.4417 - neg_log_likelihood: 80.3900 - val_loss: 83.5271 - val_squared_difference_loss: 39.9449 - val_KL_divergence_loss: 3.8236 - val_neg_log_likelihood: 79.7034\n",
      "Epoch 9/50\n",
      " - 1s - loss: 83.6250 - squared_difference_loss: 40.6537 - KL_divergence_loss: 3.5672 - neg_log_likelihood: 80.0578 - val_loss: 83.4354 - val_squared_difference_loss: 39.7113 - val_KL_divergence_loss: 3.8487 - val_neg_log_likelihood: 79.5867\n",
      "Epoch 10/50\n",
      " - 1s - loss: 83.4303 - squared_difference_loss: 40.0226 - KL_divergence_loss: 3.6880 - neg_log_likelihood: 79.7423 - val_loss: 83.2559 - val_squared_difference_loss: 39.2117 - val_KL_divergence_loss: 3.9191 - val_neg_log_likelihood: 79.3369\n",
      "Epoch 11/50\n",
      " - 1s - loss: 83.2667 - squared_difference_loss: 39.5677 - KL_divergence_loss: 3.7518 - neg_log_likelihood: 79.5149 - val_loss: 83.1028 - val_squared_difference_loss: 38.6302 - val_KL_divergence_loss: 4.0567 - val_neg_log_likelihood: 79.0461\n",
      "Epoch 12/50\n",
      " - 1s - loss: 83.1066 - squared_difference_loss: 39.0777 - KL_divergence_loss: 3.8367 - neg_log_likelihood: 79.2698 - val_loss: 83.0272 - val_squared_difference_loss: 38.4705 - val_KL_divergence_loss: 4.0610 - val_neg_log_likelihood: 78.9662\n",
      "Epoch 13/50\n",
      " - 1s - loss: 83.0200 - squared_difference_loss: 38.8794 - KL_divergence_loss: 3.8494 - neg_log_likelihood: 79.1707 - val_loss: 82.9906 - val_squared_difference_loss: 38.4161 - val_KL_divergence_loss: 4.0515 - val_neg_log_likelihood: 78.9390\n",
      "Epoch 14/50\n",
      " - 1s - loss: 82.9230 - squared_difference_loss: 38.5621 - KL_divergence_loss: 3.9109 - neg_log_likelihood: 79.0121 - val_loss: 82.9162 - val_squared_difference_loss: 38.1624 - val_KL_divergence_loss: 4.1040 - val_neg_log_likelihood: 78.8122\n",
      "Epoch 15/50\n",
      " - 1s - loss: 82.8475 - squared_difference_loss: 38.3509 - KL_divergence_loss: 3.9410 - neg_log_likelihood: 78.9065 - val_loss: 82.7538 - val_squared_difference_loss: 37.9796 - val_KL_divergence_loss: 4.0330 - val_neg_log_likelihood: 78.7208\n",
      "Epoch 16/50\n",
      " - 1s - loss: 82.7655 - squared_difference_loss: 38.1743 - KL_divergence_loss: 3.9473 - neg_log_likelihood: 78.8182 - val_loss: 82.6644 - val_squared_difference_loss: 37.5869 - val_KL_divergence_loss: 4.1399 - val_neg_log_likelihood: 78.5245\n",
      "Epoch 17/50\n",
      " - 1s - loss: 82.7316 - squared_difference_loss: 38.0632 - KL_divergence_loss: 3.9690 - neg_log_likelihood: 78.7626 - val_loss: 82.6550 - val_squared_difference_loss: 37.6943 - val_KL_divergence_loss: 4.0768 - val_neg_log_likelihood: 78.5782\n",
      "Epoch 18/50\n",
      " - 1s - loss: 82.7168 - squared_difference_loss: 37.9393 - KL_divergence_loss: 4.0162 - neg_log_likelihood: 78.7007 - val_loss: 82.6395 - val_squared_difference_loss: 37.5730 - val_KL_divergence_loss: 4.1221 - val_neg_log_likelihood: 78.5175\n",
      "Epoch 19/50\n",
      " - 1s - loss: 82.6208 - squared_difference_loss: 37.7626 - KL_divergence_loss: 4.0085 - neg_log_likelihood: 78.6123 - val_loss: 82.5042 - val_squared_difference_loss: 37.3025 - val_KL_divergence_loss: 4.1219 - val_neg_log_likelihood: 78.3822\n",
      "Epoch 20/50\n",
      " - 1s - loss: 82.5327 - squared_difference_loss: 37.5693 - KL_divergence_loss: 4.0171 - neg_log_likelihood: 78.5157 - val_loss: 82.5949 - val_squared_difference_loss: 37.6755 - val_KL_divergence_loss: 4.0261 - val_neg_log_likelihood: 78.5688\n",
      "Epoch 21/50\n",
      " - 1s - loss: 82.5098 - squared_difference_loss: 37.4866 - KL_divergence_loss: 4.0355 - neg_log_likelihood: 78.4743 - val_loss: 82.5112 - val_squared_difference_loss: 37.5323 - val_KL_divergence_loss: 4.0140 - val_neg_log_likelihood: 78.4971\n",
      "Epoch 22/50\n",
      " - 1s - loss: 82.4786 - squared_difference_loss: 37.4492 - KL_divergence_loss: 4.0230 - neg_log_likelihood: 78.4556 - val_loss: 82.4713 - val_squared_difference_loss: 37.2237 - val_KL_divergence_loss: 4.1284 - val_neg_log_likelihood: 78.3429\n",
      "Epoch 23/50\n",
      " - 1s - loss: 82.4288 - squared_difference_loss: 37.2590 - KL_divergence_loss: 4.0683 - neg_log_likelihood: 78.3605 - val_loss: 82.4500 - val_squared_difference_loss: 37.3801 - val_KL_divergence_loss: 4.0289 - val_neg_log_likelihood: 78.4211\n",
      "Epoch 24/50\n",
      " - 1s - loss: 82.3840 - squared_difference_loss: 37.2229 - KL_divergence_loss: 4.0415 - neg_log_likelihood: 78.3424 - val_loss: 82.3909 - val_squared_difference_loss: 37.0305 - val_KL_divergence_loss: 4.1446 - val_neg_log_likelihood: 78.2463\n",
      "Epoch 25/50\n",
      " - 1s - loss: 82.3860 - squared_difference_loss: 37.1708 - KL_divergence_loss: 4.0696 - neg_log_likelihood: 78.3164 - val_loss: 82.4679 - val_squared_difference_loss: 37.0575 - val_KL_divergence_loss: 4.2082 - val_neg_log_likelihood: 78.2598\n",
      "Epoch 26/50\n",
      " - 1s - loss: 82.3506 - squared_difference_loss: 37.0722 - KL_divergence_loss: 4.0835 - neg_log_likelihood: 78.2671 - val_loss: 82.5670 - val_squared_difference_loss: 37.5670 - val_KL_divergence_loss: 4.0525 - val_neg_log_likelihood: 78.5145\n",
      "Epoch 27/50\n",
      " - 1s - loss: 82.3209 - squared_difference_loss: 37.0999 - KL_divergence_loss: 4.0400 - neg_log_likelihood: 78.2810 - val_loss: 82.4127 - val_squared_difference_loss: 36.9255 - val_KL_divergence_loss: 4.2189 - val_neg_log_likelihood: 78.1938\n",
      "Epoch 28/50\n",
      " - 1s - loss: 82.2422 - squared_difference_loss: 36.8527 - KL_divergence_loss: 4.0849 - neg_log_likelihood: 78.1574 - val_loss: 82.3113 - val_squared_difference_loss: 36.9416 - val_KL_divergence_loss: 4.1095 - val_neg_log_likelihood: 78.2018\n",
      "Epoch 29/50\n",
      " - 1s - loss: 82.2449 - squared_difference_loss: 36.8899 - KL_divergence_loss: 4.0689 - neg_log_likelihood: 78.1759 - val_loss: 82.1887 - val_squared_difference_loss: 36.6115 - val_KL_divergence_loss: 4.1519 - val_neg_log_likelihood: 78.0368\n",
      "Epoch 30/50\n",
      " - 1s - loss: 82.2364 - squared_difference_loss: 36.8885 - KL_divergence_loss: 4.0612 - neg_log_likelihood: 78.1752 - val_loss: 82.2188 - val_squared_difference_loss: 36.5582 - val_KL_divergence_loss: 4.2087 - val_neg_log_likelihood: 78.0101\n",
      "Epoch 31/50\n",
      " - 1s - loss: 82.2361 - squared_difference_loss: 36.8305 - KL_divergence_loss: 4.0898 - neg_log_likelihood: 78.1463 - val_loss: 82.1328 - val_squared_difference_loss: 36.6211 - val_KL_divergence_loss: 4.0912 - val_neg_log_likelihood: 78.0415\n",
      "Epoch 32/50\n",
      " - 1s - loss: 82.2652 - squared_difference_loss: 36.9506 - KL_divergence_loss: 4.0589 - neg_log_likelihood: 78.2063 - val_loss: 82.1932 - val_squared_difference_loss: 36.4805 - val_KL_divergence_loss: 4.2219 - val_neg_log_likelihood: 77.9712\n",
      "Epoch 33/50\n",
      " - 1s - loss: 82.2033 - squared_difference_loss: 36.6897 - KL_divergence_loss: 4.1275 - neg_log_likelihood: 78.0758 - val_loss: 82.2086 - val_squared_difference_loss: 36.7588 - val_KL_divergence_loss: 4.0982 - val_neg_log_likelihood: 78.1104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50\n",
      " - 1s - loss: 82.1940 - squared_difference_loss: 36.7571 - KL_divergence_loss: 4.0845 - neg_log_likelihood: 78.1095 - val_loss: 82.0863 - val_squared_difference_loss: 36.5025 - val_KL_divergence_loss: 4.1040 - val_neg_log_likelihood: 77.9823\n",
      "Epoch 35/50\n",
      " - 1s - loss: 82.1810 - squared_difference_loss: 36.6989 - KL_divergence_loss: 4.1006 - neg_log_likelihood: 78.0804 - val_loss: 82.0989 - val_squared_difference_loss: 36.2549 - val_KL_divergence_loss: 4.2404 - val_neg_log_likelihood: 77.8585\n",
      "Epoch 36/50\n",
      " - 1s - loss: 82.1216 - squared_difference_loss: 36.5909 - KL_divergence_loss: 4.0952 - neg_log_likelihood: 78.0264 - val_loss: 82.0700 - val_squared_difference_loss: 36.5674 - val_KL_divergence_loss: 4.0553 - val_neg_log_likelihood: 78.0147\n",
      "Epoch 37/50\n",
      " - 1s - loss: 82.1639 - squared_difference_loss: 36.7399 - KL_divergence_loss: 4.0629 - neg_log_likelihood: 78.1010 - val_loss: 82.0750 - val_squared_difference_loss: 36.3606 - val_KL_divergence_loss: 4.1637 - val_neg_log_likelihood: 77.9113\n",
      "Epoch 38/50\n",
      " - 1s - loss: 82.1324 - squared_difference_loss: 36.6286 - KL_divergence_loss: 4.0871 - neg_log_likelihood: 78.0453 - val_loss: 82.1763 - val_squared_difference_loss: 36.4026 - val_KL_divergence_loss: 4.2440 - val_neg_log_likelihood: 77.9323\n",
      "Epoch 39/50\n",
      " - 1s - loss: 82.1198 - squared_difference_loss: 36.6295 - KL_divergence_loss: 4.0740 - neg_log_likelihood: 78.0457 - val_loss: 82.0453 - val_squared_difference_loss: 36.2307 - val_KL_divergence_loss: 4.1989 - val_neg_log_likelihood: 77.8464\n",
      "Epoch 40/50\n",
      " - 1s - loss: 82.1004 - squared_difference_loss: 36.5477 - KL_divergence_loss: 4.0955 - neg_log_likelihood: 78.0048 - val_loss: 82.0167 - val_squared_difference_loss: 36.4084 - val_KL_divergence_loss: 4.0815 - val_neg_log_likelihood: 77.9352\n",
      "Epoch 41/50\n",
      " - 1s - loss: 82.1150 - squared_difference_loss: 36.5729 - KL_divergence_loss: 4.0976 - neg_log_likelihood: 78.0174 - val_loss: 81.9977 - val_squared_difference_loss: 36.1034 - val_KL_divergence_loss: 4.2150 - val_neg_log_likelihood: 77.7827\n",
      "Epoch 42/50\n",
      " - 1s - loss: 82.1036 - squared_difference_loss: 36.5388 - KL_divergence_loss: 4.1032 - neg_log_likelihood: 78.0004 - val_loss: 82.0139 - val_squared_difference_loss: 36.2816 - val_KL_divergence_loss: 4.1420 - val_neg_log_likelihood: 77.8718\n",
      "Epoch 43/50\n",
      " - 1s - loss: 82.0766 - squared_difference_loss: 36.4796 - KL_divergence_loss: 4.1058 - neg_log_likelihood: 77.9708 - val_loss: 81.9568 - val_squared_difference_loss: 36.1204 - val_KL_divergence_loss: 4.1656 - val_neg_log_likelihood: 77.7912\n",
      "Epoch 44/50\n",
      " - 1s - loss: 82.0802 - squared_difference_loss: 36.4404 - KL_divergence_loss: 4.1290 - neg_log_likelihood: 77.9512 - val_loss: 81.9975 - val_squared_difference_loss: 36.2995 - val_KL_divergence_loss: 4.1168 - val_neg_log_likelihood: 77.8808\n",
      "Epoch 45/50\n",
      " - 1s - loss: 82.0756 - squared_difference_loss: 36.4434 - KL_divergence_loss: 4.1229 - neg_log_likelihood: 77.9527 - val_loss: 81.9465 - val_squared_difference_loss: 36.1842 - val_KL_divergence_loss: 4.1233 - val_neg_log_likelihood: 77.8231\n",
      "Epoch 46/50\n",
      " - 1s - loss: 82.0665 - squared_difference_loss: 36.4220 - KL_divergence_loss: 4.1244 - neg_log_likelihood: 77.9420 - val_loss: 81.9990 - val_squared_difference_loss: 36.2213 - val_KL_divergence_loss: 4.1574 - val_neg_log_likelihood: 77.8417\n",
      "Epoch 47/50\n",
      " - 1s - loss: 82.0672 - squared_difference_loss: 36.4483 - KL_divergence_loss: 4.1121 - neg_log_likelihood: 77.9552 - val_loss: 81.9281 - val_squared_difference_loss: 35.9865 - val_KL_divergence_loss: 4.2038 - val_neg_log_likelihood: 77.7243\n",
      "Epoch 48/50\n",
      " - 1s - loss: 82.0321 - squared_difference_loss: 36.3694 - KL_divergence_loss: 4.1164 - neg_log_likelihood: 77.9157 - val_loss: 81.9135 - val_squared_difference_loss: 35.9741 - val_KL_divergence_loss: 4.1954 - val_neg_log_likelihood: 77.7181\n",
      "Epoch 49/50\n",
      " - 1s - loss: 82.0535 - squared_difference_loss: 36.4381 - KL_divergence_loss: 4.1034 - neg_log_likelihood: 77.9501 - val_loss: 82.0013 - val_squared_difference_loss: 35.9690 - val_KL_divergence_loss: 4.2858 - val_neg_log_likelihood: 77.7155\n",
      "Epoch 50/50\n",
      " - 1s - loss: 82.0269 - squared_difference_loss: 36.2930 - KL_divergence_loss: 4.1495 - neg_log_likelihood: 77.8775 - val_loss: 82.0035 - val_squared_difference_loss: 36.3833 - val_KL_divergence_loss: 4.0808 - val_neg_log_likelihood: 77.9226\n",
      "\tAverage Validation Squared Error = 76.01833787000034\n",
      "Latent Var Num = 5\n",
      "Hidden Var Num = 30\n",
      "Simulation #15: VAE with 5 Latent Variable(s) and 30 Hidden Variable(s)...\n",
      "\tEvaluating model on Fold 1\n",
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/50\n",
      " - 15s - loss: 99.2794 - squared_difference_loss: 73.2519 - KL_divergence_loss: 2.9225 - neg_log_likelihood: 96.3569 - val_loss: 87.9541 - val_squared_difference_loss: 50.2322 - val_KL_divergence_loss: 3.1069 - val_neg_log_likelihood: 84.8471\n",
      "Epoch 2/50\n",
      " - 1s - loss: 86.1343 - squared_difference_loss: 47.4980 - KL_divergence_loss: 2.6543 - neg_log_likelihood: 83.4800 - val_loss: 84.9396 - val_squared_difference_loss: 42.3204 - val_KL_divergence_loss: 4.0484 - val_neg_log_likelihood: 80.8912\n",
      "Epoch 3/50\n",
      " - 1s - loss: 84.6406 - squared_difference_loss: 43.0768 - KL_divergence_loss: 3.3712 - neg_log_likelihood: 81.2694 - val_loss: 84.1231 - val_squared_difference_loss: 40.3432 - val_KL_divergence_loss: 4.2205 - val_neg_log_likelihood: 79.9026\n",
      "Epoch 4/50\n",
      " - 1s - loss: 84.0669 - squared_difference_loss: 41.4791 - KL_divergence_loss: 3.5964 - neg_log_likelihood: 80.4705 - val_loss: 83.5736 - val_squared_difference_loss: 39.1073 - val_KL_divergence_loss: 4.2889 - val_neg_log_likelihood: 79.2847\n",
      "Epoch 5/50\n",
      " - 1s - loss: 83.7243 - squared_difference_loss: 40.4595 - KL_divergence_loss: 3.7636 - neg_log_likelihood: 79.9607 - val_loss: 83.3183 - val_squared_difference_loss: 38.4771 - val_KL_divergence_loss: 4.3488 - val_neg_log_likelihood: 78.9696\n",
      "Epoch 6/50\n",
      " - 1s - loss: 83.5090 - squared_difference_loss: 39.9117 - KL_divergence_loss: 3.8221 - neg_log_likelihood: 79.6869 - val_loss: 83.1326 - val_squared_difference_loss: 38.1892 - val_KL_divergence_loss: 4.3070 - val_neg_log_likelihood: 78.8256\n",
      "Epoch 7/50\n",
      " - 1s - loss: 83.3441 - squared_difference_loss: 39.4913 - KL_divergence_loss: 3.8674 - neg_log_likelihood: 79.4767 - val_loss: 82.9878 - val_squared_difference_loss: 37.9844 - val_KL_divergence_loss: 4.2646 - val_neg_log_likelihood: 78.7232\n",
      "Epoch 8/50\n",
      " - 1s - loss: 83.2054 - squared_difference_loss: 39.1479 - KL_divergence_loss: 3.9005 - neg_log_likelihood: 79.3049 - val_loss: 82.9709 - val_squared_difference_loss: 37.9788 - val_KL_divergence_loss: 4.2506 - val_neg_log_likelihood: 78.7204\n",
      "Epoch 9/50\n",
      " - 1s - loss: 83.1047 - squared_difference_loss: 38.8870 - KL_divergence_loss: 3.9301 - neg_log_likelihood: 79.1745 - val_loss: 83.0075 - val_squared_difference_loss: 38.0786 - val_KL_divergence_loss: 4.2372 - val_neg_log_likelihood: 78.7703\n",
      "Epoch 10/50\n",
      " - 1s - loss: 83.0107 - squared_difference_loss: 38.6480 - KL_divergence_loss: 3.9557 - neg_log_likelihood: 79.0550 - val_loss: 82.8883 - val_squared_difference_loss: 37.7895 - val_KL_divergence_loss: 4.2626 - val_neg_log_likelihood: 78.6257\n",
      "Epoch 11/50\n",
      " - 1s - loss: 82.9593 - squared_difference_loss: 38.4688 - KL_divergence_loss: 3.9939 - neg_log_likelihood: 78.9654 - val_loss: 82.7552 - val_squared_difference_loss: 37.5403 - val_KL_divergence_loss: 4.2540 - val_neg_log_likelihood: 78.5011\n",
      "Epoch 12/50\n",
      " - 1s - loss: 82.8594 - squared_difference_loss: 38.2695 - KL_divergence_loss: 3.9937 - neg_log_likelihood: 78.8658 - val_loss: 82.8173 - val_squared_difference_loss: 37.8303 - val_KL_divergence_loss: 4.1711 - val_neg_log_likelihood: 78.6461\n",
      "Epoch 13/50\n",
      " - 1s - loss: 82.7973 - squared_difference_loss: 38.0992 - KL_divergence_loss: 4.0167 - neg_log_likelihood: 78.7806 - val_loss: 82.9398 - val_squared_difference_loss: 37.8599 - val_KL_divergence_loss: 4.2789 - val_neg_log_likelihood: 78.6609\n",
      "Epoch 14/50\n",
      " - 1s - loss: 82.7386 - squared_difference_loss: 37.9644 - KL_divergence_loss: 4.0254 - neg_log_likelihood: 78.7132 - val_loss: 82.6579 - val_squared_difference_loss: 37.4766 - val_KL_divergence_loss: 4.1886 - val_neg_log_likelihood: 78.4693\n",
      "Epoch 15/50\n",
      " - 1s - loss: 82.6784 - squared_difference_loss: 37.8058 - KL_divergence_loss: 4.0445 - neg_log_likelihood: 78.6339 - val_loss: 82.5865 - val_squared_difference_loss: 37.4111 - val_KL_divergence_loss: 4.1500 - val_neg_log_likelihood: 78.4365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50\n",
      " - 1s - loss: 82.6152 - squared_difference_loss: 37.5727 - KL_divergence_loss: 4.0979 - neg_log_likelihood: 78.5173 - val_loss: 82.8420 - val_squared_difference_loss: 38.1470 - val_KL_divergence_loss: 4.0375 - val_neg_log_likelihood: 78.8045\n",
      "Epoch 17/50\n",
      " - 1s - loss: 82.5664 - squared_difference_loss: 37.5194 - KL_divergence_loss: 4.0757 - neg_log_likelihood: 78.4907 - val_loss: 82.6367 - val_squared_difference_loss: 37.6803 - val_KL_divergence_loss: 4.0655 - val_neg_log_likelihood: 78.5712\n",
      "Epoch 18/50\n",
      " - 1s - loss: 82.4843 - squared_difference_loss: 37.3732 - KL_divergence_loss: 4.0667 - neg_log_likelihood: 78.4176 - val_loss: 82.6923 - val_squared_difference_loss: 37.3441 - val_KL_divergence_loss: 4.2893 - val_neg_log_likelihood: 78.4030\n",
      "Epoch 19/50\n",
      " - 1s - loss: 82.4432 - squared_difference_loss: 37.1807 - KL_divergence_loss: 4.1218 - neg_log_likelihood: 78.3213 - val_loss: 82.7082 - val_squared_difference_loss: 37.5757 - val_KL_divergence_loss: 4.1893 - val_neg_log_likelihood: 78.5189\n",
      "Epoch 20/50\n",
      " - 1s - loss: 82.3490 - squared_difference_loss: 37.0773 - KL_divergence_loss: 4.0793 - neg_log_likelihood: 78.2696 - val_loss: 82.6494 - val_squared_difference_loss: 37.5217 - val_KL_divergence_loss: 4.1575 - val_neg_log_likelihood: 78.4919\n",
      "Epoch 21/50\n",
      " - 1s - loss: 82.2926 - squared_difference_loss: 36.8885 - KL_divergence_loss: 4.1174 - neg_log_likelihood: 78.1752 - val_loss: 82.6620 - val_squared_difference_loss: 37.5227 - val_KL_divergence_loss: 4.1697 - val_neg_log_likelihood: 78.4923\n",
      "Epoch 22/50\n",
      " - 1s - loss: 82.3225 - squared_difference_loss: 36.9144 - KL_divergence_loss: 4.1344 - neg_log_likelihood: 78.1882 - val_loss: 82.5886 - val_squared_difference_loss: 37.3278 - val_KL_divergence_loss: 4.1938 - val_neg_log_likelihood: 78.3949\n",
      "Epoch 23/50\n",
      " - 1s - loss: 82.2921 - squared_difference_loss: 36.8420 - KL_divergence_loss: 4.1401 - neg_log_likelihood: 78.1520 - val_loss: 82.5605 - val_squared_difference_loss: 37.3374 - val_KL_divergence_loss: 4.1608 - val_neg_log_likelihood: 78.3997\n",
      "Epoch 24/50\n",
      " - 1s - loss: 82.2228 - squared_difference_loss: 36.7685 - KL_divergence_loss: 4.1075 - neg_log_likelihood: 78.1152 - val_loss: 82.4808 - val_squared_difference_loss: 37.1452 - val_KL_divergence_loss: 4.1772 - val_neg_log_likelihood: 78.3036\n",
      "Epoch 25/50\n",
      " - 1s - loss: 82.1837 - squared_difference_loss: 36.6923 - KL_divergence_loss: 4.1066 - neg_log_likelihood: 78.0772 - val_loss: 82.4446 - val_squared_difference_loss: 37.1939 - val_KL_divergence_loss: 4.1166 - val_neg_log_likelihood: 78.3279\n",
      "Epoch 26/50\n",
      " - 1s - loss: 82.1932 - squared_difference_loss: 36.6320 - KL_divergence_loss: 4.1462 - neg_log_likelihood: 78.0470 - val_loss: 82.4192 - val_squared_difference_loss: 37.1934 - val_KL_divergence_loss: 4.0915 - val_neg_log_likelihood: 78.3277\n",
      "Epoch 27/50\n",
      " - 1s - loss: 82.1854 - squared_difference_loss: 36.6063 - KL_divergence_loss: 4.1512 - neg_log_likelihood: 78.0341 - val_loss: 82.5825 - val_squared_difference_loss: 37.2156 - val_KL_divergence_loss: 4.2437 - val_neg_log_likelihood: 78.3388\n",
      "Epoch 28/50\n",
      " - 1s - loss: 82.1277 - squared_difference_loss: 36.4802 - KL_divergence_loss: 4.1566 - neg_log_likelihood: 77.9711 - val_loss: 82.7238 - val_squared_difference_loss: 37.3305 - val_KL_divergence_loss: 4.3275 - val_neg_log_likelihood: 78.3963\n",
      "Epoch 29/50\n",
      " - 1s - loss: 82.1231 - squared_difference_loss: 36.4767 - KL_divergence_loss: 4.1538 - neg_log_likelihood: 77.9694 - val_loss: 82.4742 - val_squared_difference_loss: 37.1934 - val_KL_divergence_loss: 4.1465 - val_neg_log_likelihood: 78.3277\n",
      "Epoch 30/50\n",
      " - 1s - loss: 82.1007 - squared_difference_loss: 36.3630 - KL_divergence_loss: 4.1882 - neg_log_likelihood: 77.9125 - val_loss: 82.4488 - val_squared_difference_loss: 36.8643 - val_KL_divergence_loss: 4.2857 - val_neg_log_likelihood: 78.1632\n",
      "Epoch 31/50\n",
      " - 1s - loss: 82.0894 - squared_difference_loss: 36.3292 - KL_divergence_loss: 4.1938 - neg_log_likelihood: 77.8956 - val_loss: 82.5063 - val_squared_difference_loss: 37.3260 - val_KL_divergence_loss: 4.1123 - val_neg_log_likelihood: 78.3940\n",
      "Epoch 32/50\n",
      " - 1s - loss: 82.0339 - squared_difference_loss: 36.2608 - KL_divergence_loss: 4.1725 - neg_log_likelihood: 77.8614 - val_loss: 82.5798 - val_squared_difference_loss: 37.4481 - val_KL_divergence_loss: 4.1247 - val_neg_log_likelihood: 78.4551\n",
      "Epoch 33/50\n",
      " - 1s - loss: 82.0240 - squared_difference_loss: 36.2639 - KL_divergence_loss: 4.1610 - neg_log_likelihood: 77.8629 - val_loss: 82.4062 - val_squared_difference_loss: 36.6839 - val_KL_divergence_loss: 4.3333 - val_neg_log_likelihood: 78.0730\n",
      "Epoch 34/50\n",
      " - 1s - loss: 82.0352 - squared_difference_loss: 36.2190 - KL_divergence_loss: 4.1947 - neg_log_likelihood: 77.8405 - val_loss: 82.6809 - val_squared_difference_loss: 37.6221 - val_KL_divergence_loss: 4.1388 - val_neg_log_likelihood: 78.5420\n",
      "Epoch 35/50\n",
      " - 1s - loss: 81.9850 - squared_difference_loss: 36.1670 - KL_divergence_loss: 4.1705 - neg_log_likelihood: 77.8145 - val_loss: 82.6991 - val_squared_difference_loss: 37.5324 - val_KL_divergence_loss: 4.2019 - val_neg_log_likelihood: 78.4972\n",
      "Epoch 36/50\n",
      " - 1s - loss: 81.9933 - squared_difference_loss: 36.1483 - KL_divergence_loss: 4.1881 - neg_log_likelihood: 77.8052 - val_loss: 82.3048 - val_squared_difference_loss: 36.6887 - val_KL_divergence_loss: 4.2294 - val_neg_log_likelihood: 78.0753\n",
      "Epoch 37/50\n",
      " - 1s - loss: 81.9197 - squared_difference_loss: 36.0331 - KL_divergence_loss: 4.1722 - neg_log_likelihood: 77.7475 - val_loss: 82.3381 - val_squared_difference_loss: 36.6974 - val_KL_divergence_loss: 4.2584 - val_neg_log_likelihood: 78.0797\n",
      "Epoch 38/50\n",
      " - 1s - loss: 81.9484 - squared_difference_loss: 36.0514 - KL_divergence_loss: 4.1917 - neg_log_likelihood: 77.7567 - val_loss: 82.3313 - val_squared_difference_loss: 36.9065 - val_KL_divergence_loss: 4.1471 - val_neg_log_likelihood: 78.1843\n",
      "Epoch 39/50\n",
      " - 1s - loss: 81.9233 - squared_difference_loss: 36.0084 - KL_divergence_loss: 4.1881 - neg_log_likelihood: 77.7352 - val_loss: 82.2709 - val_squared_difference_loss: 36.4091 - val_KL_divergence_loss: 4.3353 - val_neg_log_likelihood: 77.9355\n",
      "Epoch 40/50\n",
      " - 1s - loss: 81.9296 - squared_difference_loss: 36.0008 - KL_divergence_loss: 4.1983 - neg_log_likelihood: 77.7314 - val_loss: 82.2186 - val_squared_difference_loss: 36.3129 - val_KL_divergence_loss: 4.3312 - val_neg_log_likelihood: 77.8875\n",
      "Epoch 41/50\n",
      " - 1s - loss: 81.9044 - squared_difference_loss: 35.9390 - KL_divergence_loss: 4.2039 - neg_log_likelihood: 77.7005 - val_loss: 82.4648 - val_squared_difference_loss: 37.0112 - val_KL_divergence_loss: 4.2282 - val_neg_log_likelihood: 78.2366\n",
      "Epoch 42/50\n",
      " - 1s - loss: 81.8677 - squared_difference_loss: 35.8664 - KL_divergence_loss: 4.2035 - neg_log_likelihood: 77.6642 - val_loss: 82.6573 - val_squared_difference_loss: 37.5243 - val_KL_divergence_loss: 4.1642 - val_neg_log_likelihood: 78.4932\n",
      "Epoch 43/50\n",
      " - 1s - loss: 81.9084 - squared_difference_loss: 35.9539 - KL_divergence_loss: 4.2004 - neg_log_likelihood: 77.7080 - val_loss: 82.4458 - val_squared_difference_loss: 36.6775 - val_KL_divergence_loss: 4.3761 - val_neg_log_likelihood: 78.0697\n",
      "Epoch 44/50\n",
      " - 1s - loss: 81.9193 - squared_difference_loss: 35.9766 - KL_divergence_loss: 4.2000 - neg_log_likelihood: 77.7193 - val_loss: 82.3928 - val_squared_difference_loss: 36.6342 - val_KL_divergence_loss: 4.3447 - val_neg_log_likelihood: 78.0481\n",
      "Epoch 45/50\n",
      " - 1s - loss: 81.8364 - squared_difference_loss: 35.7419 - KL_divergence_loss: 4.2344 - neg_log_likelihood: 77.6019 - val_loss: 82.3042 - val_squared_difference_loss: 36.5738 - val_KL_divergence_loss: 4.2864 - val_neg_log_likelihood: 78.0179\n",
      "Epoch 46/50\n",
      " - 1s - loss: 81.8418 - squared_difference_loss: 35.7726 - KL_divergence_loss: 4.2245 - neg_log_likelihood: 77.6173 - val_loss: 82.3338 - val_squared_difference_loss: 36.7557 - val_KL_divergence_loss: 4.2250 - val_neg_log_likelihood: 78.1089\n",
      "Epoch 47/50\n",
      " - 1s - loss: 81.8022 - squared_difference_loss: 35.7191 - KL_divergence_loss: 4.2116 - neg_log_likelihood: 77.5905 - val_loss: 82.2226 - val_squared_difference_loss: 36.4618 - val_KL_divergence_loss: 4.2607 - val_neg_log_likelihood: 77.9619\n",
      "Epoch 48/50\n",
      " - 1s - loss: 81.7822 - squared_difference_loss: 35.6306 - KL_divergence_loss: 4.2359 - neg_log_likelihood: 77.5463 - val_loss: 82.0239 - val_squared_difference_loss: 36.1416 - val_KL_divergence_loss: 4.2220 - val_neg_log_likelihood: 77.8018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      " - 1s - loss: 81.8120 - squared_difference_loss: 35.6765 - KL_divergence_loss: 4.2428 - neg_log_likelihood: 77.5693 - val_loss: 81.9999 - val_squared_difference_loss: 36.1091 - val_KL_divergence_loss: 4.2144 - val_neg_log_likelihood: 77.7855\n",
      "Epoch 50/50\n",
      " - 1s - loss: 81.7781 - squared_difference_loss: 35.7212 - KL_divergence_loss: 4.1865 - neg_log_likelihood: 77.5916 - val_loss: 81.9328 - val_squared_difference_loss: 35.9014 - val_KL_divergence_loss: 4.2510 - val_neg_log_likelihood: 77.6817\n",
      "\tAverage Validation Squared Error = 75.84467640141108\n",
      "Latent Var Num = 5\n",
      "Hidden Var Num = 40\n",
      "Simulation #16: VAE with 5 Latent Variable(s) and 40 Hidden Variable(s)...\n",
      "\tEvaluating model on Fold 1\n",
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/50\n",
      " - 15s - loss: 96.6751 - squared_difference_loss: 67.3696 - KL_divergence_loss: 3.2593 - neg_log_likelihood: 93.4158 - val_loss: 87.1839 - val_squared_difference_loss: 46.8194 - val_KL_divergence_loss: 4.0432 - val_neg_log_likelihood: 83.1407\n",
      "Epoch 2/50\n",
      " - 1s - loss: 85.2943 - squared_difference_loss: 44.9126 - KL_divergence_loss: 3.1070 - neg_log_likelihood: 82.1873 - val_loss: 84.1963 - val_squared_difference_loss: 39.9804 - val_KL_divergence_loss: 4.4751 - val_neg_log_likelihood: 79.7212\n",
      "Epoch 3/50\n",
      " - 1s - loss: 84.0985 - squared_difference_loss: 41.3989 - KL_divergence_loss: 3.6680 - neg_log_likelihood: 80.4305 - val_loss: 83.5103 - val_squared_difference_loss: 39.0038 - val_KL_divergence_loss: 4.2774 - val_neg_log_likelihood: 79.2329\n",
      "Epoch 4/50\n",
      " - 1s - loss: 83.6526 - squared_difference_loss: 40.3259 - KL_divergence_loss: 3.7587 - neg_log_likelihood: 79.8939 - val_loss: 83.2122 - val_squared_difference_loss: 38.1375 - val_KL_divergence_loss: 4.4124 - val_neg_log_likelihood: 78.7997\n",
      "Epoch 5/50\n",
      " - 1s - loss: 83.4071 - squared_difference_loss: 39.6393 - KL_divergence_loss: 3.8565 - neg_log_likelihood: 79.5506 - val_loss: 83.0211 - val_squared_difference_loss: 37.8576 - val_KL_divergence_loss: 4.3613 - val_neg_log_likelihood: 78.6598\n",
      "Epoch 6/50\n",
      " - 1s - loss: 83.2251 - squared_difference_loss: 39.1346 - KL_divergence_loss: 3.9268 - neg_log_likelihood: 79.2983 - val_loss: 82.9099 - val_squared_difference_loss: 37.7526 - val_KL_divergence_loss: 4.3026 - val_neg_log_likelihood: 78.6073\n",
      "Epoch 7/50\n",
      " - 1s - loss: 83.1157 - squared_difference_loss: 38.7573 - KL_divergence_loss: 4.0061 - neg_log_likelihood: 79.1096 - val_loss: 82.8652 - val_squared_difference_loss: 37.7002 - val_KL_divergence_loss: 4.2842 - val_neg_log_likelihood: 78.5811\n",
      "Epoch 8/50\n",
      " - 1s - loss: 82.9785 - squared_difference_loss: 38.4742 - KL_divergence_loss: 4.0104 - neg_log_likelihood: 78.9681 - val_loss: 82.7020 - val_squared_difference_loss: 37.3825 - val_KL_divergence_loss: 4.2797 - val_neg_log_likelihood: 78.4223\n",
      "Epoch 9/50\n",
      " - 1s - loss: 82.8574 - squared_difference_loss: 38.1800 - KL_divergence_loss: 4.0364 - neg_log_likelihood: 78.8210 - val_loss: 82.6753 - val_squared_difference_loss: 37.2919 - val_KL_divergence_loss: 4.2983 - val_neg_log_likelihood: 78.3770\n",
      "Epoch 10/50\n",
      " - 1s - loss: 82.7609 - squared_difference_loss: 37.8804 - KL_divergence_loss: 4.0897 - neg_log_likelihood: 78.6712 - val_loss: 82.6779 - val_squared_difference_loss: 37.2904 - val_KL_divergence_loss: 4.3017 - val_neg_log_likelihood: 78.3762\n",
      "Epoch 11/50\n",
      " - 1s - loss: 82.6575 - squared_difference_loss: 37.6365 - KL_divergence_loss: 4.1083 - neg_log_likelihood: 78.5492 - val_loss: 82.6440 - val_squared_difference_loss: 37.1629 - val_KL_divergence_loss: 4.3315 - val_neg_log_likelihood: 78.3124\n",
      "Epoch 12/50\n",
      " - 1s - loss: 82.5805 - squared_difference_loss: 37.4565 - KL_divergence_loss: 4.1213 - neg_log_likelihood: 78.4593 - val_loss: 82.6829 - val_squared_difference_loss: 37.2047 - val_KL_divergence_loss: 4.3495 - val_neg_log_likelihood: 78.3334\n",
      "Epoch 13/50\n",
      " - 1s - loss: 82.5089 - squared_difference_loss: 37.3256 - KL_divergence_loss: 4.1151 - neg_log_likelihood: 78.3938 - val_loss: 82.6597 - val_squared_difference_loss: 37.2699 - val_KL_divergence_loss: 4.2937 - val_neg_log_likelihood: 78.3659\n",
      "Epoch 14/50\n",
      " - 1s - loss: 82.4354 - squared_difference_loss: 37.1277 - KL_divergence_loss: 4.1405 - neg_log_likelihood: 78.2948 - val_loss: 82.7165 - val_squared_difference_loss: 37.2026 - val_KL_divergence_loss: 4.3841 - val_neg_log_likelihood: 78.3323\n",
      "Epoch 15/50\n",
      " - 1s - loss: 82.3635 - squared_difference_loss: 37.0179 - KL_divergence_loss: 4.1236 - neg_log_likelihood: 78.2399 - val_loss: 82.6079 - val_squared_difference_loss: 37.1317 - val_KL_divergence_loss: 4.3111 - val_neg_log_likelihood: 78.2969\n",
      "Epoch 16/50\n",
      " - 1s - loss: 82.3060 - squared_difference_loss: 36.8234 - KL_divergence_loss: 4.1633 - neg_log_likelihood: 78.1427 - val_loss: 82.6037 - val_squared_difference_loss: 37.4911 - val_KL_divergence_loss: 4.1271 - val_neg_log_likelihood: 78.4766\n",
      "Epoch 17/50\n",
      " - 1s - loss: 82.2808 - squared_difference_loss: 36.8043 - KL_divergence_loss: 4.1477 - neg_log_likelihood: 78.1331 - val_loss: 82.6184 - val_squared_difference_loss: 37.3119 - val_KL_divergence_loss: 4.2314 - val_neg_log_likelihood: 78.3869\n",
      "Epoch 18/50\n",
      " - 1s - loss: 82.1964 - squared_difference_loss: 36.6789 - KL_divergence_loss: 4.1259 - neg_log_likelihood: 78.0704 - val_loss: 82.4566 - val_squared_difference_loss: 36.8205 - val_KL_divergence_loss: 4.3154 - val_neg_log_likelihood: 78.1412\n",
      "Epoch 19/50\n",
      " - 1s - loss: 82.1691 - squared_difference_loss: 36.5314 - KL_divergence_loss: 4.1724 - neg_log_likelihood: 77.9967 - val_loss: 82.3221 - val_squared_difference_loss: 36.7426 - val_KL_divergence_loss: 4.2198 - val_neg_log_likelihood: 78.1023\n",
      "Epoch 20/50\n",
      " - 1s - loss: 82.1300 - squared_difference_loss: 36.4833 - KL_divergence_loss: 4.1574 - neg_log_likelihood: 77.9726 - val_loss: 82.3052 - val_squared_difference_loss: 36.9730 - val_KL_divergence_loss: 4.0877 - val_neg_log_likelihood: 78.2175\n",
      "Epoch 21/50\n",
      " - 1s - loss: 82.0933 - squared_difference_loss: 36.4142 - KL_divergence_loss: 4.1552 - neg_log_likelihood: 77.9381 - val_loss: 82.1914 - val_squared_difference_loss: 36.5636 - val_KL_divergence_loss: 4.1786 - val_neg_log_likelihood: 78.0128\n",
      "Epoch 22/50\n",
      " - 1s - loss: 82.0702 - squared_difference_loss: 36.3341 - KL_divergence_loss: 4.1721 - neg_log_likelihood: 77.8980 - val_loss: 82.2140 - val_squared_difference_loss: 36.4865 - val_KL_divergence_loss: 4.2398 - val_neg_log_likelihood: 77.9742\n",
      "Epoch 23/50\n",
      " - 1s - loss: 82.0519 - squared_difference_loss: 36.3236 - KL_divergence_loss: 4.1591 - neg_log_likelihood: 77.8928 - val_loss: 82.0710 - val_squared_difference_loss: 36.1687 - val_KL_divergence_loss: 4.2557 - val_neg_log_likelihood: 77.8153\n",
      "Epoch 24/50\n",
      " - 1s - loss: 82.0041 - squared_difference_loss: 36.1732 - KL_divergence_loss: 4.1865 - neg_log_likelihood: 77.8176 - val_loss: 82.1309 - val_squared_difference_loss: 36.3837 - val_KL_divergence_loss: 4.2081 - val_neg_log_likelihood: 77.9228\n",
      "Epoch 25/50\n",
      " - 1s - loss: 81.9819 - squared_difference_loss: 36.1667 - KL_divergence_loss: 4.1676 - neg_log_likelihood: 77.8143 - val_loss: 82.0345 - val_squared_difference_loss: 36.1227 - val_KL_divergence_loss: 4.2422 - val_neg_log_likelihood: 77.7923\n",
      "Epoch 26/50\n",
      " - 1s - loss: 81.9601 - squared_difference_loss: 36.0805 - KL_divergence_loss: 4.1888 - neg_log_likelihood: 77.7713 - val_loss: 81.9230 - val_squared_difference_loss: 35.9567 - val_KL_divergence_loss: 4.2137 - val_neg_log_likelihood: 77.7094\n",
      "Epoch 27/50\n",
      " - 1s - loss: 81.9246 - squared_difference_loss: 36.0964 - KL_divergence_loss: 4.1454 - neg_log_likelihood: 77.7792 - val_loss: 81.8936 - val_squared_difference_loss: 35.7217 - val_KL_divergence_loss: 4.3017 - val_neg_log_likelihood: 77.5918\n",
      "Epoch 28/50\n",
      " - 1s - loss: 81.9447 - squared_difference_loss: 36.0246 - KL_divergence_loss: 4.2014 - neg_log_likelihood: 77.7433 - val_loss: 81.8549 - val_squared_difference_loss: 35.9297 - val_KL_divergence_loss: 4.1591 - val_neg_log_likelihood: 77.6959\n",
      "Epoch 29/50\n",
      " - 1s - loss: 81.8808 - squared_difference_loss: 35.9932 - KL_divergence_loss: 4.1532 - neg_log_likelihood: 77.7276 - val_loss: 81.8730 - val_squared_difference_loss: 35.8632 - val_KL_divergence_loss: 4.2104 - val_neg_log_likelihood: 77.6626\n",
      "Epoch 30/50\n",
      " - 1s - loss: 81.8729 - squared_difference_loss: 35.9104 - KL_divergence_loss: 4.1867 - neg_log_likelihood: 77.6862 - val_loss: 81.7773 - val_squared_difference_loss: 35.5320 - val_KL_divergence_loss: 4.2803 - val_neg_log_likelihood: 77.4970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      " - 1s - loss: 81.8337 - squared_difference_loss: 35.8583 - KL_divergence_loss: 4.1735 - neg_log_likelihood: 77.6602 - val_loss: 81.8110 - val_squared_difference_loss: 35.3972 - val_KL_divergence_loss: 4.3813 - val_neg_log_likelihood: 77.4296\n",
      "Epoch 32/50\n",
      " - 1s - loss: 81.8248 - squared_difference_loss: 35.7502 - KL_divergence_loss: 4.2187 - neg_log_likelihood: 77.6061 - val_loss: 81.7163 - val_squared_difference_loss: 35.5350 - val_KL_divergence_loss: 4.2177 - val_neg_log_likelihood: 77.4985\n",
      "Epoch 33/50\n",
      " - 1s - loss: 81.8435 - squared_difference_loss: 35.8401 - KL_divergence_loss: 4.1924 - neg_log_likelihood: 77.6510 - val_loss: 81.6996 - val_squared_difference_loss: 35.5988 - val_KL_divergence_loss: 4.1691 - val_neg_log_likelihood: 77.5304\n",
      "Epoch 34/50\n",
      " - 1s - loss: 81.8175 - squared_difference_loss: 35.7983 - KL_divergence_loss: 4.1874 - neg_log_likelihood: 77.6302 - val_loss: 81.7291 - val_squared_difference_loss: 35.4391 - val_KL_divergence_loss: 4.2786 - val_neg_log_likelihood: 77.4506\n",
      "Epoch 35/50\n",
      " - 1s - loss: 81.8311 - squared_difference_loss: 35.7755 - KL_divergence_loss: 4.2123 - neg_log_likelihood: 77.6188 - val_loss: 81.6863 - val_squared_difference_loss: 35.1423 - val_KL_divergence_loss: 4.3842 - val_neg_log_likelihood: 77.3021\n",
      "Epoch 36/50\n",
      " - 1s - loss: 81.8071 - squared_difference_loss: 35.6694 - KL_divergence_loss: 4.2414 - neg_log_likelihood: 77.5657 - val_loss: 81.6808 - val_squared_difference_loss: 35.3026 - val_KL_divergence_loss: 4.2985 - val_neg_log_likelihood: 77.3823\n",
      "Epoch 37/50\n",
      " - 1s - loss: 81.7830 - squared_difference_loss: 35.7383 - KL_divergence_loss: 4.1828 - neg_log_likelihood: 77.6002 - val_loss: 81.7483 - val_squared_difference_loss: 35.3263 - val_KL_divergence_loss: 4.3541 - val_neg_log_likelihood: 77.3941\n",
      "Epoch 38/50\n",
      " - 1s - loss: 81.7618 - squared_difference_loss: 35.6311 - KL_divergence_loss: 4.2153 - neg_log_likelihood: 77.5465 - val_loss: 81.8185 - val_squared_difference_loss: 35.5675 - val_KL_divergence_loss: 4.3037 - val_neg_log_likelihood: 77.5148\n",
      "Epoch 39/50\n",
      " - 1s - loss: 81.7904 - squared_difference_loss: 35.6697 - KL_divergence_loss: 4.2245 - neg_log_likelihood: 77.5658 - val_loss: 81.6847 - val_squared_difference_loss: 35.1063 - val_KL_divergence_loss: 4.4005 - val_neg_log_likelihood: 77.2841\n",
      "Epoch 40/50\n",
      " - 1s - loss: 81.7213 - squared_difference_loss: 35.5691 - KL_divergence_loss: 4.2057 - neg_log_likelihood: 77.5156 - val_loss: 81.6013 - val_squared_difference_loss: 35.2739 - val_KL_divergence_loss: 4.2333 - val_neg_log_likelihood: 77.3680\n",
      "Epoch 41/50\n",
      " - 1s - loss: 81.7328 - squared_difference_loss: 35.5731 - KL_divergence_loss: 4.2152 - neg_log_likelihood: 77.5175 - val_loss: 81.6042 - val_squared_difference_loss: 35.0632 - val_KL_divergence_loss: 4.3416 - val_neg_log_likelihood: 77.2626\n",
      "Epoch 42/50\n",
      " - 1s - loss: 81.7128 - squared_difference_loss: 35.4753 - KL_divergence_loss: 4.2441 - neg_log_likelihood: 77.4686 - val_loss: 81.5760 - val_squared_difference_loss: 35.0997 - val_KL_divergence_loss: 4.2952 - val_neg_log_likelihood: 77.2808\n",
      "Epoch 43/50\n",
      " - 1s - loss: 81.7275 - squared_difference_loss: 35.5266 - KL_divergence_loss: 4.2332 - neg_log_likelihood: 77.4943 - val_loss: 81.6216 - val_squared_difference_loss: 35.0555 - val_KL_divergence_loss: 4.3629 - val_neg_log_likelihood: 77.2587\n",
      "Epoch 44/50\n",
      " - 1s - loss: 81.7133 - squared_difference_loss: 35.5115 - KL_divergence_loss: 4.2265 - neg_log_likelihood: 77.4867 - val_loss: 81.5962 - val_squared_difference_loss: 35.2286 - val_KL_divergence_loss: 4.2509 - val_neg_log_likelihood: 77.3453\n",
      "Epoch 45/50\n",
      " - 1s - loss: 81.6950 - squared_difference_loss: 35.4081 - KL_divergence_loss: 4.2600 - neg_log_likelihood: 77.4350 - val_loss: 81.5517 - val_squared_difference_loss: 35.1778 - val_KL_divergence_loss: 4.2318 - val_neg_log_likelihood: 77.3199\n",
      "Epoch 46/50\n",
      " - 1s - loss: 81.6640 - squared_difference_loss: 35.3978 - KL_divergence_loss: 4.2342 - neg_log_likelihood: 77.4299 - val_loss: 81.4852 - val_squared_difference_loss: 34.9848 - val_KL_divergence_loss: 4.2617 - val_neg_log_likelihood: 77.2234\n",
      "Epoch 47/50\n",
      " - 1s - loss: 81.7126 - squared_difference_loss: 35.4455 - KL_divergence_loss: 4.2589 - neg_log_likelihood: 77.4538 - val_loss: 81.5534 - val_squared_difference_loss: 35.0603 - val_KL_divergence_loss: 4.2922 - val_neg_log_likelihood: 77.2611\n",
      "Epoch 48/50\n",
      " - 1s - loss: 81.6523 - squared_difference_loss: 35.3688 - KL_divergence_loss: 4.2369 - neg_log_likelihood: 77.4154 - val_loss: 81.5405 - val_squared_difference_loss: 35.0475 - val_KL_divergence_loss: 4.2857 - val_neg_log_likelihood: 77.2548\n",
      "Epoch 49/50\n",
      " - 1s - loss: 81.6890 - squared_difference_loss: 35.4185 - KL_divergence_loss: 4.2487 - neg_log_likelihood: 77.4403 - val_loss: 81.5086 - val_squared_difference_loss: 34.9244 - val_KL_divergence_loss: 4.3154 - val_neg_log_likelihood: 77.1932\n",
      "Epoch 50/50\n",
      " - 1s - loss: 81.6621 - squared_difference_loss: 35.3356 - KL_divergence_loss: 4.2633 - neg_log_likelihood: 77.3988 - val_loss: 81.5244 - val_squared_difference_loss: 34.7862 - val_KL_divergence_loss: 4.4003 - val_neg_log_likelihood: 77.1241\n",
      "\tAverage Validation Squared Error = 75.34837364168796\n",
      "Latent Var Num = 5\n",
      "Hidden Var Num = 50\n",
      "Simulation #17: VAE with 5 Latent Variable(s) and 50 Hidden Variable(s)...\n",
      "\tEvaluating model on Fold 1\n",
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/50\n",
      " - 15s - loss: 94.4148 - squared_difference_loss: 63.1208 - KL_divergence_loss: 3.1234 - neg_log_likelihood: 91.2914 - val_loss: 86.1740 - val_squared_difference_loss: 46.0202 - val_KL_divergence_loss: 3.4329 - val_neg_log_likelihood: 82.7411\n",
      "Epoch 2/50\n",
      " - 1s - loss: 85.0888 - squared_difference_loss: 44.3749 - KL_divergence_loss: 3.1703 - neg_log_likelihood: 81.9184 - val_loss: 83.9518 - val_squared_difference_loss: 39.9674 - val_KL_divergence_loss: 4.2371 - val_neg_log_likelihood: 79.7147\n",
      "Epoch 3/50\n",
      " - 1s - loss: 83.9431 - squared_difference_loss: 41.1379 - KL_divergence_loss: 3.6432 - neg_log_likelihood: 80.2999 - val_loss: 83.3424 - val_squared_difference_loss: 38.2513 - val_KL_divergence_loss: 4.4858 - val_neg_log_likelihood: 78.8566\n",
      "Epoch 4/50\n",
      " - 1s - loss: 83.5091 - squared_difference_loss: 39.8313 - KL_divergence_loss: 3.8624 - neg_log_likelihood: 79.6467 - val_loss: 83.1416 - val_squared_difference_loss: 37.6714 - val_KL_divergence_loss: 4.5749 - val_neg_log_likelihood: 78.5667\n",
      "Epoch 5/50\n",
      " - 1s - loss: 83.2659 - squared_difference_loss: 39.1648 - KL_divergence_loss: 3.9525 - neg_log_likelihood: 79.3134 - val_loss: 83.0611 - val_squared_difference_loss: 37.7224 - val_KL_divergence_loss: 4.4689 - val_neg_log_likelihood: 78.5922\n",
      "Epoch 6/50\n",
      " - 1s - loss: 83.0961 - squared_difference_loss: 38.6789 - KL_divergence_loss: 4.0256 - neg_log_likelihood: 79.0704 - val_loss: 82.8911 - val_squared_difference_loss: 37.3336 - val_KL_divergence_loss: 4.4933 - val_neg_log_likelihood: 78.3978\n",
      "Epoch 7/50\n",
      " - 1s - loss: 82.9544 - squared_difference_loss: 38.3792 - KL_divergence_loss: 4.0338 - neg_log_likelihood: 78.9206 - val_loss: 82.7454 - val_squared_difference_loss: 37.0785 - val_KL_divergence_loss: 4.4751 - val_neg_log_likelihood: 78.2702\n",
      "Epoch 8/50\n",
      " - 1s - loss: 82.8319 - squared_difference_loss: 38.0495 - KL_divergence_loss: 4.0761 - neg_log_likelihood: 78.7558 - val_loss: 82.7170 - val_squared_difference_loss: 37.1946 - val_KL_divergence_loss: 4.3887 - val_neg_log_likelihood: 78.3283\n",
      "Epoch 9/50\n",
      " - 1s - loss: 82.7467 - squared_difference_loss: 37.8503 - KL_divergence_loss: 4.0905 - neg_log_likelihood: 78.6562 - val_loss: 82.6181 - val_squared_difference_loss: 36.9905 - val_KL_divergence_loss: 4.3919 - val_neg_log_likelihood: 78.2262\n",
      "Epoch 10/50\n",
      " - 1s - loss: 82.6550 - squared_difference_loss: 37.5856 - KL_divergence_loss: 4.1312 - neg_log_likelihood: 78.5238 - val_loss: 82.6412 - val_squared_difference_loss: 37.2188 - val_KL_divergence_loss: 4.3008 - val_neg_log_likelihood: 78.3404\n",
      "Epoch 11/50\n",
      " - 1s - loss: 82.5889 - squared_difference_loss: 37.4572 - KL_divergence_loss: 4.1293 - neg_log_likelihood: 78.4596 - val_loss: 82.6235 - val_squared_difference_loss: 37.0249 - val_KL_divergence_loss: 4.3801 - val_neg_log_likelihood: 78.2434\n",
      "Epoch 12/50\n",
      " - 1s - loss: 82.4688 - squared_difference_loss: 37.1717 - KL_divergence_loss: 4.1520 - neg_log_likelihood: 78.3168 - val_loss: 82.6722 - val_squared_difference_loss: 37.5522 - val_KL_divergence_loss: 4.1651 - val_neg_log_likelihood: 78.5071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      " - 1s - loss: 82.4026 - squared_difference_loss: 37.1481 - KL_divergence_loss: 4.0975 - neg_log_likelihood: 78.3051 - val_loss: 82.4794 - val_squared_difference_loss: 36.8003 - val_KL_divergence_loss: 4.3483 - val_neg_log_likelihood: 78.1312\n",
      "Epoch 14/50\n",
      " - 1s - loss: 82.3179 - squared_difference_loss: 36.8277 - KL_divergence_loss: 4.1730 - neg_log_likelihood: 78.1449 - val_loss: 82.4487 - val_squared_difference_loss: 36.6276 - val_KL_divergence_loss: 4.4039 - val_neg_log_likelihood: 78.0448\n",
      "Epoch 15/50\n",
      " - 1s - loss: 82.2406 - squared_difference_loss: 36.6318 - KL_divergence_loss: 4.1937 - neg_log_likelihood: 78.0469 - val_loss: 82.4397 - val_squared_difference_loss: 37.0116 - val_KL_divergence_loss: 4.2029 - val_neg_log_likelihood: 78.2368\n",
      "Epoch 16/50\n",
      " - 1s - loss: 82.2049 - squared_difference_loss: 36.6241 - KL_divergence_loss: 4.1618 - neg_log_likelihood: 78.0430 - val_loss: 82.4243 - val_squared_difference_loss: 36.7667 - val_KL_divergence_loss: 4.3099 - val_neg_log_likelihood: 78.1143\n",
      "Epoch 17/50\n",
      " - 1s - loss: 82.1170 - squared_difference_loss: 36.5240 - KL_divergence_loss: 4.1240 - neg_log_likelihood: 77.9930 - val_loss: 82.3044 - val_squared_difference_loss: 36.3836 - val_KL_divergence_loss: 4.3816 - val_neg_log_likelihood: 77.9228\n",
      "Epoch 18/50\n",
      " - 1s - loss: 82.0916 - squared_difference_loss: 36.3090 - KL_divergence_loss: 4.2061 - neg_log_likelihood: 77.8855 - val_loss: 82.0880 - val_squared_difference_loss: 35.9869 - val_KL_divergence_loss: 4.3635 - val_neg_log_likelihood: 77.7245\n",
      "Epoch 19/50\n",
      " - 1s - loss: 82.0303 - squared_difference_loss: 36.2398 - KL_divergence_loss: 4.1794 - neg_log_likelihood: 77.8509 - val_loss: 82.0824 - val_squared_difference_loss: 36.0176 - val_KL_divergence_loss: 4.3426 - val_neg_log_likelihood: 77.7398\n",
      "Epoch 20/50\n",
      " - 1s - loss: 82.0032 - squared_difference_loss: 36.1690 - KL_divergence_loss: 4.1877 - neg_log_likelihood: 77.8155 - val_loss: 82.0824 - val_squared_difference_loss: 36.0719 - val_KL_divergence_loss: 4.3154 - val_neg_log_likelihood: 77.7670\n",
      "Epoch 21/50\n",
      " - 1s - loss: 81.9612 - squared_difference_loss: 36.0372 - KL_divergence_loss: 4.2116 - neg_log_likelihood: 77.7496 - val_loss: 81.9487 - val_squared_difference_loss: 35.8570 - val_KL_divergence_loss: 4.2892 - val_neg_log_likelihood: 77.6595\n",
      "Epoch 22/50\n",
      " - 1s - loss: 81.9059 - squared_difference_loss: 35.9754 - KL_divergence_loss: 4.1872 - neg_log_likelihood: 77.7187 - val_loss: 82.0089 - val_squared_difference_loss: 35.8629 - val_KL_divergence_loss: 4.3464 - val_neg_log_likelihood: 77.6624\n",
      "Epoch 23/50\n",
      " - 1s - loss: 81.9404 - squared_difference_loss: 36.0021 - KL_divergence_loss: 4.2083 - neg_log_likelihood: 77.7321 - val_loss: 82.0020 - val_squared_difference_loss: 35.9934 - val_KL_divergence_loss: 4.2743 - val_neg_log_likelihood: 77.7277\n",
      "Epoch 24/50\n",
      " - 1s - loss: 81.9101 - squared_difference_loss: 35.9516 - KL_divergence_loss: 4.2033 - neg_log_likelihood: 77.7068 - val_loss: 81.9915 - val_squared_difference_loss: 36.0085 - val_KL_divergence_loss: 4.2562 - val_neg_log_likelihood: 77.7353\n",
      "Epoch 25/50\n",
      " - 1s - loss: 81.8691 - squared_difference_loss: 35.8180 - KL_divergence_loss: 4.2291 - neg_log_likelihood: 77.6400 - val_loss: 81.9638 - val_squared_difference_loss: 35.7450 - val_KL_divergence_loss: 4.3603 - val_neg_log_likelihood: 77.6035\n",
      "Epoch 26/50\n",
      " - 1s - loss: 81.8221 - squared_difference_loss: 35.7876 - KL_divergence_loss: 4.1973 - neg_log_likelihood: 77.6248 - val_loss: 81.8928 - val_squared_difference_loss: 35.5768 - val_KL_divergence_loss: 4.3734 - val_neg_log_likelihood: 77.5194\n",
      "Epoch 27/50\n",
      " - 1s - loss: 81.8148 - squared_difference_loss: 35.7230 - KL_divergence_loss: 4.2223 - neg_log_likelihood: 77.5925 - val_loss: 81.9930 - val_squared_difference_loss: 36.1310 - val_KL_divergence_loss: 4.1965 - val_neg_log_likelihood: 77.7965\n",
      "Epoch 28/50\n",
      " - 1s - loss: 81.8229 - squared_difference_loss: 35.7701 - KL_divergence_loss: 4.2068 - neg_log_likelihood: 77.6160 - val_loss: 81.8045 - val_squared_difference_loss: 35.5612 - val_KL_divergence_loss: 4.2928 - val_neg_log_likelihood: 77.5116\n",
      "Epoch 29/50\n",
      " - 1s - loss: 81.8294 - squared_difference_loss: 35.7742 - KL_divergence_loss: 4.2113 - neg_log_likelihood: 77.6181 - val_loss: 81.7548 - val_squared_difference_loss: 35.4906 - val_KL_divergence_loss: 4.2785 - val_neg_log_likelihood: 77.4763\n",
      "Epoch 30/50\n",
      " - 1s - loss: 81.7346 - squared_difference_loss: 35.6099 - KL_divergence_loss: 4.1987 - neg_log_likelihood: 77.5360 - val_loss: 81.8220 - val_squared_difference_loss: 35.5303 - val_KL_divergence_loss: 4.3258 - val_neg_log_likelihood: 77.4962\n",
      "Epoch 31/50\n",
      " - 1s - loss: 81.7665 - squared_difference_loss: 35.6209 - KL_divergence_loss: 4.2251 - neg_log_likelihood: 77.5414 - val_loss: 81.7777 - val_squared_difference_loss: 35.4549 - val_KL_divergence_loss: 4.3193 - val_neg_log_likelihood: 77.4584\n",
      "Epoch 32/50\n",
      " - 1s - loss: 81.7305 - squared_difference_loss: 35.5195 - KL_divergence_loss: 4.2397 - neg_log_likelihood: 77.4908 - val_loss: 81.6805 - val_squared_difference_loss: 35.2206 - val_KL_divergence_loss: 4.3392 - val_neg_log_likelihood: 77.3413\n",
      "Epoch 33/50\n",
      " - 1s - loss: 81.7131 - squared_difference_loss: 35.5157 - KL_divergence_loss: 4.2243 - neg_log_likelihood: 77.4889 - val_loss: 81.6810 - val_squared_difference_loss: 35.2051 - val_KL_divergence_loss: 4.3475 - val_neg_log_likelihood: 77.3335\n",
      "Epoch 34/50\n",
      " - 1s - loss: 81.7148 - squared_difference_loss: 35.4538 - KL_divergence_loss: 4.2569 - neg_log_likelihood: 77.4579 - val_loss: 81.6281 - val_squared_difference_loss: 35.1399 - val_KL_divergence_loss: 4.3271 - val_neg_log_likelihood: 77.3010\n",
      "Epoch 35/50\n",
      " - 1s - loss: 81.6916 - squared_difference_loss: 35.4640 - KL_divergence_loss: 4.2286 - neg_log_likelihood: 77.4630 - val_loss: 81.6208 - val_squared_difference_loss: 35.1231 - val_KL_divergence_loss: 4.3282 - val_neg_log_likelihood: 77.2926\n",
      "Epoch 36/50\n",
      " - 1s - loss: 81.7017 - squared_difference_loss: 35.4160 - KL_divergence_loss: 4.2627 - neg_log_likelihood: 77.4390 - val_loss: 81.6409 - val_squared_difference_loss: 35.4880 - val_KL_divergence_loss: 4.1660 - val_neg_log_likelihood: 77.4750\n",
      "Epoch 37/50\n",
      " - 1s - loss: 81.6767 - squared_difference_loss: 35.4507 - KL_divergence_loss: 4.2203 - neg_log_likelihood: 77.4564 - val_loss: 81.6446 - val_squared_difference_loss: 34.9072 - val_KL_divergence_loss: 4.4599 - val_neg_log_likelihood: 77.1846\n",
      "Epoch 38/50\n",
      " - 1s - loss: 81.6388 - squared_difference_loss: 35.3453 - KL_divergence_loss: 4.2351 - neg_log_likelihood: 77.4037 - val_loss: 81.7357 - val_squared_difference_loss: 35.3618 - val_KL_divergence_loss: 4.3238 - val_neg_log_likelihood: 77.4119\n",
      "Epoch 39/50\n",
      " - 1s - loss: 81.6428 - squared_difference_loss: 35.3532 - KL_divergence_loss: 4.2353 - neg_log_likelihood: 77.4076 - val_loss: 81.5225 - val_squared_difference_loss: 34.9651 - val_KL_divergence_loss: 4.3089 - val_neg_log_likelihood: 77.2135\n",
      "Epoch 40/50\n",
      " - 1s - loss: 81.6485 - squared_difference_loss: 35.3160 - KL_divergence_loss: 4.2595 - neg_log_likelihood: 77.3890 - val_loss: 81.6764 - val_squared_difference_loss: 35.0477 - val_KL_divergence_loss: 4.4215 - val_neg_log_likelihood: 77.2549\n",
      "Epoch 41/50\n",
      " - 1s - loss: 81.5896 - squared_difference_loss: 35.2155 - KL_divergence_loss: 4.2508 - neg_log_likelihood: 77.3388 - val_loss: 81.5532 - val_squared_difference_loss: 34.8455 - val_KL_divergence_loss: 4.3995 - val_neg_log_likelihood: 77.1538\n",
      "Epoch 42/50\n",
      " - 1s - loss: 81.5917 - squared_difference_loss: 35.1637 - KL_divergence_loss: 4.2788 - neg_log_likelihood: 77.3129 - val_loss: 81.5449 - val_squared_difference_loss: 35.1871 - val_KL_divergence_loss: 4.2204 - val_neg_log_likelihood: 77.3246\n",
      "Epoch 43/50\n",
      " - 1s - loss: 81.5950 - squared_difference_loss: 35.2248 - KL_divergence_loss: 4.2516 - neg_log_likelihood: 77.3434 - val_loss: 81.5827 - val_squared_difference_loss: 35.1539 - val_KL_divergence_loss: 4.2747 - val_neg_log_likelihood: 77.3080\n",
      "Epoch 44/50\n",
      " - 1s - loss: 81.5496 - squared_difference_loss: 35.1613 - KL_divergence_loss: 4.2380 - neg_log_likelihood: 77.3116 - val_loss: 81.5711 - val_squared_difference_loss: 35.1803 - val_KL_divergence_loss: 4.2499 - val_neg_log_likelihood: 77.3212\n",
      "Epoch 45/50\n",
      " - 1s - loss: 81.5900 - squared_difference_loss: 35.1915 - KL_divergence_loss: 4.2632 - neg_log_likelihood: 77.3267 - val_loss: 81.6120 - val_squared_difference_loss: 35.1809 - val_KL_divergence_loss: 4.2905 - val_neg_log_likelihood: 77.3215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50\n",
      " - 1s - loss: 81.5842 - squared_difference_loss: 35.1537 - KL_divergence_loss: 4.2764 - neg_log_likelihood: 77.3079 - val_loss: 81.5370 - val_squared_difference_loss: 35.1214 - val_KL_divergence_loss: 4.2452 - val_neg_log_likelihood: 77.2917\n",
      "Epoch 47/50\n",
      " - 1s - loss: 81.5509 - squared_difference_loss: 35.1364 - KL_divergence_loss: 4.2517 - neg_log_likelihood: 77.2992 - val_loss: 81.4903 - val_squared_difference_loss: 35.1669 - val_KL_divergence_loss: 4.1758 - val_neg_log_likelihood: 77.3144\n",
      "Epoch 48/50\n",
      " - 1s - loss: 81.5468 - squared_difference_loss: 35.1435 - KL_divergence_loss: 4.2440 - neg_log_likelihood: 77.3027 - val_loss: 81.5774 - val_squared_difference_loss: 34.8371 - val_KL_divergence_loss: 4.4278 - val_neg_log_likelihood: 77.1496\n",
      "Epoch 49/50\n",
      " - 1s - loss: 81.5936 - squared_difference_loss: 35.1138 - KL_divergence_loss: 4.3057 - neg_log_likelihood: 77.2879 - val_loss: 81.5951 - val_squared_difference_loss: 35.0464 - val_KL_divergence_loss: 4.3409 - val_neg_log_likelihood: 77.2542\n",
      "Epoch 50/50\n",
      " - 1s - loss: 81.5277 - squared_difference_loss: 35.0424 - KL_divergence_loss: 4.2755 - neg_log_likelihood: 77.2522 - val_loss: 81.4331 - val_squared_difference_loss: 34.7987 - val_KL_divergence_loss: 4.3028 - val_neg_log_likelihood: 77.1304\n",
      "\tAverage Validation Squared Error = 75.22735640040696\n",
      "Latent Var Num = 5\n",
      "Hidden Var Num = 60\n",
      "Simulation #18: VAE with 5 Latent Variable(s) and 60 Hidden Variable(s)...\n",
      "\tEvaluating model on Fold 1\n",
      "Train on 29329 samples, validate on 12570 samples\n",
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "X_mat = np.array(X[X.columns.values[:65]])\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "num_latent_vars = [3, 4, 5, 6, 7]\n",
    "num_hidden_vars = [10, 20, 30, 40, 50, 60]\n",
    "# num_latent_vars = [5, 10, 15, 20, 25, 30, 50, 75, 100, 150, 250, 500, 1000]\n",
    "# num_hidden_vars = [10, 25, 50, 100, 250, 500, 750, 1000, 1500, 2500, 5000]\n",
    "cv_results = pd.DataFrame(0,\n",
    "                           index=num_latent_vars,\n",
    "                           columns=num_hidden_vars)\n",
    "\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "kf.get_n_splits(X_mat)\n",
    "\n",
    "i = 1\n",
    "for lat_var_num in num_latent_vars:\n",
    "    for hid_var_num in num_hidden_vars:\n",
    "        print(\"Latent Var Num = {}\".format(lat_var_num))\n",
    "        print(\"Hidden Var Num = {}\".format(hid_var_num))\n",
    "        fold_num = 1\n",
    "        total_nll = 0.\n",
    "        \n",
    "        print('Simulation #{}: VAE with {} Latent Variable(s) and {} Hidden Variable(s)...'.format(i, lat_var_num, hid_var_num))\n",
    "        \n",
    "        for train_index, test_index in kf.split(X_mat):\n",
    "            X_train, X_test = X_mat[train_index], X_mat[test_index]\n",
    "            print(\"\\tEvaluating model on Fold {}\".format(fold_num))\n",
    "            total_nll += evaluate_vae_model(X_train, \n",
    "                                            X_test, \n",
    "                                            latent_dimensions=lat_var_num, \n",
    "                                            intermediate_dimensions=hid_var_num, n_epochs=50)[0]\n",
    "            if total_nll == float('NaN'):\n",
    "                break\n",
    "            fold_num += 1\n",
    "            if fold_num > 1:\n",
    "                break\n",
    "\n",
    "        cv_results.loc[lat_var_num, hid_var_num] = total_nll #/ n_folds\n",
    "        print('\\tAverage Validation Squared Error = {}'.format(total_nll))# / n_folds))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>40</th>\n",
       "      <th>50</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>294.108784</td>\n",
       "      <td>597.136168</td>\n",
       "      <td>296.537311</td>\n",
       "      <td>829.538323</td>\n",
       "      <td>242.739019</td>\n",
       "      <td>290.444039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306.489237</td>\n",
       "      <td>366.912645</td>\n",
       "      <td>288.961876</td>\n",
       "      <td>345.868654</td>\n",
       "      <td>1562.217812</td>\n",
       "      <td>335.192125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1360.512203</td>\n",
       "      <td>146.045588</td>\n",
       "      <td>241.453949</td>\n",
       "      <td>982.848449</td>\n",
       "      <td>502.773522</td>\n",
       "      <td>1260.337332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3059.990420</td>\n",
       "      <td>175.341459</td>\n",
       "      <td>308.329294</td>\n",
       "      <td>518.490705</td>\n",
       "      <td>453.016443</td>\n",
       "      <td>1330.632944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1011.067096</td>\n",
       "      <td>1078.551828</td>\n",
       "      <td>1096.182228</td>\n",
       "      <td>227.644510</td>\n",
       "      <td>309.594978</td>\n",
       "      <td>151.503265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            10           20           30          40           50           60\n",
       "3   294.108784   597.136168   296.537311  829.538323   242.739019   290.444039\n",
       "4   306.489237   366.912645   288.961876  345.868654  1562.217812   335.192125\n",
       "5  1360.512203   146.045588   241.453949  982.848449   502.773522  1260.337332\n",
       "6  3059.990420   175.341459   308.329294  518.490705   453.016443  1330.632944\n",
       "7  1011.067096  1078.551828  1096.182228  227.644510   309.594978   151.503265"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3583e6dc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(-cv_results)\n",
    "plt.xlabel('# Hidden Layer Variables')\n",
    "plt.ylabel('# Latent Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.covariance import ShrunkCovariance, LedoitWolf\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# #############################################################################\n",
    "# Fit the models\n",
    "\n",
    "n_components = np.arange(5, 60, 5)  # options for n_components\n",
    "\n",
    "def compute_scores(X):\n",
    "    pca = PCA(svd_solver='full')\n",
    "    fa = FactorAnalysis()\n",
    "\n",
    "    pca_scores, fa_scores = [], []\n",
    "    for n in n_components:\n",
    "        if n % 1 == 0:\n",
    "            print (\"Running PCA/FA for {} Components...\".format(n))\n",
    "        pca.n_components = n\n",
    "        fa.n_components = n\n",
    "        pca_scores.append(np.mean(cross_val_score(pca, X))) # Return the average log-likelihood of all samples.\n",
    "        fa_scores.append(np.mean(cross_val_score(fa, X))) # Return the average log-likelihood of all samples.\n",
    "    \n",
    "    return pca_scores, fa_scores\n",
    "\n",
    "def shrunk_cov_score(X):\n",
    "    shrinkages = np.logspace(-2, 0, 30)\n",
    "    cv = GridSearchCV(ShrunkCovariance(), {'shrinkage': shrinkages})\n",
    "    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X))\n",
    "\n",
    "def lw_score(X):\n",
    "    return np.mean(cross_val_score(LedoitWolf(), X))\n",
    "\n",
    "print(\"Computing PCA and FA scores...\")\n",
    "pca_scores, fa_scores = compute_scores(X_model_input)\n",
    "n_components_pca = n_components[np.argmax(pca_scores)]\n",
    "n_components_fa = n_components[np.argmax(fa_scores)]\n",
    "\n",
    "pca = PCA(svd_solver='full', n_components='mle')\n",
    "pca.fit(X_model_input)\n",
    "n_components_pca_mle = pca.n_components_\n",
    "\n",
    "print(\"best n_components by PCA CV = %d\" % n_components_pca)\n",
    "print(\"best n_components by FactorAnalysis CV = %d\" % n_components_fa)\n",
    "print(\"best n_components by PCA MLE = %d\" % n_components_pca_mle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zip(n_components, pca_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip(n_components, fa_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "#plt.plot(num_latent_vars[:6], -cv_results.loc[num_latent_vars[:6],150], 'g', label='VAE scores')\n",
    "\n",
    "plt.plot(n_components, pca_scores, 'b', label='Probabilistic PCA scores')\n",
    "# plt.axvline(n_components_pca, color='b',\n",
    "#             label='PCA CV: %d' % n_components_pca, linestyle='--')\n",
    "\n",
    "plt.plot(n_components, fa_scores, 'r', label='Factor Analysis scores')\n",
    "# plt.axvline(n_components_fa, color='r',\n",
    "#             label='FactorAnalysis CV: %d' % n_components_fa,\n",
    "#             linestyle='--')\n",
    "\n",
    "plt.axhline(lw_score(X), color='orange',\n",
    "            label='LedoitWolf MLE' % n_components_pca_mle, linestyle='-.')\n",
    "\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Log-Likelihood')\n",
    "# plt.ylabel('CV scores')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
